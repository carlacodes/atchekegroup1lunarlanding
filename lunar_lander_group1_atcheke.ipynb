{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlacodes/atchekegroup1lunarlanding/blob/sherry/lunar_lander_group1_atcheke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rKjkh_KgobbF"
      },
      "source": [
        "# Performance Analysis of DQN Algorithm on the Lunar Lander task\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Raghuram Bharadwaj Diddigi, Geraud Nangue Tasse, Yamil Vidal, Sanjukta Krishnagopal, Sara Rajaee\n",
        "\n",
        "__Content editors:__ Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "iDR5t137obbP"
      },
      "source": [
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "brZ9ZJQzobbR"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "In this project, the objective is to analyze the performance of the Deep Q-Learning algorithm on an exciting task- Lunar Lander. Before we describe the task, let us focus on two keywords here - analysis and performance. What exactly do we mean by these keywords in the context of Reinforcement Learning (RL)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "wpX-jA7GobbT"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "9EanvrrhobbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173f350c-b4ae-416e-9054-5d59e0132bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 177 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 448 kB 4.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "pE3qZJLcobbW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor,RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPoZnSTGFvEp",
        "outputId": "943d9e6e-8277-4abf-b8be-422b5e09c384"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {},
        "id": "fp1bUnClobbY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = \"/content/gdrive/My Drive/LunarLanding/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4otSbkh_QAYF",
        "outputId": "c749b8a5-f58a-4faf-a328-fd5b77f7a045"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "psfdqUCmobba"
      },
      "source": [
        "---\n",
        "# Introduction\n",
        "\n",
        "In a standard RL setting, an agent learns optimal behavior from an environment through a feedback mechanism to maximize a given objective. Many algorithms have been proposed in the RL literature that an agent can apply to learn the optimal behavior. One such popular algorithm is the Deep Q-Network (DQN). This algorithm makes use of deep neural networks to compute optimal actions. In this project, your goal is to understand the effect of the number of neural network layers on the algorithm's performance. The performance of the algorithm can be evaluated through two metrics - Speed and Stability. \n",
        "\n",
        "**Speed:** How fast the algorithm reaches the maximum possible reward. \n",
        "\n",
        "**Stability** In some applications (especially when online learning is involved), along with speed, stability of the algorithm, i.e., minimal fluctuations in performance, is equally important. \n",
        "\n",
        "In this project, you should investigate the following question:\n",
        "\n",
        "**What is the impact of number of neural network layers on speed and stability of the algorithm?**\n",
        "\n",
        "You do not have to write the DQN code from scratch. We have provided a basic implementation of the DQN algorithm. You only have to tune the hyperparameters (neural network size, learning rate, etc), observe the performance, and analyze. More details on this are provided below. \n",
        "\n",
        "Now, let us discuss the RL task we have chosen, i.e., Lunar Lander. This task consists of the lander and a landing pad marked by two flags. The episode starts with the lander moving downwards due to gravity. The objective is to land safely using different engines available on the lander with zero speed on the landing pad as quickly and fuel efficient as possible. Reward for moving from the top of the screen and landing on landing pad with zero speed is between 100 to 140 points. Each leg ground contact yields a reward of 10 points. Firing main engine leads to a reward of -0.3 points in each frame. Firing the side engine leads to a reward of -0.03 points in each frame. An additional reward of -100 or +100 points is received if the lander crashes or comes to rest respectively which also leads to end of the episode. \n",
        "\n",
        "The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "  1. Horizontal Position\n",
        "  2. Vertical Position\n",
        "  3. Horizontal Velocity\n",
        "  4. Vertical Velocity\n",
        "  5. Angle\n",
        "  6. Angular Velocity\n",
        "  7. Left Leg Contact\n",
        "  8. Right Leg Contact\n",
        "\n",
        "The actions of the agents are:\n",
        "  1. Do Nothing\n",
        "  2. Fire Main Engine\n",
        "  3. Fire Left Engine\n",
        "  4. Fire Right Engine\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/lunar_lander.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters"
      ],
      "metadata": {
        "id": "XgNuDz2mrq0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ref: \n",
        "\n",
        "1.   template https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/4-Optimization/FinRL_HyperparameterTuning_using_Optuna_basic.ipynb\n",
        "\n",
        "2.   tutorial (combined with template): https://medium.com/analytics-vidhya/hyperparameter-tuning-using-optuna-for-finrl-8a49506d2741\n",
        "\n",
        "3.  set parameter function: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py\n",
        "\n",
        "4.   callback function: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/callbacks.py#L10\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6RQ_bAC6KlAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install optuna --quiet"
      ],
      "metadata": {
        "id": "qSg7t4EJx9Pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8ccfde-ca8c-4394-abc1-165f69b1cb16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 308 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 63.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 93.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 96.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import sys"
      ],
      "metadata": {
        "id": "CHT0LZ0WyERZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define search space"
      ],
      "metadata": {
        "id": "i_gZFwkKtj5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using stable baseline zoo parameter as ref\n",
        "def sample_dqn_params(trial: optuna.Trial):\n",
        "    \"\"\"\n",
        "    Sampler for DQN hyperparams.\n",
        "    :param trial:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    learning_rate = trial.suggest_discrete_uniform('lr', 6.3e-4, 6.5e-4, 0.1e-4)\n",
        "\n",
        "    # net_arch = trial.suggest_categorical(\"net_arch\", [\"two\", \"three\"]) #, \"two_big\"\n",
        "    # net_arch = {\"two\": [64, 64], \"three\": [64, 64, 64]}[net_arch] #, \"two_big\": [256, 256]\n",
        "\n",
        "    hyperparams = {\n",
        "        \"learning_rate\": learning_rate,#\n",
        "        \"policy_kwargs\": dict(activation_fn=torch.nn.ReLU, net_arch=[64, 64]),\n",
        "        \"batch_size\": 128,  #for simplicity, we are not doing batch update.\n",
        "        \"buffer_size\": 50000, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
        "        \"learning_starts\": 0, #learning starts immediately!\n",
        "        \"gamma\": 0.99, #discount facto. range is between 0 and 1.\n",
        "        \"tau\": 1,  #the soft update coefficient for updating the target network\n",
        "        \"target_update_interval\": 250, #update the target network immediately.\n",
        "        \"train_freq\": (4,\"step\"), #train the network at every step.\n",
        "        \"max_grad_norm\": 10, #the maximum value for the gradient clipping\n",
        "        \"exploration_initial_eps\": 1, #initial value of random action probability\n",
        "        \"exploration_fraction\": 0.12, #fraction of entire training period over which the exploration rate is reduced\n",
        "        \"gradient_steps\": -1, #number of gradient steps\n",
        "        \"seed\": 1, #seed for the pseudo random generators\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "mWhSf_qjk8T4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define callback"
      ],
      "metadata": {
        "id": "fFyJprz6tp7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingCallback:\n",
        "    def __init__(self,threshold,trial_number,patience):\n",
        "      '''\n",
        "      threshold: int tolerance for increase in reward\n",
        "      trial_number: int Prune after minimum number of trials\n",
        "      patience: int patience for the threshold\n",
        "      '''\n",
        "      self.threshold = threshold\n",
        "      self.trial_number  = trial_number\n",
        "      self.patience = patience\n",
        "      self.cb_list = [] #Trials list for which threshold is reached\n",
        "    def __call__(self,study:optuna.study, frozen_trial:optuna.Trial):\n",
        "      #Setting the best value in the current trial\n",
        "      study.set_user_attr(\"previous_best_value\", study.best_value)\n",
        "      \n",
        "      #Checking if the minimum number of trials have pass\n",
        "      if frozen_trial.number >self.trial_number:\n",
        "          previous_best_value = study.user_attrs.get(\"previous_best_value\",None)\n",
        "          #Checking if the previous and current objective values have the same sign\n",
        "          if previous_best_value * study.best_value >=0:\n",
        "              #Checking for the threshold condition\n",
        "              if abs(previous_best_value-study.best_value) < self.threshold: \n",
        "                  self.cb_list.append(frozen_trial.number)\n",
        "                  #If threshold is achieved for the patience amount of time\n",
        "                  if len(self.cb_list)>self.patience:\n",
        "                      print('The study stops now...')\n",
        "                      print('With number',frozen_trial.number ,'and value ',frozen_trial.value)\n",
        "                      print('The previous and current best values are {} and {} respectively'\n",
        "                              .format(previous_best_value, study.best_value))\n",
        "                      study.stop()"
      ],
      "metadata": {
        "id": "Q4FKnTdKttOu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define template objective"
      ],
      "metadata": {
        "id": "B1qV393bpoTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = drive_path #\"models\"\n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "\n",
        "  model_log_dir = log_dir+'dqn_{}'.format(trial.number)+'/'\n",
        "  os.makedirs(model_log_dir,exist_ok=True) \n",
        "  # Create environment\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env = stable_baselines3.common.monitor.Monitor(env, filename= model_log_dir)\n",
        "\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "  hyperparameters = sample_dqn_params(trial)\n",
        "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "  # define learning steps\n",
        "  trained_dqn = model_dqn.learn(total_timesteps=1000, log_interval=10)#100000 , callback=callback\n",
        "  # save model\n",
        "  trained_dqn.save(model_log_dir +'dqn_{}.zip'.format(trial.number)) \n",
        "\n",
        "  x, y = ts2xy(load_results(model_log_dir), 'timesteps') # timesteps\n",
        "  # clear_output(wait=True)\n",
        "  #For the given hyperparamters, determine reward\n",
        "  reward = sum(y)\n",
        "  return reward\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize reward\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=2, catch=(ValueError,),callbacks=[logging_callback])"
      ],
      "metadata": {
        "id": "2s-bjWndt_a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42c3614-4a36-4756-e21d-7f08b0ad2d34"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-07-26 21:38:49,875]\u001b[0m A new study created in memory with name: dqn_study\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:548: UserWarning: The distribution is specified by [0.001, 0.01] and q=0.005, but the range is not divisible by `q`. It will be replaced by [0.001, 0.006].\n",
            "  low=low, old_high=old_high, high=high, step=q\n",
            "\u001b[32m[I 2022-07-26 21:38:54,141]\u001b[0m Trial 0 finished with value: -2644.398608 and parameters: {'lr': 0.001}. Best is trial 0 with value: -2644.398608.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:548: UserWarning: The distribution is specified by [0.001, 0.01] and q=0.005, but the range is not divisible by `q`. It will be replaced by [0.001, 0.006].\n",
            "  low=low, old_high=old_high, high=high, step=q\n",
            "\u001b[32m[I 2022-07-26 21:38:58,410]\u001b[0m Trial 1 finished with value: -2592.8647579999997 and parameters: {'lr': 0.006}. Best is trial 1 with value: -2592.8647579999997.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get the best model"
      ],
      "metadata": {
        "id": "tf91UWqvzR6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the best hyperparamters\n",
        "print('Hyperparameters after tuning',study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcDma0wSy02u",
        "outputId": "f344e12a-5643-4ea6-82d8-406ca2d2c7e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters after tuning {'lr': 0.006}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqou4GOlzWMh",
        "outputId": "04a08056-d494-42a8-d379-18cf740deafc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenTrial(number=1, values=[-2592.8647579999997], datetime_start=datetime.datetime(2022, 7, 26, 21, 38, 54, 143016), datetime_complete=datetime.datetime(2022, 7, 26, 21, 38, 58, 410379), params={'lr': 0.006}, distributions={'lr': DiscreteUniformDistribution(high=0.006, low=0.001, q=0.005)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1, state=TrialState.COMPLETE, value=None)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build three tasks for transfer learning\n"
      ],
      "metadata": {
        "id": "M5vgItPPhsw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://towardsdatascience.com/beginners-guide-to-custom-environments-in-openai-s-gym-989371673952"
      ],
      "metadata": {
        "id": "yIFSDq7eyXY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### define new env class"
      ],
      "metadata": {
        "id": "c-DTrZhLy0UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.box2d import LunarLander\n",
        "from Box2D.b2 import fixtureDef, circleShape, polygonShape, revoluteJointDef, contactListener, edgeShape\n",
        "import math"
      ],
      "metadata": {
        "id": "dTxYseql3ry3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set parameters"
      ],
      "metadata": {
        "id": "MtUy074Syhdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400"
      ],
      "metadata": {
        "id": "E6Ivff2jW7q8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### baselien with new reward"
      ],
      "metadata": {
        "id": "Hk3RJY11A0d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_LunarLander_reward(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "        \n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0: # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "0DW6wn84SBWf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### with wind and new reward "
      ],
      "metadata": {
        "id": "095bT9dTPnAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_LunarLander_wind(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "        self.wind_power = wind_power\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "\n",
        "\n",
        "        if self.enable_wind and not (\n",
        "            self.legs[0].ground_contact or self.legs[1].ground_contact\n",
        "        ):\n",
        "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            wind_mag = (\n",
        "                math.tanh(\n",
        "                    math.sin(0.02 * self.wind_idx)\n",
        "                    + (math.sin(math.pi * 0.01 * self.wind_idx))\n",
        "                )\n",
        "                * self.wind_power\n",
        "            )\n",
        "            self.wind_idx += 1\n",
        "            print('calculated wind power:')\n",
        "            print(wind_mag)\n",
        "            self.lander.ApplyForceToCenter(\n",
        "                (wind_mag, 0.0),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0: # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "lUhMa-AOPrFG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### noisy observations"
      ],
      "metadata": {
        "id": "UboymNjfCt9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "from typing import Dict, List, Tuple, Type, Union\n",
        "\n",
        "import gym\n",
        "import torch as th\n",
        "from torch import nn\n",
        "\n",
        "from stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space\n",
        "from stable_baselines3.common.type_aliases import TensorDict\n",
        "from stable_baselines3.common.utils import get_device\n",
        "\n",
        "\n",
        "class BaseFeaturesExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class that represents a features extractor.\n",
        "    :param observation_space:\n",
        "    :param features_dim: Number of features extracted.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: gym.Space, features_dim: int = 0):\n",
        "        super().__init__()\n",
        "        assert features_dim > 0\n",
        "        self._observation_space = observation_space\n",
        "        self._features_dim = features_dim\n",
        "\n",
        "    @property\n",
        "    def features_dim(self) -> int:\n",
        "        return self._features_dim\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class NoisyLunarLander(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Feature extract that adds noise to the input.\n",
        "    :param observation_space:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: gym.Space):\n",
        "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        # add noise to observations (excluding leg contact)\n",
        "        #print('Adding input noise:')      \n",
        "        input_noise = torch.zeros(size=observations.shape)\n",
        "        input_noise[:,:6]= torch.normal(mean=0.0, std=0.005, size=(1,6))\n",
        "        #print(input_noise)\n",
        "        #print(input_noise.shape)\n",
        "        #print('Observations:')\n",
        "        #print(self.flatten(observations))\n",
        "        return_array = self.flatten(observations)  + input_noise\n",
        "        #print(\"Check noise added:\")\n",
        "        #print(return_array-self.flatten(observations))\n",
        "\n",
        "        return return_array\n"
      ],
      "metadata": {
        "id": "KRFgQSPYC3Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### with obstacle and new reward"
      ],
      "metadata": {
        "id": "hU5fazVEArJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        if (\n",
        "                self.env.lander == contact.fixtureA.body\n",
        "                or self.env.lander == contact.fixtureB.body\n",
        "        ):\n",
        "            self.env.game_over = True\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = False\n",
        "\n",
        "class Custom_LunarLander_obs(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        obs_coords = [10, 10],\n",
        "        enable_obstacle: bool = True\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "\n",
        "        self.obs_coords = obs_coords\n",
        "        self.enable_obstacle = enable_obstacle\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        # defining the polygon obstacle here:\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, \n",
        "                                  pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8  \n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
        "        )   \n",
        "\n",
        "    def reset(self):\n",
        "        self._destroy()\n",
        "        self.world.contactListener_keepref = ContactDetector(self)\n",
        "        self.world.contactListener = self.world.contactListener_keepref\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        # terrain\n",
        "        CHUNKS = 11\n",
        "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
        "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
        "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
        "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
        "        self.helipad_y = H / 4\n",
        "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
        "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
        "        smooth_y = [\n",
        "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
        "            for i in range(CHUNKS)\n",
        "        ]\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "\n",
        "        # defining the polygon obstacle here----------------------------------\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, \n",
        "                                  pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        self.sky_polys = []\n",
        "        for i in range(CHUNKS - 1):\n",
        "            p1 = (chunk_x[i], smooth_y[i])\n",
        "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
        "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
        "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
        "\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        initial_y = VIEWPORT_H / SCALE\n",
        "        self.lander = self.world.CreateDynamicBody(\n",
        "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
        "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
        "        self.lander.ApplyForceToCenter(\n",
        "            (\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "            ),\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        self.legs = []\n",
        "        for i in [-1, +1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001,\n",
        "                ),\n",
        "            )\n",
        "            leg.ground_contact = False\n",
        "            leg.color1 = (0.5, 0.4, 0.9)\n",
        "            leg.color2 = (0.3, 0.3, 0.5)\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=self.lander,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(0, 0),\n",
        "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
        "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
        "            )\n",
        "            if i == -1:\n",
        "                rjd.lowerAngle = (\n",
        "                        +0.9 - 0.5\n",
        "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
        "                rjd.upperAngle = +0.9\n",
        "            else:\n",
        "                rjd.lowerAngle = -0.9\n",
        "                rjd.upperAngle = -0.9 + 0.5\n",
        "            leg.joint = self.world.CreateJoint(rjd)\n",
        "            self.legs.append(leg)\n",
        "\n",
        "        self.drawlist = [self.lander] + self.legs\n",
        "        \n",
        "\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        # print([pos.x,pos.y])\n",
        "        vel = self.lander.linearVelocity\n",
        "        # print([vel.x,vel.y])\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        state_8 = (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2), # 8: x position\n",
        "        state_9 = (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2), # 9: y position\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        # distance_to_obstacle = np.sqrt((pos.x - (self.obs_coords[0] +\n",
        "        #                                     VIEWPORT_W / SCALE / 2)) ** 2 +\n",
        "        #                         (pos.y - (self.obs_coords[1] +\n",
        "\n",
        "        #                                   (self.helipad_y + LEG_DOWN / SCALE))) ** 2)\n",
        "        distance_to_obstacle = np.sqrt(state_8 * state_8 + state_9 * state_9)\n",
        "        # print((distance_to_obstacle <= (20 / SCALE)))\n",
        "        # if (distance_to_obstacle <= (1)):\n",
        "        #     print('dangerously close to obstacle!')\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            - 150 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - -50 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "            - 50 * (distance_to_obstacle <= (20 / SCALE)) # obstacles \n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0: # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        from gym.envs.classic_control import rendering\n",
        "\n",
        "        if self.viewer is None:\n",
        "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
        "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "        # print('drawlist')\n",
        "        # print(self.drawlist)\n",
        "        for p in self.sky_polys:\n",
        "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
        "        # editing below line to draw obstacle\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((100, 100))  # Position\n",
        "                    # self.viewer.draw_circle(20).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
        "\n",
        "        for obj2 in [self.obstacle]:\n",
        "            # print('rendering obstacle')\n",
        "            # print(obj2)\n",
        "            for f in obj2.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    # print('printing circle of radius')\n",
        "                    #t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    t = rendering.Transform((self.obs_coords[0], self.obs_coords[1]))\n",
        "                    # print(f.shape.radius)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((10, 10))  # Position\n",
        "                    # self.viewer.draw_circle(2).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj2.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj2.color2, linewidth=2)\n",
        "\n",
        "        for x in [self.helipad_x1, self.helipad_x2]:\n",
        "            flagy1 = self.helipad_y\n",
        "            flagy2 = flagy1 + 50 / SCALE\n",
        "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
        "            self.viewer.draw_polygon(\n",
        "                [\n",
        "                    (x, flagy2),\n",
        "                    (x, flagy2 - 10 / SCALE),\n",
        "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
        "                ],\n",
        "                color=(0.8, 0.8, 0),\n",
        "            )\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "7Nxu5rsZ4MH1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### register environment"
      ],
      "metadata": {
        "id": "DzZ83S3UzBYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### register baseline anv"
      ],
      "metadata": {
        "id": "8SD7DX6hoX7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = Custom_LunarLander_reward()\n",
        "new_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3abf00-4c9a-4836-d5fe-ab39b9b035b6",
        "id": "mAGafux6oX7P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Custom_LunarLander_wind at 0x7fbe3b8c9f50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "check_env(new_env)"
      ],
      "metadata": {
        "id": "PkBL7IZQoX7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander-v3\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_reward,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "WvFoF2FdoX7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### register obstacle anv"
      ],
      "metadata": {
        "id": "YJkdBNEHiLle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check environment"
      ],
      "metadata": {
        "id": "xGf0_9im0Y3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = Custom_LunarLander_obs()\n",
        "new_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "GdKLT2SiGCiV",
        "outputId": "1a84d175-dfd9-4685-eab9-2fa5e5ef797f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-63c2b47b5adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustom_LunarLander_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Custom_LunarLander_obs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_env(new_env)"
      ],
      "metadata": {
        "id": "kG6IM6zW2IPm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander_obs-v1\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_obs,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "C1cIXPTaJsQG",
        "outputId": "0b427a16-9648-4ea3-fa8b-a49860a25ec8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-118a5885058b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# path to the class for creating the env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Note: entry_point also accept a class as input (and not only a string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCustom_LunarLander_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Max number of steps per episode, using a `TimeLimitWrapper`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Custom_LunarLander_obs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### define and train model in new env"
      ],
      "metadata": {
        "id": "KdG01o1IzxGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model for baseline env"
      ],
      "metadata": {
        "id": "0EfzaHwhoxTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "5J0MmqyPzq9q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "66e5baed-35fc-40ed-8f92-2d8cb8c6e0b9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aea702badd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#For example, if you would like to load Cartpole, just replace the above statement with \"env = gym.make('CartPole-v1')\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ],
      "source": [
        "log_dir = drive_path+\"baseline/\" #\"models\"\n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "\n",
        "  model_log_dir = log_dir+'dqn_{}'.format(trial.number)+'/'\n",
        "  os.makedirs(model_log_dir,exist_ok=True) \n",
        "  # Create environment\n",
        "  env = gym.make('LunarLander-v3')\n",
        "  env = stable_baselines3.common.monitor.Monitor(env, filename= model_log_dir)\n",
        "  callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.\n",
        "\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "  hyperparameters = sample_dqn_params(trial)\n",
        "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "  # define learning steps\n",
        "  model1 = model_dqn.learn(total_timesteps=1000, log_interval=10, callback=callback)#100000 \n",
        "  # save model\n",
        "  model1.save(model_log_dir +'dqn_{}.zip'.format(trial.number)) \n",
        "\n",
        "  x, y = ts2xy(load_results(model_log_dir), 'timesteps') # timesteps\n",
        "  # clear_output(wait=True)\n",
        "  #For the given hyperparamters, determine reward\n",
        "  reward = sum(y)\n",
        "  return reward\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize reward\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=2, catch=(ValueError,),callbacks=[logging_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_env = wrap_env(gym.make(\"LunarLander-v3\"))\n",
        "observation = new_env.reset()\n",
        "total_reward = 0\n",
        "\n",
        "while True:\n",
        "  new_env.render()\n",
        "  action, states = model1.predict(observation, deterministic=True)\n",
        "  observation, reward, done, info = new_env.step(action)\n",
        "  total_reward += reward\n",
        "  if done:\n",
        "    break;\n",
        "\n",
        "# print(total_reward)\n",
        "new_env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "id": "M63a8xiTJHJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = ts2xy(load_results(log_dir), 'timesteps')  # Organising the logged results in to a clean format for plotting. timesteps episodes\n",
        "plt.plot(x,y)\n",
        "plt.ylim([-300, 300])\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Episode Rewards')"
      ],
      "metadata": {
        "id": "W_NWJn78Je_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model for noisy environment env"
      ],
      "metadata": {
        "id": "O7w0kpmGDK6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using stable baseline zoo parameter as ref\n",
        "def sample_dqn_params_noisy(trial: optuna.Trial):\n",
        "    \"\"\"\n",
        "    Sampler for DQN hyperparams.\n",
        "    :param trial:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    learning_rate = trial.suggest_discrete_uniform('lr', 6.3e-4, 6.5e-4, 0.1e-4)\n",
        "\n",
        "    # net_arch = trial.suggest_categorical(\"net_arch\", [\"two\", \"three\"]) #, \"two_big\"\n",
        "    # net_arch = {\"two\": [64, 64], \"three\": [64, 64, 64]}[net_arch] #, \"two_big\": [256, 256]\n",
        "\n",
        "    hyperparams = {\n",
        "        \"learning_rate\": learning_rate,#\n",
        "        \"policy_kwargs\": dict(activation_fn=torch.nn.ReLU, net_arch=[64, 64], features_extractor_class = NoisyLunarLander),\n",
        "        \"batch_size\": 128,  #for simplicity, we are not doing batch update.\n",
        "        \"buffer_size\": 50000, #size of experience of replay buffer. Set to 1 as batch update is not done\n",
        "        \"learning_starts\": 0, #learning starts immediately!\n",
        "        \"gamma\": 0.99, #discount facto. range is between 0 and 1.\n",
        "        \"tau\": 1,  #the soft update coefficient for updating the target network\n",
        "        \"target_update_interval\": 250, #update the target network immediately.\n",
        "        \"train_freq\": (4,\"step\"), #train the network at every step.\n",
        "        \"max_grad_norm\": 10, #the maximum value for the gradient clipping\n",
        "        \"exploration_initial_eps\": 1, #initial value of random action probability\n",
        "        \"exploration_fraction\": 0.12, #fraction of entire training period over which the exploration rate is reduced\n",
        "        \"gradient_steps\": -1, #number of gradient steps\n",
        "        \"seed\": 1, #seed for the pseudo random generators\n",
        "        \"verbose\": 0\n",
        "    }\n",
        "    return hyperparams"
      ],
      "metadata": {
        "id": "1yZ6wLURIrdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "66e5baed-35fc-40ed-8f92-2d8cb8c6e0b9",
        "id": "ZIBaMrgbDK6Q"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aea702badd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#For example, if you would like to load Cartpole, just replace the above statement with \"env = gym.make('CartPole-v1')\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ],
      "source": [
        "log_dir = drive_path+\"noisy/\" #\"models\"\n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "\n",
        "  model_log_dir = log_dir+'dqn_{}'.format(trial.number)+'/'\n",
        "  os.makedirs(model_log_dir,exist_ok=True) \n",
        "  # Create environment\n",
        "  env = gym.make('LunarLander-v3')\n",
        "  env = stable_baselines3.common.monitor.Monitor(env, filename= model_log_dir)\n",
        "  callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.\n",
        "\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "  hyperparameters = sample_dqn_params_noisy(trial)\n",
        "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "  # define learning steps\n",
        "  trained_dqn = model_dqn.learn(total_timesteps=1000, log_interval=10, callback=callback)#100000 \n",
        "  # save model\n",
        "  trained_dqn.save(model_log_dir +'dqn_{}.zip'.format(trial.number)) \n",
        "\n",
        "  x, y = ts2xy(load_results(model_log_dir), 'timesteps') # timesteps\n",
        "  # clear_output(wait=True)\n",
        "  #For the given hyperparamters, determine reward\n",
        "  reward = sum(y)\n",
        "  return reward\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize reward\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=2, catch=(ValueError,),callbacks=[logging_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### model for obstacle env"
      ],
      "metadata": {
        "id": "rkJifcG-pibo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = drive_path+\"obstacle/\" #\"models\"\n",
        "\n",
        "def objective(trial:optuna.Trial):\n",
        "\n",
        "  model_log_dir = log_dir+'dqn_{}'.format(trial.number)+'/'\n",
        "  os.makedirs(model_log_dir,exist_ok=True) \n",
        "  # Create environment\n",
        "  env = gym.make('LunarLander_obs-v1')\n",
        "  env = stable_baselines3.common.monitor.Monitor(env, filename= model_log_dir)\n",
        "  callback = EvalCallback(env,log_path = log_dir, deterministic=True) #For evaluating the performance of the agent periodically and logging the results.\n",
        "\n",
        "  #Trial will suggest a set of hyperparamters from the specified range\n",
        "  hyperparameters = sample_dqn_params(trial)\n",
        "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
        "\n",
        "  # define learning steps\n",
        "  trained_dqn = model_dqn.learn(total_timesteps=1000, log_interval=10, callback=callback)#100000 \n",
        "  # save model\n",
        "  trained_dqn.save(model_log_dir +'dqn_{}.zip'.format(trial.number)) \n",
        "\n",
        "  x, y = ts2xy(load_results(model_log_dir), 'timesteps') # timesteps\n",
        "  # clear_output(wait=True)\n",
        "  #For the given hyperparamters, determine reward\n",
        "  reward = sum(y)\n",
        "  return reward\n",
        "\n",
        "#Create a study object and specify the direction as 'maximize'\n",
        "#As you want to maximize reward\n",
        "#Pruner stops not promising iterations\n",
        "#Use a pruner, else you will get error related to divergence of model\n",
        "#You can also use Multivariate samplere\n",
        "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
        "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
        "\n",
        "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
        "#You can increase the n_trials for a better search space scanning\n",
        "study.optimize(objective, n_trials=2, catch=(ValueError,),callbacks=[logging_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "-qmgq4cYo_vA",
        "outputId": "a113b292-5ee8-401d-f30c-3201ee19065c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7575250481b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander_obs-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#You can also load other environments like cartpole, MountainCar, Acrobot. Refer to https://gym.openai.com/docs/ for descriptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#For example, if you would like to load Cartpole, just replace the above statement with \"env = gym.make('CartPole-v1')\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ptpdnpwGobbz"
      },
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "1. Stable Baselines Framework: https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "\n",
        "2. Lunar Lander Environment: https://gym.openai.com/envs/LunarLander-v2/\n",
        "\n",
        "3. OpenAI gym environments: https://gym.openai.com/docs/\n",
        "\n",
        "4. A good reference for introduction to RL: http://incompleteideas.net/book/the-book-2nd.html\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lunar_lander_group1_atcheke",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}