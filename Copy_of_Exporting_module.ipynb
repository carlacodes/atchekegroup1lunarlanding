{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlacodes/atchekegroup1lunarlanding/blob/hasan/Copy_of_Exporting_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "9EanvrrhobbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac0bdcc-03cd-421c-be02-db0ac489ea3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 177 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 60.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 22.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 448 kB 6.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "pE3qZJLcobbW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor,RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPoZnSTGFvEp",
        "outputId": "e2b35a0a-3623-4c9a-8735-a7189f38e6bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "fp1bUnClobbY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "vJm_qzWkobbg"
      },
      "outputs": [],
      "source": [
        "nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
        "                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
        "\n",
        "learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.\n",
        "                      #Tip: Use smaller step-sizes for larger networks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the obstacle Env for Fine tuning "
      ],
      "metadata": {
        "id": "zb9VfXeFLkbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.box2d import LunarLander\n",
        "from Box2D.b2 import fixtureDef, circleShape, polygonShape, revoluteJointDef, contactListener, edgeShape\n",
        "import math"
      ],
      "metadata": {
        "id": "NfxblZzztMHP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400"
      ],
      "metadata": {
        "id": "E6Ivff2jW7q8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        if (\n",
        "                self.env.lander == contact.fixtureA.body\n",
        "                or self.env.lander == contact.fixtureB.body\n",
        "        ):\n",
        "            self.env.game_over = True\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = False\n",
        "\n",
        "class Custom_LunarLander_obs(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        obs_coords = [10, 10],\n",
        "        enable_obstacle: bool = True\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "\n",
        "        self.obs_coords = obs_coords\n",
        "        self.enable_obstacle = enable_obstacle\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        # defining the polygon obstacle here:\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, \n",
        "                                  pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8  \n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
        "        )   \n",
        "\n",
        "    def reset(self):\n",
        "        self._destroy()\n",
        "        self.world.contactListener_keepref = ContactDetector(self)\n",
        "        self.world.contactListener = self.world.contactListener_keepref\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        # terrain\n",
        "        CHUNKS = 11\n",
        "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
        "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
        "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
        "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
        "        self.helipad_y = H / 4\n",
        "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
        "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
        "        smooth_y = [\n",
        "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
        "            for i in range(CHUNKS)\n",
        "        ]\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "\n",
        "        # defining the polygon obstacle here----------------------------------\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        self.sky_polys = []\n",
        "        for i in range(CHUNKS - 1):\n",
        "            p1 = (chunk_x[i], smooth_y[i])\n",
        "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
        "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
        "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
        "\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        initial_y = VIEWPORT_H / SCALE\n",
        "        self.lander = self.world.CreateDynamicBody(\n",
        "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
        "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
        "        self.lander.ApplyForceToCenter(\n",
        "            (\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "            ),\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        self.legs = []\n",
        "        for i in [-1, +1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001,\n",
        "                ),\n",
        "            )\n",
        "            leg.ground_contact = False\n",
        "            leg.color1 = (0.5, 0.4, 0.9)\n",
        "            leg.color2 = (0.3, 0.3, 0.5)\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=self.lander,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(0, 0),\n",
        "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
        "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
        "            )\n",
        "            if i == -1:\n",
        "                rjd.lowerAngle = (\n",
        "                        +0.9 - 0.5\n",
        "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
        "                rjd.upperAngle = +0.9\n",
        "            else:\n",
        "                rjd.lowerAngle = -0.9\n",
        "                rjd.upperAngle = -0.9 + 0.5\n",
        "            leg.joint = self.world.CreateJoint(rjd)\n",
        "            self.legs.append(leg)\n",
        "\n",
        "        self.drawlist = [self.lander] + self.legs\n",
        "        \n",
        "\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        # print([pos.x,pos.y])\n",
        "        vel = self.lander.linearVelocity\n",
        "      \n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "\n",
        "            # (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2), # 8: x position\n",
        "            # (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2), # 9: y position\n",
        "\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        state_8 = (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2);\n",
        "        state_9 = (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2);\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        # distance_to_obstacle = np.sqrt((pos.x - (self.obs_coords[0] +\n",
        "        #                                     VIEWPORT_W / SCALE / 2)) ** 2 +\n",
        "        #                         (pos.y - (self.obs_coords[1] +\n",
        "\n",
        "        #                                   (self.helipad_y + LEG_DOWN / SCALE))) ** 2)\n",
        "        distance_to_obstacle = np.sqrt(state_8 * state_8 + state_9 * state_9)\n",
        "\n",
        "        # if (distance_to_obstacle <= (1)):\n",
        "        #     print('dangerously close to obstacle!')\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            - 125 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - 75 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "            - 75 * (distance_to_obstacle <= ((20 + 10) / SCALE)) #-125 obstacles radius and the polly radius  \n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0 : # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        from gym.envs.classic_control import rendering\n",
        "\n",
        "        if self.viewer is None:\n",
        "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
        "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "        # print('drawlist')\n",
        "        # print(self.drawlist)\n",
        "        for p in self.sky_polys:\n",
        "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
        "        # editing below line to draw obstacle\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((100, 100))  # Position\n",
        "                    # self.viewer.draw_circle(20).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
        "\n",
        "        for obj2 in [self.obstacle]:\n",
        "            # print('rendering obstacle')\n",
        "            # print(obj2)\n",
        "            for f in obj2.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    # print('printing circle of radius')\n",
        "                    #t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    t = rendering.Transform((self.obs_coords[0], self.obs_coords[1]))\n",
        "                    # print(f.shape.radius)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((10, 10))  # Position\n",
        "                    # self.viewer.draw_circle(2).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj2.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj2.color2, linewidth=2)\n",
        "\n",
        "        for x in [self.helipad_x1, self.helipad_x2]:\n",
        "            flagy1 = self.helipad_y\n",
        "            flagy2 = flagy1 + 50 / SCALE\n",
        "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
        "            self.viewer.draw_polygon(\n",
        "                [\n",
        "                    (x, flagy2),\n",
        "                    (x, flagy2 - 10 / SCALE),\n",
        "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
        "                ],\n",
        "                color=(0.8, 0.8, 0),\n",
        "            )\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "MAwXYRAQhpvg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env2 = Custom_LunarLander_obs()\n",
        "new_env2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee666fce-5c24-48cc-8517-2c00a044cee3",
        "id": "dxrSUxAgiRPg"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Custom_LunarLander_obs at 0x7f96eb019210>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "check_env(new_env2)"
      ],
      "metadata": {
        "id": "5yacX9tliV_Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for env in gym.envs.registration.registry.env_specs.copy():\n",
        "#     if 'LunarLander-v3' in env:\n",
        "#         print(\"Remove {} from registry\".format(env))\n",
        "#         del gym.envs.registration.registry.env_specs[env]"
      ],
      "metadata": {
        "id": "nNOEgfEolroH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander-v4\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_obs,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "pI_O6IZfi6RW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting the Baseline Model to Pytorch and Fine tuning the Last Layer Using the Obstacle Environment"
      ],
      "metadata": {
        "id": "-zXPpu7LQpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "print('run pytorch model')\n",
        "import gym\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from stable_baselines3 import DQN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLbWfW6K8tpF",
        "outputId": "d2d5d11b-59d0-4632-b7f4-6d04ea9033d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run pytorch model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/Hasanaldhahi3/atchekegroup1lunarlanding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Og3pc49huE",
        "outputId": "86d31752-9a23-4ade-f165-ccbd9d3c313c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'atchekegroup1lunarlanding'...\n",
            "remote: Enumerating objects: 739, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 739 (delta 55), reused 42 (delta 42), pack-reused 678\u001b[K\n",
            "Receiving objects: 100% (739/739), 24.48 MiB | 31.49 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "log_dir_ = dir_prefix + \"DQN_Youtube/\"\n",
        "os.makedirs(log_dir_, exist_ok=True)\n",
        "spec=importlib.util.spec_from_file_location(\"DeepQNetwork\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_1 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_1)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"plotLearning\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/utils.py\")\n",
        "foo_2 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_2)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"Agent\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_3 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_3)\n"
      ],
      "metadata": {
        "id": "CsJZwSVj9U_e"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_prefix = \"./files/\"\n",
        "log_dir_obstacle = dir_prefix + \"DQN_fine_tuned_files/\"\n",
        "os.makedirs(log_dir_obstacle, exist_ok=True)"
      ],
      "metadata": {
        "id": "K5MHt4nN0SzO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining an Agent class with decaying Learning rate and Decaying epsilon (explotation ratio)"
      ],
      "metadata": {
        "id": "pb62pmk5RL50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
        "                 n_actions, parameters):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "        self.parameters = parameters;\n",
        "\n",
        "        print(self.parameters)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters, lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions = self.fc3(x)\n",
        "\n",
        "        return actions"
      ],
      "metadata": {
        "id": "nMxnRqvjUr9b"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent_lr(foo_3.Agent):\n",
        "  def __init__(self,parameters, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
        "                max_mem_size=100000, eps_end=0.05, eps_dec=5e-4, ):\n",
        "    self.parameters = parameters;\n",
        "    foo_3.Agent.__init__(self,gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
        "                  max_mem_size=100000, eps_end=0.05, eps_dec=5e-4);\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.eps_min = eps_end\n",
        "    self.eps_dec = eps_dec\n",
        "    self.lr = lr\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.mem_size = max_mem_size\n",
        "    self.batch_size = batch_size\n",
        "    self.mem_cntr = 0\n",
        "    self.iter_cntr = 0\n",
        "    self.replace_target = 100\n",
        "    self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
        "                                input_dims=input_dims,\n",
        "                                fc1_dims=256, fc2_dims=256, parameters = self.parameters)\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
        "                                  dtype=np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
        "                                      dtype=np.float32)\n",
        "    self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "  \n",
        "\n",
        "  def learn(self):\n",
        "    if self.mem_cntr < self.batch_size:\n",
        "        return\n",
        "\n",
        "    self.Q_eval.optimizer.zero_grad()\n",
        "\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "    batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "    batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "    new_state_batch = T.tensor(\n",
        "            self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "    action_batch = self.action_memory[batch]\n",
        "    reward_batch = T.tensor(\n",
        "            self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "    terminal_batch = T.tensor(\n",
        "            self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "\n",
        "    q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "    q_next = self.Q_eval.forward(new_state_batch)\n",
        "    q_next[terminal_batch] = 0.0\n",
        "\n",
        "    q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
        "\n",
        "    loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
        "    loss.backward()\n",
        "    self.Q_eval.optimizer.step()\n",
        "   \n",
        "\n",
        "\n",
        "    self.iter_cntr += 1\n",
        "    self.epsilon = self.epsilon - self.eps_dec  if self.epsilon > self.eps_min else self.eps_min\n",
        "    self.lr = self.lr - self.eps_dec if self.lr > 0.001 else 0.001\n",
        "        \n"
      ],
      "metadata": {
        "id": "UvI6zPH8BY8j"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model 1 the Baseline Model weights and exporting its weight to pytorch "
      ],
      "metadata": {
        "id": "F9WY06U1Qjwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "import gym\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/baseline_pretrained.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "print(dqn_torch_v.parameters())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for param in dqn_torch_v.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = 64  # 8 states we have for the polly to move \n",
        "num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "dqn_torch_v.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v.to(device)\n",
        "# optimizer_1 = torch.optim.Adam(dqn_torch_v.parameters(), lr=1e-2)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def obs_to_torch(obs):\n",
        "    # TF: NHWC\n",
        "    # PyTorch: NCHW\n",
        "    # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "    # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "    # # Normalize\n",
        "    # obs = obs / 255.0\n",
        "    obs = th.tensor(obs).float()\n",
        "    obs = obs.to(device)\n",
        "    return obs\n",
        "\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "print(next(dqn_torch_v.parameters()).device)\n",
        "while not done:\n",
        "    action = th.argmax(dqn_torch_v(obs_to_torch(obs))).item()\n",
        "    # action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "\n",
        "print(episode_reward)"
      ],
      "metadata": {
        "id": "47vjrBZP9FAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e4c0ec-8129-4e72-b034-06d78cf8b6e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Is CUDA supported by this system?True\n",
            "CUDA version: 11.3\n",
            "ID of current CUDA device:0\n",
            "cuda:0\n",
            "loaded model\n",
            "Parameter containing:\n",
            "tensor([[-0.2699,  0.2215, -0.2805,  ...,  0.0768,  0.2879,  0.3533],\n",
            "        [-0.0257, -0.2925, -0.0951,  ...,  0.1328,  0.1329, -0.3405],\n",
            "        [ 0.3069,  0.2371, -0.0466,  ..., -0.2950,  0.3136, -0.1654],\n",
            "        ...,\n",
            "        [ 0.2993, -0.2371, -0.1341,  ...,  0.1764,  0.0587, -0.2268],\n",
            "        [ 0.1419, -0.3017,  0.3421,  ...,  0.3309,  0.3151, -0.1235],\n",
            "        [-0.2089, -0.1852, -0.3029,  ..., -0.1262, -0.1147,  0.2384]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([-0.0336,  0.2579,  0.3049, -0.3168, -0.0405,  0.0551,  0.0542,  0.2759,\n",
            "         0.3215, -0.2255,  0.0892, -0.3376, -0.1296,  0.0549,  0.0086,  0.3483,\n",
            "        -0.3105, -0.0151, -0.1516, -0.0812,  0.2399, -0.0945, -0.0795,  0.1701,\n",
            "         0.0569, -0.1270,  0.1957,  0.0594,  0.0798,  0.3439,  0.2332,  0.2575,\n",
            "        -0.3462,  0.2634, -0.3212, -0.1445,  0.2926,  0.1328, -0.0704,  0.3341,\n",
            "        -0.2070,  0.2270, -0.1216,  0.3309,  0.3299, -0.3321, -0.3431, -0.1876,\n",
            "         0.2460, -0.2405, -0.0594,  0.1098,  0.0619, -0.1331, -0.0226,  0.0106,\n",
            "         0.2693, -0.2799, -0.1559,  0.1557, -0.3094, -0.0167, -0.1749, -0.2252,\n",
            "        -0.2997, -0.0884,  0.2729,  0.1063,  0.2781,  0.0420, -0.1359, -0.1636,\n",
            "        -0.1561, -0.1241, -0.1332,  0.1855, -0.3280,  0.1731,  0.2616,  0.0994,\n",
            "        -0.0322,  0.0901, -0.2865,  0.1353,  0.3408, -0.0473, -0.1434,  0.2903,\n",
            "         0.0677,  0.1557,  0.2872,  0.0385, -0.2873, -0.1876, -0.1646, -0.3522,\n",
            "        -0.1121,  0.0571,  0.0714,  0.2680, -0.0580, -0.2201, -0.2486,  0.2876,\n",
            "         0.0375, -0.0763, -0.0524, -0.1231,  0.2352,  0.2835, -0.1164,  0.3422,\n",
            "        -0.2345, -0.1577, -0.3509, -0.3399,  0.0260,  0.3076,  0.0837, -0.1969,\n",
            "         0.3497,  0.0974,  0.1489,  0.2827, -0.2747, -0.0516,  0.3323,  0.2814,\n",
            "         0.1320,  0.0699,  0.2809, -0.0457,  0.2268, -0.0130, -0.1604, -0.0925,\n",
            "        -0.3000, -0.0935,  0.0340,  0.0236, -0.1917,  0.0240,  0.2989,  0.0586,\n",
            "        -0.1174, -0.1682,  0.3185, -0.2086, -0.0210, -0.0636, -0.3340, -0.0155,\n",
            "         0.0991,  0.2144,  0.0567,  0.2103, -0.2173, -0.0564,  0.0525,  0.3312,\n",
            "         0.0679, -0.2402,  0.2016,  0.3393,  0.3408,  0.0808,  0.3329, -0.1862,\n",
            "        -0.1870, -0.2554,  0.3173, -0.2191,  0.0509, -0.2519, -0.2181, -0.1824,\n",
            "         0.0459,  0.2204,  0.2475,  0.0878,  0.0011,  0.2317,  0.3526, -0.0642,\n",
            "        -0.0062,  0.2188,  0.3320, -0.3254, -0.3214,  0.3010, -0.0947,  0.1782,\n",
            "        -0.3135, -0.0796,  0.2452, -0.1053,  0.2338,  0.1890, -0.1025,  0.3486,\n",
            "         0.0631,  0.0882, -0.0176,  0.1981, -0.3266,  0.2033,  0.0188, -0.3023,\n",
            "         0.2795,  0.3414,  0.1982, -0.3114, -0.0374,  0.3310,  0.2813, -0.3048,\n",
            "         0.0528, -0.1564, -0.1738, -0.0187, -0.1911, -0.2737,  0.2147, -0.2513,\n",
            "         0.2119, -0.0172, -0.1393, -0.1406, -0.0802, -0.1132, -0.0987, -0.0053,\n",
            "        -0.1186,  0.1322, -0.0319,  0.2155, -0.0330, -0.0953, -0.0801,  0.2992,\n",
            "         0.3217, -0.1825, -0.3405,  0.1533,  0.3523, -0.0699,  0.0331, -0.2227,\n",
            "        -0.1190,  0.1804, -0.0838,  0.2980,  0.1437,  0.3370,  0.3465,  0.0366],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "<generator object Module.parameters at 0x7f96e83f6250>\n",
            "cuda:0\n",
            "-157.11231508871305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "agent = Agent_lr(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "              input_dims=[8], lr=0.001,  eps_dec=2e-4, parameters= dqn_torch_v.parameters())\n",
        "scores_1, eps_history = [], []\n",
        "n_games = 250\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "      action = agent.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent.store_transition(observation, action, reward, \n",
        "                              observation_, done)\n",
        "      agent.learn()\n",
        "      observation = observation_\n",
        "    scores_1.append(score)\n",
        "    eps_history.append(agent.epsilon)\n",
        "\n",
        "    avg_score = np.mean(scores_1[-100:])\n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "            'average score %.2f' % avg_score,\n",
        "            'epsilon %.2f' % agent.epsilon)\n",
        "x_1 = [i+1 for i in range(n_games)]\n",
        "filename = 'lunar_lander.png'\n",
        "foo_2.plotLearning(x_1, scores_1, eps_history, filename)\n",
        "    # x_np = np.array(x)[indices.astype(int)]\n",
        "    # performance_metrics(x_np,scores)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oMihRr6Q0sES",
        "outputId": "3643bc19-7bf0-48f2-9122-0f7d0bfd3034"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score -111.32 average score -111.32 epsilon 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  state = T.tensor([observation]).to(self.Q_eval.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  1 score -185.06 average score -148.19 epsilon 0.97\n",
            "episode  2 score -315.99 average score -204.12 epsilon 0.96\n",
            "episode  3 score -421.33 average score -258.42 epsilon 0.94\n",
            "episode  4 score -491.36 average score -305.01 epsilon 0.92\n",
            "episode  5 score -89.08 average score -269.02 epsilon 0.90\n",
            "episode  6 score -37.61 average score -235.96 epsilon 0.88\n",
            "episode  7 score -437.55 average score -261.16 epsilon 0.86\n",
            "episode  8 score -89.42 average score -242.08 epsilon 0.85\n",
            "episode  9 score -338.41 average score -251.71 epsilon 0.83\n",
            "episode  10 score -184.40 average score -245.59 epsilon 0.81\n",
            "episode  11 score -213.13 average score -242.89 epsilon 0.79\n",
            "episode  12 score -28.93 average score -226.43 epsilon 0.77\n",
            "episode  13 score -370.17 average score -236.70 epsilon 0.75\n",
            "episode  14 score -99.58 average score -227.56 epsilon 0.74\n",
            "episode  15 score -55.37 average score -216.80 epsilon 0.73\n",
            "episode  16 score -202.38 average score -215.95 epsilon 0.71\n",
            "episode  17 score -98.56 average score -209.43 epsilon 0.69\n",
            "episode  18 score -473.81 average score -223.34 epsilon 0.68\n",
            "episode  19 score -64.64 average score -215.41 epsilon 0.66\n",
            "episode  20 score -312.89 average score -220.05 epsilon 0.64\n",
            "episode  21 score -474.77 average score -231.63 epsilon 0.62\n",
            "episode  22 score -199.35 average score -230.22 epsilon 0.61\n",
            "episode  23 score -441.61 average score -239.03 epsilon 0.59\n",
            "episode  24 score -289.19 average score -241.04 epsilon 0.58\n",
            "episode  25 score -512.48 average score -251.48 epsilon 0.56\n",
            "episode  26 score -520.47 average score -261.44 epsilon 0.55\n",
            "episode  27 score -75.40 average score -254.80 epsilon 0.54\n",
            "episode  28 score -529.30 average score -264.26 epsilon 0.52\n",
            "episode  29 score -208.08 average score -262.39 epsilon 0.50\n",
            "episode  30 score -93.54 average score -256.94 epsilon 0.49\n",
            "episode  31 score -514.36 average score -264.99 epsilon 0.47\n",
            "episode  32 score -486.51 average score -271.70 epsilon 0.46\n",
            "episode  33 score -407.66 average score -275.70 epsilon 0.44\n",
            "episode  34 score -496.53 average score -282.01 epsilon 0.42\n",
            "episode  35 score -261.90 average score -281.45 epsilon 0.41\n",
            "episode  36 score -338.59 average score -282.99 epsilon 0.39\n",
            "episode  37 score -484.54 average score -288.30 epsilon 0.38\n",
            "episode  38 score -538.53 average score -294.71 epsilon 0.36\n",
            "episode  39 score -89.00 average score -289.57 epsilon 0.35\n",
            "episode  40 score -382.83 average score -291.85 epsilon 0.33\n",
            "episode  41 score -68.87 average score -286.54 epsilon 0.32\n",
            "episode  42 score -385.91 average score -288.85 epsilon 0.31\n",
            "episode  43 score -451.75 average score -292.55 epsilon 0.29\n",
            "episode  44 score -454.13 average score -296.14 epsilon 0.28\n",
            "episode  45 score -537.84 average score -301.40 epsilon 0.27\n",
            "episode  46 score -79.31 average score -296.67 epsilon 0.25\n",
            "episode  47 score -242.00 average score -295.53 epsilon 0.24\n",
            "episode  48 score 33.69 average score -288.81 epsilon 0.23\n",
            "episode  49 score -450.47 average score -292.05 epsilon 0.22\n",
            "episode  50 score -616.57 average score -298.41 epsilon 0.20\n",
            "episode  51 score -480.33 average score -301.91 epsilon 0.19\n",
            "episode  52 score -374.22 average score -303.27 epsilon 0.17\n",
            "episode  53 score -83.39 average score -299.20 epsilon 0.16\n",
            "episode  54 score -436.67 average score -301.70 epsilon 0.15\n",
            "episode  55 score -485.97 average score -304.99 epsilon 0.13\n",
            "episode  56 score -318.76 average score -305.23 epsilon 0.12\n",
            "episode  57 score -564.08 average score -309.69 epsilon 0.10\n",
            "episode  58 score -635.84 average score -315.22 epsilon 0.09\n",
            "episode  59 score -571.18 average score -319.49 epsilon 0.07\n",
            "episode  60 score -418.34 average score -321.11 epsilon 0.06\n",
            "episode  61 score -598.65 average score -325.58 epsilon 0.04\n",
            "episode  62 score -578.70 average score -329.60 epsilon 0.03\n",
            "episode  63 score -587.99 average score -333.64 epsilon 0.02\n",
            "episode  64 score -369.17 average score -334.19 epsilon 0.01\n",
            "episode  65 score -609.33 average score -338.36 epsilon 0.01\n",
            "episode  66 score -605.36 average score -342.34 epsilon 0.01\n",
            "episode  67 score -63.63 average score -338.24 epsilon 0.01\n",
            "episode  68 score -545.98 average score -341.25 epsilon 0.01\n",
            "episode  69 score -501.41 average score -343.54 epsilon 0.01\n",
            "episode  70 score -650.19 average score -347.86 epsilon 0.01\n",
            "episode  71 score -465.18 average score -349.49 epsilon 0.01\n",
            "episode  72 score -526.02 average score -351.91 epsilon 0.01\n",
            "episode  73 score -535.30 average score -354.39 epsilon 0.01\n",
            "episode  74 score -493.31 average score -356.24 epsilon 0.01\n",
            "episode  75 score -228.06 average score -354.55 epsilon 0.01\n",
            "episode  76 score -723.32 average score -359.34 epsilon 0.01\n",
            "episode  77 score -311.02 average score -358.72 epsilon 0.01\n",
            "episode  78 score -606.01 average score -361.85 epsilon 0.01\n",
            "episode  79 score -184.28 average score -359.63 epsilon 0.01\n",
            "episode  80 score -419.02 average score -360.36 epsilon 0.01\n",
            "episode  81 score -540.02 average score -362.56 epsilon 0.01\n",
            "episode  82 score -391.37 average score -362.90 epsilon 0.01\n",
            "episode  83 score -546.41 average score -365.09 epsilon 0.01\n",
            "episode  84 score -611.49 average score -367.99 epsilon 0.01\n",
            "episode  85 score -526.18 average score -369.83 epsilon 0.01\n",
            "episode  86 score -617.40 average score -372.67 epsilon 0.01\n",
            "episode  87 score -339.97 average score -372.30 epsilon 0.01\n",
            "episode  88 score -51.08 average score -368.69 epsilon 0.01\n",
            "episode  89 score -553.45 average score -370.74 epsilon 0.01\n",
            "episode  90 score -555.21 average score -372.77 epsilon 0.01\n",
            "episode  91 score -678.52 average score -376.09 epsilon 0.01\n",
            "episode  92 score -319.50 average score -375.49 epsilon 0.01\n",
            "episode  93 score -317.25 average score -374.87 epsilon 0.01\n",
            "episode  94 score -362.41 average score -374.73 epsilon 0.01\n",
            "episode  95 score -543.46 average score -376.49 epsilon 0.01\n",
            "episode  96 score -498.84 average score -377.75 epsilon 0.01\n",
            "episode  97 score -395.57 average score -377.94 epsilon 0.01\n",
            "episode  98 score -599.40 average score -380.17 epsilon 0.01\n",
            "episode  99 score -570.23 average score -382.07 epsilon 0.01\n",
            "episode  100 score -567.97 average score -386.64 epsilon 0.01\n",
            "episode  101 score -549.45 average score -390.28 epsilon 0.01\n",
            "episode  102 score -451.34 average score -391.64 epsilon 0.01\n",
            "episode  103 score -530.56 average score -392.73 epsilon 0.01\n",
            "episode  104 score -601.07 average score -393.83 epsilon 0.01\n",
            "episode  105 score -559.35 average score -398.53 epsilon 0.01\n",
            "episode  106 score -625.19 average score -404.40 epsilon 0.01\n",
            "episode  107 score -640.99 average score -406.44 epsilon 0.01\n",
            "episode  108 score -566.33 average score -411.21 epsilon 0.01\n",
            "episode  109 score -464.20 average score -412.47 epsilon 0.01\n",
            "episode  110 score -653.64 average score -417.16 epsilon 0.01\n",
            "episode  111 score -590.72 average score -420.93 epsilon 0.01\n",
            "episode  112 score -49.10 average score -421.14 epsilon 0.01\n",
            "episode  113 score -587.85 average score -423.31 epsilon 0.01\n",
            "episode  114 score -550.32 average score -427.82 epsilon 0.01\n",
            "episode  115 score -536.55 average score -432.63 epsilon 0.01\n",
            "episode  116 score -531.73 average score -435.93 epsilon 0.01\n",
            "episode  117 score -305.51 average score -437.99 epsilon 0.01\n",
            "episode  118 score -557.78 average score -438.83 epsilon 0.01\n",
            "episode  119 score -565.07 average score -443.84 epsilon 0.01\n",
            "episode  120 score -711.82 average score -447.83 epsilon 0.01\n",
            "episode  121 score -649.68 average score -449.58 epsilon 0.01\n",
            "episode  122 score -698.32 average score -454.57 epsilon 0.01\n",
            "episode  123 score -623.38 average score -456.38 epsilon 0.01\n",
            "episode  124 score -605.57 average score -459.55 epsilon 0.01\n",
            "episode  125 score -386.33 average score -458.29 epsilon 0.01\n",
            "episode  126 score -42.25 average score -453.50 epsilon 0.01\n",
            "episode  127 score -598.84 average score -458.74 epsilon 0.01\n",
            "episode  128 score -580.40 average score -459.25 epsilon 0.01\n",
            "episode  129 score -743.76 average score -464.61 epsilon 0.01\n",
            "episode  130 score -595.30 average score -469.62 epsilon 0.01\n",
            "episode  131 score -600.05 average score -470.48 epsilon 0.01\n",
            "episode  132 score -518.71 average score -470.80 epsilon 0.01\n",
            "episode  133 score -534.24 average score -472.07 epsilon 0.01\n",
            "episode  134 score -582.96 average score -472.93 epsilon 0.01\n",
            "episode  135 score -680.10 average score -477.12 epsilon 0.01\n",
            "episode  136 score -523.43 average score -478.96 epsilon 0.01\n",
            "episode  137 score -481.54 average score -478.93 epsilon 0.01\n",
            "episode  138 score -537.21 average score -478.92 epsilon 0.01\n",
            "episode  139 score -549.72 average score -483.53 epsilon 0.01\n",
            "episode  140 score -290.33 average score -482.60 epsilon 0.01\n",
            "episode  141 score -614.28 average score -488.06 epsilon 0.01\n",
            "episode  142 score -525.35 average score -489.45 epsilon 0.01\n",
            "episode  143 score -491.05 average score -489.84 epsilon 0.01\n",
            "episode  144 score -554.33 average score -490.85 epsilon 0.01\n",
            "episode  145 score -594.33 average score -491.41 epsilon 0.01\n",
            "episode  146 score -164.78 average score -492.27 epsilon 0.01\n",
            "episode  147 score -509.33 average score -494.94 epsilon 0.01\n",
            "episode  148 score -705.93 average score -502.34 epsilon 0.01\n",
            "episode  149 score -600.00 average score -503.83 epsilon 0.01\n",
            "episode  150 score -575.43 average score -503.42 epsilon 0.01\n",
            "episode  151 score -667.85 average score -505.29 epsilon 0.01\n",
            "episode  152 score -201.95 average score -503.57 epsilon 0.01\n",
            "episode  153 score -624.90 average score -508.99 epsilon 0.01\n",
            "episode  154 score -319.47 average score -507.81 epsilon 0.01\n",
            "episode  155 score -634.70 average score -509.30 epsilon 0.01\n",
            "episode  156 score -52.84 average score -506.64 epsilon 0.01\n",
            "episode  157 score -561.84 average score -506.62 epsilon 0.01\n",
            "episode  158 score -570.83 average score -505.97 epsilon 0.01\n",
            "episode  159 score -344.47 average score -503.70 epsilon 0.01\n",
            "episode  160 score -481.04 average score -504.33 epsilon 0.01\n",
            "episode  161 score -680.02 average score -505.14 epsilon 0.01\n",
            "episode  162 score -485.01 average score -504.21 epsilon 0.01\n",
            "episode  163 score -613.38 average score -504.46 epsilon 0.01\n",
            "episode  164 score -586.81 average score -506.64 epsilon 0.01\n",
            "episode  165 score -300.02 average score -503.54 epsilon 0.01\n",
            "episode  166 score -501.64 average score -502.51 epsilon 0.01\n",
            "episode  167 score -528.06 average score -507.15 epsilon 0.01\n",
            "episode  168 score -595.71 average score -507.65 epsilon 0.01\n",
            "episode  169 score -601.00 average score -508.64 epsilon 0.01\n",
            "episode  170 score -646.83 average score -508.61 epsilon 0.01\n",
            "episode  171 score -551.22 average score -509.47 epsilon 0.01\n",
            "episode  172 score -385.56 average score -508.07 epsilon 0.01\n",
            "episode  173 score -566.78 average score -508.38 epsilon 0.01\n",
            "episode  174 score -379.14 average score -507.24 epsilon 0.01\n",
            "episode  175 score -599.90 average score -510.96 epsilon 0.01\n",
            "episode  176 score -620.44 average score -509.93 epsilon 0.01\n",
            "episode  177 score -547.44 average score -512.29 epsilon 0.01\n",
            "episode  178 score -665.17 average score -512.89 epsilon 0.01\n",
            "episode  179 score -528.02 average score -516.32 epsilon 0.01\n",
            "episode  180 score -579.65 average score -517.93 epsilon 0.01\n",
            "episode  181 score -589.52 average score -518.42 epsilon 0.01\n",
            "episode  182 score -592.53 average score -520.44 epsilon 0.01\n",
            "episode  183 score -586.35 average score -520.83 epsilon 0.01\n",
            "episode  184 score -83.88 average score -515.56 epsilon 0.01\n",
            "episode  185 score -573.33 average score -516.03 epsilon 0.01\n",
            "episode  186 score -582.64 average score -515.68 epsilon 0.01\n",
            "episode  187 score -507.79 average score -517.36 epsilon 0.01\n",
            "episode  188 score -715.78 average score -524.01 epsilon 0.01\n",
            "episode  189 score -373.08 average score -522.20 epsilon 0.01\n",
            "episode  190 score -574.81 average score -522.40 epsilon 0.01\n",
            "episode  191 score -699.14 average score -522.61 epsilon 0.01\n",
            "episode  192 score -668.54 average score -526.10 epsilon 0.01\n",
            "episode  193 score -427.07 average score -527.19 epsilon 0.01\n",
            "episode  194 score -559.06 average score -529.16 epsilon 0.01\n",
            "episode  195 score -630.56 average score -530.03 epsilon 0.01\n",
            "episode  196 score -608.45 average score -531.13 epsilon 0.01\n",
            "episode  197 score -519.17 average score -532.36 epsilon 0.01\n",
            "episode  198 score -573.45 average score -532.11 epsilon 0.01\n",
            "episode  199 score -559.97 average score -532.00 epsilon 0.01\n",
            "episode  200 score -373.45 average score -530.06 epsilon 0.01\n",
            "episode  201 score -599.84 average score -530.56 epsilon 0.01\n",
            "episode  202 score -600.12 average score -532.05 epsilon 0.01\n",
            "episode  203 score -583.05 average score -532.57 epsilon 0.01\n",
            "episode  204 score -619.56 average score -532.76 epsilon 0.01\n",
            "episode  205 score -73.94 average score -527.90 epsilon 0.01\n",
            "episode  206 score -510.94 average score -526.76 epsilon 0.01\n",
            "episode  207 score -557.74 average score -525.93 epsilon 0.01\n",
            "episode  208 score -574.91 average score -526.02 epsilon 0.01\n",
            "episode  209 score -587.91 average score -527.25 epsilon 0.01\n",
            "episode  210 score -563.75 average score -526.35 epsilon 0.01\n",
            "episode  211 score -617.67 average score -526.62 epsilon 0.01\n",
            "episode  212 score -601.33 average score -532.15 epsilon 0.01\n",
            "episode  213 score -628.50 average score -532.55 epsilon 0.01\n",
            "episode  214 score -707.08 average score -534.12 epsilon 0.01\n",
            "episode  215 score -576.94 average score -534.52 epsilon 0.01\n",
            "episode  216 score -61.56 average score -529.82 epsilon 0.01\n",
            "episode  217 score -425.13 average score -531.02 epsilon 0.01\n",
            "episode  218 score -712.50 average score -532.57 epsilon 0.01\n",
            "episode  219 score -503.82 average score -531.95 epsilon 0.01\n",
            "episode  220 score -572.47 average score -530.56 epsilon 0.01\n",
            "episode  221 score -635.19 average score -530.41 epsilon 0.01\n",
            "episode  222 score -594.32 average score -529.37 epsilon 0.01\n",
            "episode  223 score -737.68 average score -530.52 epsilon 0.01\n",
            "episode  224 score -692.30 average score -531.38 epsilon 0.01\n",
            "episode  225 score -432.30 average score -531.84 epsilon 0.01\n",
            "episode  226 score -655.75 average score -537.98 epsilon 0.01\n",
            "episode  227 score -665.53 average score -538.65 epsilon 0.01\n",
            "episode  228 score -666.11 average score -539.50 epsilon 0.01\n",
            "episode  229 score -549.35 average score -537.56 epsilon 0.01\n",
            "episode  230 score -609.74 average score -537.70 epsilon 0.01\n",
            "episode  231 score -493.84 average score -536.64 epsilon 0.01\n",
            "episode  232 score -452.08 average score -535.98 epsilon 0.01\n",
            "episode  233 score -671.03 average score -537.34 epsilon 0.01\n",
            "episode  234 score -588.65 average score -537.40 epsilon 0.01\n",
            "episode  235 score -482.36 average score -535.42 epsilon 0.01\n",
            "episode  236 score -539.60 average score -535.58 epsilon 0.01\n",
            "episode  237 score -580.24 average score -536.57 epsilon 0.01\n",
            "episode  238 score -677.51 average score -537.97 epsilon 0.01\n",
            "episode  239 score -640.80 average score -538.89 epsilon 0.01\n",
            "episode  240 score -544.88 average score -541.43 epsilon 0.01\n",
            "episode  241 score -90.13 average score -536.19 epsilon 0.01\n",
            "episode  242 score -620.73 average score -537.14 epsilon 0.01\n",
            "episode  243 score -629.04 average score -538.52 epsilon 0.01\n",
            "episode  244 score -542.29 average score -538.40 epsilon 0.01\n",
            "episode  245 score -607.17 average score -538.53 epsilon 0.01\n",
            "episode  246 score -566.35 average score -542.55 epsilon 0.01\n",
            "episode  247 score -541.50 average score -542.87 epsilon 0.01\n",
            "episode  248 score -586.41 average score -541.67 epsilon 0.01\n",
            "episode  249 score -576.16 average score -541.43 epsilon 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.Q_eval.state_dict(), \"/content/files/DQN_fine_tuned_files/tuned_baseline_extra_layer.zip\" )"
      ],
      "metadata": {
        "id": "QTQk94MGTzYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model 2 the Noisey Model weights and exporting its weight to pytorch "
      ],
      "metadata": {
        "id": "eYjHkyzrTAL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "import gym\n",
        "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/Noisey_pretrain.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v_2 = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v_2.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "print(dqn_torch_v_2.parameters())\n",
        "for param in dqn_torch_v_2.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = 64  # 8 states we have for the polly to move \n",
        "num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "dqn_torch_v_2.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v_2.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v_2.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def obs_to_torch(obs):\n",
        "    # TF: NHWC\n",
        "    # PyTorch: NCHW\n",
        "    # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "    # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "    # # Normalize\n",
        "    # obs = obs / 255.0\n",
        "    obs = th.tensor(obs).float()\n",
        "    obs = obs.to(device)\n",
        "    return obs\n",
        "\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "print(next(dqn_torch_v_2.parameters()).device)\n",
        "while not done:\n",
        "    action = th.argmax(dqn_torch_v_2(obs_to_torch(obs))).item()\n",
        "    # action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "\n",
        "print(episode_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcae9a4-9332-4e7b-a14f-5b6e5a9c58b9",
        "id": "EjeoZX1tTAMK"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Is CUDA supported by this system?True\n",
            "CUDA version: 11.3\n",
            "Name of current CUDA device:Tesla T4\n",
            "ID of current CUDA device:0\n",
            "cuda:0\n",
            "loaded model\n",
            "Parameter containing:\n",
            "tensor([[-0.2699,  0.2215, -0.2805,  ...,  0.0768,  0.2879,  0.3533],\n",
            "        [-0.0257, -0.2925, -0.0951,  ...,  0.1328,  0.1329, -0.3405],\n",
            "        [ 0.3069,  0.2371, -0.0466,  ..., -0.2950,  0.3136, -0.1654],\n",
            "        ...,\n",
            "        [ 0.2993, -0.2371, -0.1341,  ...,  0.1764,  0.0587, -0.2268],\n",
            "        [ 0.1419, -0.3017,  0.3421,  ...,  0.3309,  0.3151, -0.1235],\n",
            "        [-0.2089, -0.1852, -0.3029,  ..., -0.1262, -0.1147,  0.2384]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([-0.0336,  0.2579,  0.3049, -0.3168, -0.0405,  0.0551,  0.0542,  0.2759,\n",
            "         0.3215, -0.2255,  0.0892, -0.3376, -0.1296,  0.0549,  0.0086,  0.3483,\n",
            "        -0.3105, -0.0151, -0.1516, -0.0812,  0.2399, -0.0945, -0.0795,  0.1701,\n",
            "         0.0569, -0.1270,  0.1957,  0.0594,  0.0798,  0.3439,  0.2332,  0.2575,\n",
            "        -0.3462,  0.2634, -0.3212, -0.1445,  0.2926,  0.1328, -0.0704,  0.3341,\n",
            "        -0.2070,  0.2270, -0.1216,  0.3309,  0.3299, -0.3321, -0.3431, -0.1876,\n",
            "         0.2460, -0.2405, -0.0594,  0.1098,  0.0619, -0.1331, -0.0226,  0.0106,\n",
            "         0.2693, -0.2799, -0.1559,  0.1557, -0.3094, -0.0167, -0.1749, -0.2252,\n",
            "        -0.2997, -0.0884,  0.2729,  0.1063,  0.2781,  0.0420, -0.1359, -0.1636,\n",
            "        -0.1561, -0.1241, -0.1332,  0.1855, -0.3280,  0.1731,  0.2616,  0.0994,\n",
            "        -0.0322,  0.0901, -0.2865,  0.1353,  0.3408, -0.0473, -0.1434,  0.2903,\n",
            "         0.0677,  0.1557,  0.2872,  0.0385, -0.2873, -0.1876, -0.1646, -0.3522,\n",
            "        -0.1121,  0.0571,  0.0714,  0.2680, -0.0580, -0.2201, -0.2486,  0.2876,\n",
            "         0.0375, -0.0763, -0.0524, -0.1231,  0.2352,  0.2835, -0.1164,  0.3422,\n",
            "        -0.2345, -0.1577, -0.3509, -0.3399,  0.0260,  0.3076,  0.0837, -0.1969,\n",
            "         0.3497,  0.0974,  0.1489,  0.2827, -0.2747, -0.0516,  0.3323,  0.2814,\n",
            "         0.1320,  0.0699,  0.2809, -0.0457,  0.2268, -0.0130, -0.1604, -0.0925,\n",
            "        -0.3000, -0.0935,  0.0340,  0.0236, -0.1917,  0.0240,  0.2989,  0.0586,\n",
            "        -0.1174, -0.1682,  0.3185, -0.2086, -0.0210, -0.0636, -0.3340, -0.0155,\n",
            "         0.0991,  0.2144,  0.0567,  0.2103, -0.2173, -0.0564,  0.0525,  0.3312,\n",
            "         0.0679, -0.2402,  0.2016,  0.3393,  0.3408,  0.0808,  0.3329, -0.1862,\n",
            "        -0.1870, -0.2554,  0.3173, -0.2191,  0.0509, -0.2519, -0.2181, -0.1824,\n",
            "         0.0459,  0.2204,  0.2475,  0.0878,  0.0011,  0.2317,  0.3526, -0.0642,\n",
            "        -0.0062,  0.2188,  0.3320, -0.3254, -0.3214,  0.3010, -0.0947,  0.1782,\n",
            "        -0.3135, -0.0796,  0.2452, -0.1053,  0.2338,  0.1890, -0.1025,  0.3486,\n",
            "         0.0631,  0.0882, -0.0176,  0.1981, -0.3266,  0.2033,  0.0188, -0.3023,\n",
            "         0.2795,  0.3414,  0.1982, -0.3114, -0.0374,  0.3310,  0.2813, -0.3048,\n",
            "         0.0528, -0.1564, -0.1738, -0.0187, -0.1911, -0.2737,  0.2147, -0.2513,\n",
            "         0.2119, -0.0172, -0.1393, -0.1406, -0.0802, -0.1132, -0.0987, -0.0053,\n",
            "        -0.1186,  0.1322, -0.0319,  0.2155, -0.0330, -0.0953, -0.0801,  0.2992,\n",
            "         0.3217, -0.1825, -0.3405,  0.1533,  0.3523, -0.0699,  0.0331, -0.2227,\n",
            "        -0.1190,  0.1804, -0.0838,  0.2980,  0.1437,  0.3370,  0.3465,  0.0366],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "<generator object Module.parameters at 0x7f96e84c0ed0>\n",
            "cuda:0\n",
            "-102.69367268448568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "agent2 = Agent_lr(gamma=0.99, epsilon=0.99, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "              input_dims=[8], lr=0.001, eps_dec = 2e-4, parameters = dqn_torch_v_2.parameters())\n",
        "\n",
        "\n",
        "\n",
        "scores_2, eps_history = [], []\n",
        "n_games = 250\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "      action = agent2.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent2.store_transition(observation, action, reward, \n",
        "                              observation_, done)\n",
        "      agent2.learn()\n",
        "      observation = observation_\n",
        "    scores_2.append(score)\n",
        "    eps_history.append(agent2.epsilon)\n",
        "\n",
        "    avg_score = np.mean(scores_2[-100:])\n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "            'average score %.2f' % avg_score,\n",
        "            'epsilon %.2f' % agent2.epsilon)\n",
        "x_2 = [i+1 for i in range(n_games)]\n",
        "filename = 'lunar_lander.png'\n",
        "foo_2.plotLearning(x_2, scores_2, eps_history, filename)\n",
        "# x_np = np.array(x)[indices.astype(int)]\n",
        "# performance_metrics(x_np,scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tyKRwp_gpRDh",
        "outputId": "702d4654-1e11-4f79-9224-f737dbceffa2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score -120.10 average score -120.10 epsilon 0.98\n",
            "episode  1 score -113.32 average score -116.71 epsilon 0.96\n",
            "episode  2 score -419.78 average score -217.73 epsilon 0.94\n",
            "episode  3 score -429.66 average score -270.72 epsilon 0.91\n",
            "episode  4 score -342.49 average score -285.07 epsilon 0.90\n",
            "episode  5 score -140.05 average score -260.90 epsilon 0.87\n",
            "episode  6 score -324.43 average score -269.98 epsilon 0.85\n",
            "episode  7 score -335.40 average score -278.16 epsilon 0.82\n",
            "episode  8 score -223.95 average score -272.13 epsilon 0.80\n",
            "episode  9 score -4.97 average score -245.42 epsilon 0.78\n",
            "episode  10 score -230.28 average score -244.04 epsilon 0.76\n",
            "episode  11 score -301.26 average score -248.81 epsilon 0.74\n",
            "episode  12 score -340.74 average score -255.88 epsilon 0.72\n",
            "episode  13 score -262.98 average score -256.39 epsilon 0.69\n",
            "episode  14 score -337.88 average score -261.82 epsilon 0.65\n",
            "episode  15 score -321.61 average score -265.56 epsilon 0.63\n",
            "episode  16 score -253.33 average score -264.84 epsilon 0.60\n",
            "episode  17 score -231.60 average score -262.99 epsilon 0.57\n",
            "episode  18 score -61.23 average score -252.37 epsilon 0.55\n",
            "episode  19 score 13.22 average score -239.09 epsilon 0.53\n",
            "episode  20 score -185.48 average score -236.54 epsilon 0.51\n",
            "episode  21 score -273.81 average score -238.23 epsilon 0.49\n",
            "episode  22 score -156.75 average score -234.69 epsilon 0.45\n",
            "episode  23 score -317.89 average score -238.16 epsilon 0.43\n",
            "episode  24 score -224.78 average score -237.62 epsilon 0.41\n",
            "episode  25 score -291.99 average score -239.71 epsilon 0.39\n",
            "episode  26 score -312.85 average score -242.42 epsilon 0.37\n",
            "episode  27 score -227.67 average score -241.90 epsilon 0.34\n",
            "episode  28 score -335.41 average score -245.12 epsilon 0.31\n",
            "episode  29 score -1160.93 average score -275.65 epsilon 0.27\n",
            "episode  30 score -764.20 average score -291.41 epsilon 0.24\n",
            "episode  31 score -278.25 average score -291.00 epsilon 0.22\n",
            "episode  32 score -228.24 average score -289.09 epsilon 0.20\n",
            "episode  33 score -238.30 average score -287.60 epsilon 0.17\n",
            "episode  34 score -343.25 average score -289.19 epsilon 0.15\n",
            "episode  35 score -187.64 average score -286.37 epsilon 0.13\n",
            "episode  36 score -645.70 average score -296.08 epsilon 0.09\n",
            "episode  37 score -592.90 average score -303.89 epsilon 0.06\n",
            "episode  38 score -269.42 average score -303.01 epsilon 0.03\n",
            "episode  39 score -194.07 average score -300.29 epsilon 0.01\n",
            "episode  40 score -307.86 average score -300.47 epsilon 0.01\n",
            "episode  41 score -226.87 average score -298.72 epsilon 0.01\n",
            "episode  42 score -328.33 average score -299.41 epsilon 0.01\n",
            "episode  43 score -221.79 average score -297.64 epsilon 0.01\n",
            "episode  44 score -220.52 average score -295.93 epsilon 0.01\n",
            "episode  45 score -956.01 average score -310.28 epsilon 0.01\n",
            "episode  46 score -198.86 average score -307.91 epsilon 0.01\n",
            "episode  47 score -319.52 average score -308.15 epsilon 0.01\n",
            "episode  48 score -250.37 average score -306.97 epsilon 0.01\n",
            "episode  49 score -1695.30 average score -334.74 epsilon 0.01\n",
            "episode  50 score -325.30 average score -334.55 epsilon 0.01\n",
            "episode  51 score -1029.99 average score -347.93 epsilon 0.01\n",
            "episode  52 score -634.34 average score -353.33 epsilon 0.01\n",
            "episode  53 score -3312.30 average score -408.13 epsilon 0.01\n",
            "episode  54 score -271.21 average score -405.64 epsilon 0.01\n",
            "episode  55 score -828.84 average score -413.19 epsilon 0.01\n",
            "episode  56 score -241.09 average score -410.17 epsilon 0.01\n",
            "episode  57 score -953.04 average score -419.53 epsilon 0.01\n",
            "episode  58 score -213.33 average score -416.04 epsilon 0.01\n",
            "episode  59 score -308.95 average score -414.25 epsilon 0.01\n",
            "episode  60 score -2437.95 average score -447.43 epsilon 0.01\n",
            "episode  61 score -305.15 average score -445.13 epsilon 0.01\n",
            "episode  62 score -768.20 average score -450.26 epsilon 0.01\n",
            "episode  63 score -316.34 average score -448.17 epsilon 0.01\n",
            "episode  64 score -190.87 average score -444.21 epsilon 0.01\n",
            "episode  65 score -200.44 average score -440.52 epsilon 0.01\n",
            "episode  66 score -439.00 average score -440.50 epsilon 0.01\n",
            "episode  67 score -421.93 average score -440.22 epsilon 0.01\n",
            "episode  68 score -196.17 average score -436.69 epsilon 0.01\n",
            "episode  69 score -338.37 average score -435.28 epsilon 0.01\n",
            "episode  70 score -604.68 average score -437.67 epsilon 0.01\n",
            "episode  71 score -1514.39 average score -452.62 epsilon 0.01\n",
            "episode  72 score -615.61 average score -454.85 epsilon 0.01\n",
            "episode  73 score -124.44 average score -450.39 epsilon 0.01\n",
            "episode  74 score -201.95 average score -447.08 epsilon 0.01\n",
            "episode  75 score -317.67 average score -445.37 epsilon 0.01\n",
            "episode  76 score -685.49 average score -448.49 epsilon 0.01\n",
            "episode  77 score -227.44 average score -445.66 epsilon 0.01\n",
            "episode  78 score -272.02 average score -443.46 epsilon 0.01\n",
            "episode  79 score -299.41 average score -441.66 epsilon 0.01\n",
            "episode  80 score -255.65 average score -439.36 epsilon 0.01\n",
            "episode  81 score -644.46 average score -441.86 epsilon 0.01\n",
            "episode  82 score -1259.45 average score -451.71 epsilon 0.01\n",
            "episode  83 score -410.65 average score -451.23 epsilon 0.01\n",
            "episode  84 score -205.53 average score -448.34 epsilon 0.01\n",
            "episode  85 score -331.68 average score -446.98 epsilon 0.01\n",
            "episode  86 score -730.29 average score -450.24 epsilon 0.01\n",
            "episode  87 score -231.13 average score -447.75 epsilon 0.01\n",
            "episode  88 score -477.14 average score -448.08 epsilon 0.01\n",
            "episode  89 score -760.32 average score -451.55 epsilon 0.01\n",
            "episode  90 score -321.48 average score -450.12 epsilon 0.01\n",
            "episode  91 score -250.25 average score -447.94 epsilon 0.01\n",
            "episode  92 score -313.30 average score -446.50 epsilon 0.01\n",
            "episode  93 score -688.20 average score -449.07 epsilon 0.01\n",
            "episode  94 score -737.45 average score -452.10 epsilon 0.01\n",
            "episode  95 score -294.74 average score -450.46 epsilon 0.01\n",
            "episode  96 score -732.54 average score -453.37 epsilon 0.01\n",
            "episode  97 score -291.64 average score -451.72 epsilon 0.01\n",
            "episode  98 score -1536.19 average score -462.68 epsilon 0.01\n",
            "episode  99 score -977.84 average score -467.83 epsilon 0.01\n",
            "episode  100 score -255.31 average score -469.18 epsilon 0.01\n",
            "episode  101 score -1575.80 average score -483.80 epsilon 0.01\n",
            "episode  102 score -494.49 average score -484.55 epsilon 0.01\n",
            "episode  103 score -1320.17 average score -493.46 epsilon 0.01\n",
            "episode  104 score -294.15 average score -492.97 epsilon 0.01\n",
            "episode  105 score -303.48 average score -494.61 epsilon 0.01\n",
            "episode  106 score -510.20 average score -496.46 epsilon 0.01\n",
            "episode  107 score -929.70 average score -502.41 epsilon 0.01\n",
            "episode  108 score -1527.63 average score -515.44 epsilon 0.01\n",
            "episode  109 score -248.41 average score -517.88 epsilon 0.01\n",
            "episode  110 score -1249.51 average score -528.07 epsilon 0.01\n",
            "episode  111 score -799.95 average score -533.06 epsilon 0.01\n",
            "episode  112 score -236.93 average score -532.02 epsilon 0.01\n",
            "episode  113 score -700.69 average score -536.40 epsilon 0.01\n",
            "episode  114 score -296.97 average score -535.99 epsilon 0.01\n",
            "episode  115 score -269.30 average score -535.46 epsilon 0.01\n",
            "episode  116 score -4699.07 average score -579.92 epsilon 0.01\n",
            "episode  117 score -284.93 average score -580.46 epsilon 0.01\n",
            "episode  118 score -1308.03 average score -592.92 epsilon 0.01\n",
            "episode  119 score -264.15 average score -595.70 epsilon 0.01\n",
            "episode  120 score -508.07 average score -598.92 epsilon 0.01\n",
            "episode  121 score -529.58 average score -601.48 epsilon 0.01\n",
            "episode  122 score -420.44 average score -604.12 epsilon 0.01\n",
            "episode  123 score -380.80 average score -604.75 epsilon 0.01\n",
            "episode  124 score -2042.07 average score -622.92 epsilon 0.01\n",
            "episode  125 score -2522.47 average score -645.22 epsilon 0.01\n",
            "episode  126 score -228.38 average score -644.38 epsilon 0.01\n",
            "episode  127 score -412.66 average score -646.23 epsilon 0.01\n",
            "episode  128 score -1724.78 average score -660.12 epsilon 0.01\n",
            "episode  129 score -369.45 average score -652.21 epsilon 0.01\n",
            "episode  130 score -148.86 average score -646.06 epsilon 0.01\n",
            "episode  131 score -1231.80 average score -655.59 epsilon 0.01\n",
            "episode  132 score -1482.41 average score -668.13 epsilon 0.01\n",
            "episode  133 score -317.43 average score -668.92 epsilon 0.01\n",
            "episode  134 score -486.97 average score -670.36 epsilon 0.01\n",
            "episode  135 score -328.65 average score -671.77 epsilon 0.01\n",
            "episode  136 score -236.23 average score -667.68 epsilon 0.01\n",
            "episode  137 score -2267.30 average score -684.42 epsilon 0.01\n",
            "episode  138 score -572.24 average score -687.45 epsilon 0.01\n",
            "episode  139 score -531.18 average score -690.82 epsilon 0.01\n",
            "episode  140 score -400.49 average score -691.75 epsilon 0.01\n",
            "episode  141 score -385.96 average score -693.34 epsilon 0.01\n",
            "episode  142 score -928.47 average score -699.34 epsilon 0.01\n",
            "episode  143 score -207.67 average score -699.20 epsilon 0.01\n",
            "episode  144 score -168.38 average score -698.68 epsilon 0.01\n",
            "episode  145 score -243.60 average score -691.55 epsilon 0.01\n",
            "episode  146 score -588.36 average score -695.45 epsilon 0.01\n",
            "episode  147 score -235.49 average score -694.61 epsilon 0.01\n",
            "episode  148 score -249.69 average score -694.60 epsilon 0.01\n",
            "episode  149 score -204.22 average score -679.69 epsilon 0.01\n",
            "episode  150 score -355.83 average score -679.99 epsilon 0.01\n",
            "episode  151 score -284.49 average score -672.54 epsilon 0.01\n",
            "episode  152 score -313.40 average score -669.33 epsilon 0.01\n",
            "episode  153 score -526.86 average score -641.47 epsilon 0.01\n",
            "episode  154 score -520.71 average score -643.97 epsilon 0.01\n",
            "episode  155 score -337.61 average score -639.06 epsilon 0.01\n",
            "episode  156 score -216.18 average score -638.81 epsilon 0.01\n",
            "episode  157 score -252.67 average score -631.80 epsilon 0.01\n",
            "episode  158 score -300.25 average score -632.67 epsilon 0.01\n",
            "episode  159 score -313.92 average score -632.72 epsilon 0.01\n",
            "episode  160 score -537.45 average score -613.72 epsilon 0.01\n",
            "episode  161 score -508.49 average score -615.75 epsilon 0.01\n",
            "episode  162 score -335.94 average score -611.43 epsilon 0.01\n",
            "episode  163 score -1733.78 average score -625.60 epsilon 0.01\n",
            "episode  164 score -230.81 average score -626.00 epsilon 0.01\n",
            "episode  165 score -297.09 average score -626.97 epsilon 0.01\n",
            "episode  166 score -309.90 average score -625.68 epsilon 0.01\n",
            "episode  167 score -287.77 average score -624.34 epsilon 0.01\n",
            "episode  168 score -1204.60 average score -634.42 epsilon 0.01\n",
            "episode  169 score -248.79 average score -633.53 epsilon 0.01\n",
            "episode  170 score -329.61 average score -630.77 epsilon 0.01\n",
            "episode  171 score -766.85 average score -623.30 epsilon 0.01\n",
            "episode  172 score -417.17 average score -621.32 epsilon 0.01\n",
            "episode  173 score -1383.42 average score -633.90 epsilon 0.01\n",
            "episode  174 score -273.70 average score -634.62 epsilon 0.01\n",
            "episode  175 score -265.28 average score -634.10 epsilon 0.01\n",
            "episode  176 score -2114.94 average score -648.39 epsilon 0.01\n",
            "episode  177 score -384.15 average score -649.96 epsilon 0.01\n",
            "episode  178 score -240.30 average score -649.64 epsilon 0.01\n",
            "episode  179 score -269.33 average score -649.34 epsilon 0.01\n",
            "episode  180 score -281.51 average score -649.60 epsilon 0.01\n",
            "episode  181 score -231.84 average score -645.47 epsilon 0.01\n",
            "episode  182 score -212.30 average score -635.00 epsilon 0.01\n",
            "episode  183 score -958.30 average score -640.48 epsilon 0.01\n",
            "episode  184 score -1133.69 average score -649.76 epsilon 0.01\n",
            "episode  185 score -241.77 average score -648.86 epsilon 0.01\n",
            "episode  186 score -2797.67 average score -669.54 epsilon 0.01\n",
            "episode  187 score -296.87 average score -670.19 epsilon 0.01\n",
            "episode  188 score -261.56 average score -668.04 epsilon 0.01\n",
            "episode  189 score -211.05 average score -662.54 epsilon 0.01\n",
            "episode  190 score -272.67 average score -662.06 epsilon 0.01\n",
            "episode  191 score -775.09 average score -667.30 epsilon 0.01\n",
            "episode  192 score -1154.68 average score -675.72 epsilon 0.01\n",
            "episode  193 score -224.17 average score -671.08 epsilon 0.01\n",
            "episode  194 score -1526.67 average score -678.97 epsilon 0.01\n",
            "episode  195 score -269.47 average score -678.72 epsilon 0.01\n",
            "episode  196 score -327.52 average score -674.67 epsilon 0.01\n",
            "episode  197 score -263.74 average score -674.39 epsilon 0.01\n",
            "episode  198 score -279.64 average score -661.82 epsilon 0.01\n",
            "episode  199 score -215.99 average score -654.20 epsilon 0.01\n",
            "episode  200 score -1094.19 average score -662.59 epsilon 0.01\n",
            "episode  201 score -661.42 average score -653.45 epsilon 0.01\n",
            "episode  202 score -258.11 average score -651.09 epsilon 0.01\n",
            "episode  203 score -312.53 average score -641.01 epsilon 0.01\n",
            "episode  204 score -532.98 average score -643.40 epsilon 0.01\n",
            "episode  205 score -718.92 average score -647.55 epsilon 0.01\n",
            "episode  206 score -186.41 average score -644.31 epsilon 0.01\n",
            "episode  207 score -117.49 average score -636.19 epsilon 0.01\n",
            "episode  208 score -992.61 average score -630.84 epsilon 0.01\n",
            "episode  209 score -323.10 average score -631.59 epsilon 0.01\n",
            "episode  210 score -508.03 average score -624.17 epsilon 0.01\n",
            "episode  211 score -257.44 average score -618.75 epsilon 0.01\n",
            "episode  212 score -805.80 average score -624.44 epsilon 0.01\n",
            "episode  213 score -198.06 average score -619.41 epsilon 0.01\n",
            "episode  214 score -328.22 average score -619.72 epsilon 0.01\n",
            "episode  215 score -237.25 average score -619.40 epsilon 0.01\n",
            "episode  216 score -323.61 average score -575.65 epsilon 0.01\n",
            "episode  217 score -315.95 average score -575.96 epsilon 0.01\n",
            "episode  218 score -260.97 average score -565.49 epsilon 0.01\n",
            "episode  219 score -413.64 average score -566.98 epsilon 0.01\n",
            "episode  220 score -348.90 average score -565.39 epsilon 0.01\n",
            "episode  221 score -207.64 average score -562.17 epsilon 0.01\n",
            "episode  222 score -244.34 average score -560.41 epsilon 0.01\n",
            "episode  223 score -260.80 average score -559.21 epsilon 0.01\n",
            "episode  224 score -2345.16 average score -562.24 epsilon 0.01\n",
            "episode  225 score -203.62 average score -539.05 epsilon 0.01\n",
            "episode  226 score -464.00 average score -541.41 epsilon 0.01\n",
            "episode  227 score -341.81 average score -540.70 epsilon 0.01\n",
            "episode  228 score -953.92 average score -532.99 epsilon 0.01\n",
            "episode  229 score -779.89 average score -537.10 epsilon 0.01\n",
            "episode  230 score -302.06 average score -538.63 epsilon 0.01\n",
            "episode  231 score -231.67 average score -528.63 epsilon 0.01\n",
            "episode  232 score -448.04 average score -518.28 epsilon 0.01\n",
            "episode  233 score -292.92 average score -518.04 epsilon 0.01\n",
            "episode  234 score -314.12 average score -516.31 epsilon 0.01\n",
            "episode  235 score -251.56 average score -515.54 epsilon 0.01\n",
            "episode  236 score -565.40 average score -518.83 epsilon 0.01\n",
            "episode  237 score -692.16 average score -503.08 epsilon 0.01\n",
            "episode  238 score -257.81 average score -499.94 epsilon 0.01\n",
            "episode  239 score -160.90 average score -496.23 epsilon 0.01\n",
            "episode  240 score -109.14 average score -493.32 epsilon 0.01\n",
            "episode  241 score -289.77 average score -492.36 epsilon 0.01\n",
            "episode  242 score -319.93 average score -486.27 epsilon 0.01\n",
            "episode  243 score -738.41 average score -491.58 epsilon 0.01\n",
            "episode  244 score -397.51 average score -493.87 epsilon 0.01\n",
            "episode  245 score -545.67 average score -496.89 epsilon 0.01\n",
            "episode  246 score -186.24 average score -492.87 epsilon 0.01\n",
            "episode  247 score -440.40 average score -494.92 epsilon 0.01\n",
            "episode  248 score -537.68 average score -497.80 epsilon 0.01\n",
            "episode  249 score -218.71 average score -497.94 epsilon 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent_2.Q_eval)\n",
        "torch.save(agent2.Q_eval.state_dict(), \"/content/files/DQN_fine_tuned_files/DQN_tuned_nosiey_added_layer.zip\")"
      ],
      "metadata": {
        "id": "wDOVWK2Q1PM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning the last hidden layer without adding any extra layer "
      ],
      "metadata": {
        "id": "SJi1RarZsnI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model 1 the Baseline Model weights and exporting its weight to pytorch "
      ],
      "metadata": {
        "id": "VlZkMXEpswjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "import gym\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/baseline_pretrained.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "print(dqn_torch_v.parameters())\n",
        "\n",
        "for child in dqn_torch_v.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            # print(param)\n",
        "            # print(ct)\n",
        "            param.requires_grad = False\n",
        "            # print(\"the param\")\n",
        "            # print(param.requires_grad)\n",
        "\n",
        "for param in child.parameters():\n",
        "  # print(param)\n",
        "  # print(ct)\n",
        "  # param.requires_grad = False\n",
        "  print(\"the param\")\n",
        "  print(param.requires_grad)\n",
        "\n",
        "print(dqn_torch_v.parameters())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for param in dqn_torch_v.parameters():\n",
        "  param.requires_grad = False\n",
        "  # print(1)\n",
        "  # print(param)\n",
        "# num_ftrs = 64  # 8 states we have for the polly to move \n",
        "# num_classes = 4 # number of Actions at final layer \n",
        "# # ResNet final fully connected layer\n",
        "# dqn_torch_v.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# def obs_to_torch(obs):\n",
        "#     # TF: NHWC\n",
        "#     # PyTorch: NCHW\n",
        "#     # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "#     # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "#     # # Normalize\n",
        "#     # obs = obs / 255.0\n",
        "#     obs = th.tensor(obs).float()\n",
        "#     obs = obs.to(device)\n",
        "#     return obs\n",
        "\n",
        "\n",
        "# env = gym.make('LunarLander-v4')\n",
        "\n",
        "# episode_reward = 0\n",
        "# done = False\n",
        "# obs = env.reset()\n",
        "# print(next(dqn_torch_v.parameters()).device)\n",
        "# while not done:\n",
        "#     action = th.argmax(dqn_torch_v(obs_to_torch(obs))).item()\n",
        "#     # action = env.action_space.sample()\n",
        "#     obs, reward, done, _ = env.step(action)\n",
        "#     episode_reward += reward\n",
        "\n",
        "# print(episode_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c726e83-9f13-4408-b138-86a0d4974c34",
        "id": "A5kxDJSjswjf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Is CUDA supported by this system?True\n",
            "CUDA version: 11.3\n",
            "ID of current CUDA device:0\n",
            "cuda:0\n",
            "loaded model\n",
            "<generator object Module.parameters at 0x7f96b26d0cd0>\n",
            "<generator object Module.parameters at 0x7f96b26d0cd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "agent = Agent_lr(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "              input_dims=[8], lr=0.001,  eps_dec=2e-4, parameters = dqn_torch_v.parameters())\n",
        "scores_1, eps_history = [], []\n",
        "n_games = 250\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "      action = agent.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent.store_transition(observation, action, reward, \n",
        "                              observation_, done)\n",
        "      agent.learn()\n",
        "      observation = observation_\n",
        "    scores_1.append(score)\n",
        "    eps_history.append(agent.epsilon)\n",
        "\n",
        "    avg_score = np.mean(scores_1[-100:])\n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "            'average score %.2f' % avg_score,\n",
        "            'epsilon %.2f' % agent.epsilon)\n",
        "x_1 = [i+1 for i in range(n_games)]\n",
        "filename = 'lunar_lander.png'\n",
        "foo_2.plotLearning(x_1, scores_1, eps_history, filename)\n",
        "    # x_np = np.array(x)[indices.astype(int)]\n",
        "    # performance_metrics(x_np,scores)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6577a75-d76f-437f-bfdb-4c3a02bcdbf1",
        "id": "ChBhkZ2jswjg"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7f96b26e7650>\n",
            "episode  0 score -69.63 average score -69.63 epsilon 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  1 score -250.27 average score -159.95 epsilon 0.98\n",
            "episode  2 score -52.44 average score -124.11 epsilon 0.97\n",
            "episode  3 score -271.67 average score -161.00 epsilon 0.95\n",
            "episode  4 score -323.38 average score -193.48 epsilon 0.93\n",
            "episode  5 score -287.15 average score -209.09 epsilon 0.91\n",
            "episode  6 score -29.87 average score -183.49 epsilon 0.90\n",
            "episode  7 score -46.37 average score -166.35 epsilon 0.88\n",
            "episode  8 score -529.43 average score -206.69 epsilon 0.86\n",
            "episode  9 score -135.89 average score -199.61 epsilon 0.83\n",
            "episode  10 score -484.11 average score -225.47 epsilon 0.80\n",
            "episode  11 score 5.32 average score -206.24 epsilon 0.79\n",
            "episode  12 score -306.28 average score -213.94 epsilon 0.77\n",
            "episode  13 score -237.23 average score -215.60 epsilon 0.76\n",
            "episode  14 score -275.63 average score -219.60 epsilon 0.74\n",
            "episode  15 score -255.61 average score -221.85 epsilon 0.72\n",
            "episode  16 score -315.72 average score -227.37 epsilon 0.70\n",
            "episode  17 score -354.19 average score -234.42 epsilon 0.68\n",
            "episode  18 score -293.72 average score -237.54 epsilon 0.65\n",
            "episode  19 score -254.72 average score -238.40 epsilon 0.62\n",
            "episode  20 score -115.29 average score -232.54 epsilon 0.60\n",
            "episode  21 score -114.25 average score -227.16 epsilon 0.57\n",
            "episode  22 score -255.17 average score -228.38 epsilon 0.55\n",
            "episode  23 score -743.07 average score -249.82 epsilon 0.50\n",
            "episode  24 score -484.37 average score -259.21 epsilon 0.47\n",
            "episode  25 score -204.59 average score -257.11 epsilon 0.45\n",
            "episode  26 score -744.99 average score -275.18 epsilon 0.40\n",
            "episode  27 score -914.34 average score -298.00 epsilon 0.37\n",
            "episode  28 score -386.85 average score -301.07 epsilon 0.35\n",
            "episode  29 score -401.44 average score -304.41 epsilon 0.33\n",
            "episode  30 score -538.95 average score -311.98 epsilon 0.31\n",
            "episode  31 score -684.05 average score -323.60 epsilon 0.29\n",
            "episode  32 score -753.98 average score -336.65 epsilon 0.25\n",
            "episode  33 score -387.87 average score -338.15 epsilon 0.23\n",
            "episode  34 score -700.56 average score -348.51 epsilon 0.20\n",
            "episode  35 score -1383.08 average score -377.25 epsilon 0.16\n",
            "episode  36 score -327.67 average score -375.91 epsilon 0.14\n",
            "episode  37 score -626.81 average score -382.51 epsilon 0.12\n",
            "episode  38 score -808.64 average score -393.44 epsilon 0.10\n",
            "episode  39 score -463.38 average score -395.18 epsilon 0.08\n",
            "episode  40 score -1274.83 average score -416.64 epsilon 0.05\n",
            "episode  41 score -554.54 average score -419.92 epsilon 0.03\n",
            "episode  42 score -571.00 average score -423.44 epsilon 0.02\n",
            "episode  43 score -544.03 average score -426.18 epsilon 0.01\n",
            "episode  44 score -550.59 average score -428.94 epsilon 0.01\n",
            "episode  45 score -694.85 average score -434.72 epsilon 0.01\n",
            "episode  46 score -461.93 average score -435.30 epsilon 0.01\n",
            "episode  47 score -1329.50 average score -453.93 epsilon 0.01\n",
            "episode  48 score -1332.66 average score -471.86 epsilon 0.01\n",
            "episode  49 score -383.99 average score -470.11 epsilon 0.01\n",
            "episode  50 score -950.19 average score -479.52 epsilon 0.01\n",
            "episode  51 score -716.40 average score -484.07 epsilon 0.01\n",
            "episode  52 score -618.14 average score -486.60 epsilon 0.01\n",
            "episode  53 score -516.97 average score -487.17 epsilon 0.01\n",
            "episode  54 score -1346.60 average score -502.79 epsilon 0.01\n",
            "episode  55 score -2489.78 average score -538.27 epsilon 0.01\n",
            "episode  56 score -584.15 average score -539.08 epsilon 0.01\n",
            "episode  57 score -574.79 average score -539.69 epsilon 0.01\n",
            "episode  58 score -580.47 average score -540.39 epsilon 0.01\n",
            "episode  59 score -626.25 average score -541.82 epsilon 0.01\n",
            "episode  60 score -2186.82 average score -568.78 epsilon 0.01\n",
            "episode  61 score -1200.03 average score -578.97 epsilon 0.01\n",
            "episode  62 score -632.05 average score -579.81 epsilon 0.01\n",
            "episode  63 score -930.24 average score -585.28 epsilon 0.01\n",
            "episode  64 score -378.32 average score -582.10 epsilon 0.01\n",
            "episode  65 score -803.51 average score -585.45 epsilon 0.01\n",
            "episode  66 score -664.14 average score -586.63 epsilon 0.01\n",
            "episode  67 score -982.99 average score -592.46 epsilon 0.01\n",
            "episode  68 score -661.11 average score -593.45 epsilon 0.01\n",
            "episode  69 score -2059.98 average score -614.40 epsilon 0.01\n",
            "episode  70 score -399.23 average score -611.37 epsilon 0.01\n",
            "episode  71 score -702.70 average score -612.64 epsilon 0.01\n",
            "episode  72 score -636.26 average score -612.96 epsilon 0.01\n",
            "episode  73 score -1070.30 average score -619.14 epsilon 0.01\n",
            "episode  74 score -1033.84 average score -624.67 epsilon 0.01\n",
            "episode  75 score -2244.09 average score -645.98 epsilon 0.01\n",
            "episode  76 score -470.09 average score -643.70 epsilon 0.01\n",
            "episode  77 score -583.82 average score -642.93 epsilon 0.01\n",
            "episode  78 score -2365.42 average score -664.73 epsilon 0.01\n",
            "episode  79 score -795.23 average score -666.36 epsilon 0.01\n",
            "episode  80 score -941.41 average score -669.76 epsilon 0.01\n",
            "episode  81 score -556.96 average score -668.38 epsilon 0.01\n",
            "episode  82 score -518.31 average score -666.58 epsilon 0.01\n",
            "episode  83 score -943.38 average score -669.87 epsilon 0.01\n",
            "episode  84 score -700.61 average score -670.23 epsilon 0.01\n",
            "episode  85 score -1207.90 average score -676.49 epsilon 0.01\n",
            "episode  86 score -667.55 average score -676.38 epsilon 0.01\n",
            "episode  87 score -625.86 average score -675.81 epsilon 0.01\n",
            "episode  88 score -481.34 average score -673.62 epsilon 0.01\n",
            "episode  89 score -663.99 average score -673.52 epsilon 0.01\n",
            "episode  90 score -1570.17 average score -683.37 epsilon 0.01\n",
            "episode  91 score -581.33 average score -682.26 epsilon 0.01\n",
            "episode  92 score -2922.62 average score -706.35 epsilon 0.01\n",
            "episode  93 score -1198.87 average score -711.59 epsilon 0.01\n",
            "episode  94 score -367.79 average score -707.97 epsilon 0.01\n",
            "episode  95 score -465.51 average score -705.45 epsilon 0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-4dddee4a1eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-6a787c3a32c3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.Q_eval.state_dict(), \"/content/files/DQN_fine_tuned_files/tuned_baseline_no_extra_layer.zip\" )"
      ],
      "metadata": {
        "id": "YtdHGPFcswjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "import gym\n",
        "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/Noisey_pretrain.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v_2 = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v_2.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "print(dqn_torch_v_2.parameters())\n",
        "for param in dqn_torch_v_2.parameters():\n",
        "  param.requires_grad = False\n",
        "# num_ftrs = 64  # 8 states we have for the polly to move \n",
        "# num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "# dqn_torch_v_2.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v_2.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v_2.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def obs_to_torch(obs):\n",
        "    # TF: NHWC\n",
        "    # PyTorch: NCHW\n",
        "    # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "    # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "    # # Normalize\n",
        "    # obs = obs / 255.0\n",
        "    obs = th.tensor(obs).float()\n",
        "    obs = obs.to(device)\n",
        "    return obs\n",
        "\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "print(next(dqn_torch_v_2.parameters()).device)\n",
        "while not done:\n",
        "    action = th.argmax(dqn_torch_v_2(obs_to_torch(obs))).item()\n",
        "    # action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "\n",
        "print(episode_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc053ec-3d48-41a9-9910-71586dfaca37",
        "id": "4VBYovK0swjg"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Is CUDA supported by this system?True\n",
            "CUDA version: 11.3\n",
            "Name of current CUDA device:Tesla T4\n",
            "ID of current CUDA device:0\n",
            "cuda:0\n",
            "loaded model\n",
            "Parameter containing:\n",
            "tensor([[-0.2699,  0.2215, -0.2805,  ...,  0.0768,  0.2879,  0.3533],\n",
            "        [-0.0257, -0.2925, -0.0951,  ...,  0.1328,  0.1329, -0.3405],\n",
            "        [ 0.3069,  0.2371, -0.0466,  ..., -0.2950,  0.3136, -0.1654],\n",
            "        ...,\n",
            "        [ 0.2993, -0.2371, -0.1341,  ...,  0.1764,  0.0587, -0.2268],\n",
            "        [ 0.1419, -0.3017,  0.3421,  ...,  0.3309,  0.3151, -0.1235],\n",
            "        [-0.2089, -0.1852, -0.3029,  ..., -0.1262, -0.1147,  0.2384]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([-0.0336,  0.2579,  0.3049, -0.3168, -0.0405,  0.0551,  0.0542,  0.2759,\n",
            "         0.3215, -0.2255,  0.0892, -0.3376, -0.1296,  0.0549,  0.0086,  0.3483,\n",
            "        -0.3105, -0.0151, -0.1516, -0.0812,  0.2399, -0.0945, -0.0795,  0.1701,\n",
            "         0.0569, -0.1270,  0.1957,  0.0594,  0.0798,  0.3439,  0.2332,  0.2575,\n",
            "        -0.3462,  0.2634, -0.3212, -0.1445,  0.2926,  0.1328, -0.0704,  0.3341,\n",
            "        -0.2070,  0.2270, -0.1216,  0.3309,  0.3299, -0.3321, -0.3431, -0.1876,\n",
            "         0.2460, -0.2405, -0.0594,  0.1098,  0.0619, -0.1331, -0.0226,  0.0106,\n",
            "         0.2693, -0.2799, -0.1559,  0.1557, -0.3094, -0.0167, -0.1749, -0.2252,\n",
            "        -0.2997, -0.0884,  0.2729,  0.1063,  0.2781,  0.0420, -0.1359, -0.1636,\n",
            "        -0.1561, -0.1241, -0.1332,  0.1855, -0.3280,  0.1731,  0.2616,  0.0994,\n",
            "        -0.0322,  0.0901, -0.2865,  0.1353,  0.3408, -0.0473, -0.1434,  0.2903,\n",
            "         0.0677,  0.1557,  0.2872,  0.0385, -0.2873, -0.1876, -0.1646, -0.3522,\n",
            "        -0.1121,  0.0571,  0.0714,  0.2680, -0.0580, -0.2201, -0.2486,  0.2876,\n",
            "         0.0375, -0.0763, -0.0524, -0.1231,  0.2352,  0.2835, -0.1164,  0.3422,\n",
            "        -0.2345, -0.1577, -0.3509, -0.3399,  0.0260,  0.3076,  0.0837, -0.1969,\n",
            "         0.3497,  0.0974,  0.1489,  0.2827, -0.2747, -0.0516,  0.3323,  0.2814,\n",
            "         0.1320,  0.0699,  0.2809, -0.0457,  0.2268, -0.0130, -0.1604, -0.0925,\n",
            "        -0.3000, -0.0935,  0.0340,  0.0236, -0.1917,  0.0240,  0.2989,  0.0586,\n",
            "        -0.1174, -0.1682,  0.3185, -0.2086, -0.0210, -0.0636, -0.3340, -0.0155,\n",
            "         0.0991,  0.2144,  0.0567,  0.2103, -0.2173, -0.0564,  0.0525,  0.3312,\n",
            "         0.0679, -0.2402,  0.2016,  0.3393,  0.3408,  0.0808,  0.3329, -0.1862,\n",
            "        -0.1870, -0.2554,  0.3173, -0.2191,  0.0509, -0.2519, -0.2181, -0.1824,\n",
            "         0.0459,  0.2204,  0.2475,  0.0878,  0.0011,  0.2317,  0.3526, -0.0642,\n",
            "        -0.0062,  0.2188,  0.3320, -0.3254, -0.3214,  0.3010, -0.0947,  0.1782,\n",
            "        -0.3135, -0.0796,  0.2452, -0.1053,  0.2338,  0.1890, -0.1025,  0.3486,\n",
            "         0.0631,  0.0882, -0.0176,  0.1981, -0.3266,  0.2033,  0.0188, -0.3023,\n",
            "         0.2795,  0.3414,  0.1982, -0.3114, -0.0374,  0.3310,  0.2813, -0.3048,\n",
            "         0.0528, -0.1564, -0.1738, -0.0187, -0.1911, -0.2737,  0.2147, -0.2513,\n",
            "         0.2119, -0.0172, -0.1393, -0.1406, -0.0802, -0.1132, -0.0987, -0.0053,\n",
            "        -0.1186,  0.1322, -0.0319,  0.2155, -0.0330, -0.0953, -0.0801,  0.2992,\n",
            "         0.3217, -0.1825, -0.3405,  0.1533,  0.3523, -0.0699,  0.0331, -0.2227,\n",
            "        -0.1190,  0.1804, -0.0838,  0.2980,  0.1437,  0.3370,  0.3465,  0.0366],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1\n",
            "<generator object Module.parameters at 0x7f96e83c6bd0>\n",
            "cuda:0\n",
            "-58.75666139934117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "agent2 = Agent_lr(gamma=0.99, epsilon=0.99, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "              input_dims=[8], lr=0.001, eps_dec = 2e-4, parameters = dqn_torch_v_2.parameters())\n",
        "\n",
        "print(agent2.Q_eval.parameters)\n",
        "\n",
        "\n",
        "\n",
        "scores_2, eps_history = [], []\n",
        "n_games = 250\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "      action = agent2.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent2.store_transition(observation, action, reward, \n",
        "                              observation_, done)\n",
        "      agent2.learn()\n",
        "      observation = observation_\n",
        "    scores_2.append(score)\n",
        "    eps_history.append(agent2.epsilon)\n",
        "\n",
        "    avg_score = np.mean(scores_2[-100:])\n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "            'average score %.2f' % avg_score,\n",
        "            'epsilon %.2f' % agent2.epsilon)\n",
        "x_2 = [i+1 for i in range(n_games)]\n",
        "filename = 'lunar_lander.png'\n",
        "foo_2.plotLearning(x_2, scores_2, eps_history, filename)\n",
        "# x_np = np.array(x)[indices.astype(int)]\n",
        "# performance_metrics(x_np,scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "665a1615-5957-4511-975a-350b28c9a6af",
        "id": "jyRnjqGCswjh"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7f96e83c6d50>\n",
            "episode  0 score -242.63 average score -242.63 epsilon 0.98\n",
            "episode  1 score -94.87 average score -168.75 epsilon 0.97\n",
            "episode  2 score -25.59 average score -121.03 epsilon 0.95\n",
            "episode  3 score -81.04 average score -111.03 epsilon 0.93\n",
            "episode  4 score -8.69 average score -90.57 epsilon 0.92\n",
            "episode  5 score -108.38 average score -93.53 epsilon 0.91\n",
            "episode  6 score -75.85 average score -91.01 epsilon 0.89\n",
            "episode  7 score -303.04 average score -117.51 epsilon 0.87\n",
            "episode  8 score -106.80 average score -116.32 epsilon 0.86\n",
            "episode  9 score -53.66 average score -110.05 epsilon 0.84\n",
            "episode  10 score -504.63 average score -145.93 epsilon 0.81\n",
            "episode  11 score -419.55 average score -168.73 epsilon 0.80\n",
            "episode  12 score -96.77 average score -163.19 epsilon 0.78\n",
            "episode  13 score -151.60 average score -162.36 epsilon 0.76\n",
            "episode  14 score -247.41 average score -168.03 epsilon 0.75\n",
            "episode  15 score -454.56 average score -185.94 epsilon 0.73\n",
            "episode  16 score -289.66 average score -192.04 epsilon 0.71\n",
            "episode  17 score -105.90 average score -187.26 epsilon 0.70\n",
            "episode  18 score -124.60 average score -183.96 epsilon 0.68\n",
            "episode  19 score -247.18 average score -187.12 epsilon 0.67\n",
            "episode  20 score -363.76 average score -195.53 epsilon 0.65\n",
            "episode  21 score -206.61 average score -196.04 epsilon 0.64\n",
            "episode  22 score -477.41 average score -208.27 epsilon 0.62\n",
            "episode  23 score -373.26 average score -215.14 epsilon 0.60\n",
            "episode  24 score -459.62 average score -224.92 epsilon 0.58\n",
            "episode  25 score -302.66 average score -227.91 epsilon 0.57\n",
            "episode  26 score 8.73 average score -219.15 epsilon 0.56\n",
            "episode  27 score -462.52 average score -227.84 epsilon 0.54\n",
            "episode  28 score -544.50 average score -238.76 epsilon 0.52\n",
            "episode  29 score -391.87 average score -243.86 epsilon 0.51\n",
            "episode  30 score -251.47 average score -244.11 epsilon 0.49\n",
            "episode  31 score -502.13 average score -252.17 epsilon 0.48\n",
            "episode  32 score -425.56 average score -257.43 epsilon 0.46\n",
            "episode  33 score -359.95 average score -260.44 epsilon 0.44\n",
            "episode  34 score -335.19 average score -262.58 epsilon 0.43\n",
            "episode  35 score -452.90 average score -267.86 epsilon 0.41\n",
            "episode  36 score -555.72 average score -275.64 epsilon 0.40\n",
            "episode  37 score -506.02 average score -281.71 epsilon 0.38\n",
            "episode  38 score -598.28 average score -289.82 epsilon 0.36\n",
            "episode  39 score -555.43 average score -296.46 epsilon 0.35\n",
            "episode  40 score -409.48 average score -299.22 epsilon 0.32\n",
            "episode  41 score -519.45 average score -304.46 epsilon 0.31\n",
            "episode  42 score -373.95 average score -306.08 epsilon 0.29\n",
            "episode  43 score -579.57 average score -312.29 epsilon 0.28\n",
            "episode  44 score -429.74 average score -314.90 epsilon 0.27\n",
            "episode  45 score -508.71 average score -319.12 epsilon 0.25\n",
            "episode  46 score -528.21 average score -323.57 epsilon 0.24\n",
            "episode  47 score -569.56 average score -328.69 epsilon 0.22\n",
            "episode  48 score -448.76 average score -331.14 epsilon 0.21\n",
            "episode  49 score -358.64 average score -331.69 epsilon 0.19\n",
            "episode  50 score -571.69 average score -336.40 epsilon 0.18\n",
            "episode  51 score -761.33 average score -344.57 epsilon 0.16\n",
            "episode  52 score -704.54 average score -351.36 epsilon 0.15\n",
            "episode  53 score -220.43 average score -348.94 epsilon 0.14\n",
            "episode  54 score -831.63 average score -357.71 epsilon 0.12\n",
            "episode  55 score -533.13 average score -360.85 epsilon 0.11\n",
            "episode  56 score -583.75 average score -364.76 epsilon 0.09\n",
            "episode  57 score -463.66 average score -366.46 epsilon 0.08\n",
            "episode  58 score -577.01 average score -370.03 epsilon 0.06\n",
            "episode  59 score -374.13 average score -370.10 epsilon 0.05\n",
            "episode  60 score -846.73 average score -377.91 epsilon 0.03\n",
            "episode  61 score -461.76 average score -379.26 epsilon 0.02\n",
            "episode  62 score -574.19 average score -382.36 epsilon 0.01\n",
            "episode  63 score -424.65 average score -383.02 epsilon 0.01\n",
            "episode  64 score -488.43 average score -384.64 epsilon 0.01\n",
            "episode  65 score -403.54 average score -384.93 epsilon 0.01\n",
            "episode  66 score -442.35 average score -385.78 epsilon 0.01\n",
            "episode  67 score -391.55 average score -385.87 epsilon 0.01\n",
            "episode  68 score -372.58 average score -385.68 epsilon 0.01\n",
            "episode  69 score -812.48 average score -391.77 epsilon 0.01\n",
            "episode  70 score -675.76 average score -395.77 epsilon 0.01\n",
            "episode  71 score -721.43 average score -400.30 epsilon 0.01\n",
            "episode  72 score -468.72 average score -401.23 epsilon 0.01\n",
            "episode  73 score -637.28 average score -404.42 epsilon 0.01\n",
            "episode  74 score -611.48 average score -407.18 epsilon 0.01\n",
            "episode  75 score -484.66 average score -408.20 epsilon 0.01\n",
            "episode  76 score -369.66 average score -407.70 epsilon 0.01\n",
            "episode  77 score -494.03 average score -408.81 epsilon 0.01\n",
            "episode  78 score -804.87 average score -413.82 epsilon 0.01\n",
            "episode  79 score -462.60 average score -414.43 epsilon 0.01\n",
            "episode  80 score -463.20 average score -415.04 epsilon 0.01\n",
            "episode  81 score -438.73 average score -415.32 epsilon 0.01\n",
            "episode  82 score -724.82 average score -419.05 epsilon 0.01\n",
            "episode  83 score -720.44 average score -422.64 epsilon 0.01\n",
            "episode  84 score -545.09 average score -424.08 epsilon 0.01\n",
            "episode  85 score -344.12 average score -423.15 epsilon 0.01\n",
            "episode  86 score -325.92 average score -422.03 epsilon 0.01\n",
            "episode  87 score -808.09 average score -426.42 epsilon 0.01\n",
            "episode  88 score -320.00 average score -425.23 epsilon 0.01\n",
            "episode  89 score -1315.11 average score -435.11 epsilon 0.01\n",
            "episode  90 score -731.33 average score -438.37 epsilon 0.01\n",
            "episode  91 score -459.02 average score -438.59 epsilon 0.01\n",
            "episode  92 score -411.21 average score -438.30 epsilon 0.01\n",
            "episode  93 score -797.60 average score -442.12 epsilon 0.01\n",
            "episode  94 score -514.26 average score -442.88 epsilon 0.01\n",
            "episode  95 score -507.91 average score -443.56 epsilon 0.01\n",
            "episode  96 score -439.54 average score -443.52 epsilon 0.01\n",
            "episode  97 score -918.01 average score -448.36 epsilon 0.01\n",
            "episode  98 score -470.62 average score -448.58 epsilon 0.01\n",
            "episode  99 score -523.68 average score -449.33 epsilon 0.01\n",
            "episode  100 score -437.11 average score -451.28 epsilon 0.01\n",
            "episode  101 score -652.79 average score -456.86 epsilon 0.01\n",
            "episode  102 score -494.72 average score -461.55 epsilon 0.01\n",
            "episode  103 score -365.18 average score -464.39 epsilon 0.01\n",
            "episode  104 score -256.13 average score -466.86 epsilon 0.01\n",
            "episode  105 score -798.30 average score -473.76 epsilon 0.01\n",
            "episode  106 score -390.42 average score -476.91 epsilon 0.01\n",
            "episode  107 score -447.71 average score -478.36 epsilon 0.01\n",
            "episode  108 score -443.59 average score -481.72 epsilon 0.01\n",
            "episode  109 score -572.74 average score -486.91 epsilon 0.01\n",
            "episode  110 score -437.94 average score -486.25 epsilon 0.01\n",
            "episode  111 score -951.17 average score -491.56 epsilon 0.01\n",
            "episode  112 score -337.82 average score -493.97 epsilon 0.01\n",
            "episode  113 score -755.51 average score -500.01 epsilon 0.01\n",
            "episode  114 score -543.04 average score -502.97 epsilon 0.01\n",
            "episode  115 score -920.00 average score -507.62 epsilon 0.01\n",
            "episode  116 score -775.50 average score -512.48 epsilon 0.01\n",
            "episode  117 score -500.74 average score -516.43 epsilon 0.01\n",
            "episode  118 score -789.95 average score -523.08 epsilon 0.01\n",
            "episode  119 score -454.54 average score -525.16 epsilon 0.01\n",
            "episode  120 score -411.63 average score -525.64 epsilon 0.01\n",
            "episode  121 score -868.99 average score -532.26 epsilon 0.01\n",
            "episode  122 score -497.44 average score -532.46 epsilon 0.01\n",
            "episode  123 score -778.08 average score -536.51 epsilon 0.01\n",
            "episode  124 score -712.18 average score -539.04 epsilon 0.01\n",
            "episode  125 score -396.14 average score -539.97 epsilon 0.01\n",
            "episode  126 score -885.22 average score -548.91 epsilon 0.01\n",
            "episode  127 score -764.06 average score -551.92 epsilon 0.01\n",
            "episode  128 score -786.14 average score -554.34 epsilon 0.01\n",
            "episode  129 score -684.45 average score -557.27 epsilon 0.01\n",
            "episode  130 score -491.67 average score -559.67 epsilon 0.01\n",
            "episode  131 score -312.00 average score -557.77 epsilon 0.01\n",
            "episode  132 score -389.87 average score -557.41 epsilon 0.01\n",
            "episode  133 score -810.36 average score -561.91 epsilon 0.01\n",
            "episode  134 score -512.42 average score -563.69 epsilon 0.01\n",
            "episode  135 score -519.01 average score -564.35 epsilon 0.01\n",
            "episode  136 score -501.30 average score -563.80 epsilon 0.01\n",
            "episode  137 score -564.05 average score -564.38 epsilon 0.01\n",
            "episode  138 score -876.05 average score -567.16 epsilon 0.01\n",
            "episode  139 score -704.26 average score -568.65 epsilon 0.01\n",
            "episode  140 score -639.78 average score -570.95 epsilon 0.01\n",
            "episode  141 score -325.06 average score -569.01 epsilon 0.01\n",
            "episode  142 score -433.30 average score -569.60 epsilon 0.01\n",
            "episode  143 score -433.49 average score -568.14 epsilon 0.01\n",
            "episode  144 score -399.93 average score -567.84 epsilon 0.01\n",
            "episode  145 score -728.29 average score -570.04 epsilon 0.01\n",
            "episode  146 score -737.64 average score -572.13 epsilon 0.01\n",
            "episode  147 score -487.56 average score -571.31 epsilon 0.01\n",
            "episode  148 score -283.24 average score -569.66 epsilon 0.01\n",
            "episode  149 score -777.57 average score -573.85 epsilon 0.01\n",
            "episode  150 score -318.13 average score -571.31 epsilon 0.01\n",
            "episode  151 score -604.79 average score -569.75 epsilon 0.01\n",
            "episode  152 score -582.96 average score -568.53 epsilon 0.01\n",
            "episode  153 score -439.66 average score -570.72 epsilon 0.01\n",
            "episode  154 score -356.45 average score -565.97 epsilon 0.01\n",
            "episode  155 score -497.49 average score -565.62 epsilon 0.01\n",
            "episode  156 score -397.19 average score -563.75 epsilon 0.01\n",
            "episode  157 score -302.66 average score -562.14 epsilon 0.01\n",
            "episode  158 score -400.80 average score -560.38 epsilon 0.01\n",
            "episode  159 score -617.46 average score -562.81 epsilon 0.01\n",
            "episode  160 score -326.62 average score -557.61 epsilon 0.01\n",
            "episode  161 score -403.54 average score -557.03 epsilon 0.01\n",
            "episode  162 score -456.61 average score -555.85 epsilon 0.01\n",
            "episode  163 score -757.96 average score -559.19 epsilon 0.01\n",
            "episode  164 score -334.70 average score -557.65 epsilon 0.01\n",
            "episode  165 score -391.78 average score -557.53 epsilon 0.01\n",
            "episode  166 score -441.00 average score -557.52 epsilon 0.01\n",
            "episode  167 score -447.72 average score -558.08 epsilon 0.01\n",
            "episode  168 score -440.82 average score -558.76 epsilon 0.01\n",
            "episode  169 score -510.60 average score -555.74 epsilon 0.01\n",
            "episode  170 score -411.61 average score -553.10 epsilon 0.01\n",
            "episode  171 score -453.25 average score -550.42 epsilon 0.01\n",
            "episode  172 score -287.31 average score -548.60 epsilon 0.01\n",
            "episode  173 score -720.66 average score -549.44 epsilon 0.01\n",
            "episode  174 score -408.94 average score -547.41 epsilon 0.01\n",
            "episode  175 score -502.76 average score -547.59 epsilon 0.01\n",
            "episode  176 score -458.91 average score -548.49 epsilon 0.01\n",
            "episode  177 score -671.35 average score -550.26 epsilon 0.01\n",
            "episode  178 score -945.63 average score -551.67 epsilon 0.01\n",
            "episode  179 score -333.92 average score -550.38 epsilon 0.01\n",
            "episode  180 score -518.94 average score -550.94 epsilon 0.01\n",
            "episode  181 score -340.64 average score -549.96 epsilon 0.01\n",
            "episode  182 score -441.48 average score -547.12 epsilon 0.01\n",
            "episode  183 score -707.86 average score -547.00 epsilon 0.01\n",
            "episode  184 score -699.88 average score -548.55 epsilon 0.01\n",
            "episode  185 score -328.35 average score -548.39 epsilon 0.01\n",
            "episode  186 score -478.20 average score -549.91 epsilon 0.01\n",
            "episode  187 score -754.78 average score -549.38 epsilon 0.01\n",
            "episode  188 score -727.48 average score -553.45 epsilon 0.01\n",
            "episode  189 score -714.97 average score -547.45 epsilon 0.01\n",
            "episode  190 score -338.61 average score -543.52 epsilon 0.01\n",
            "episode  191 score -262.99 average score -541.56 epsilon 0.01\n",
            "episode  192 score -621.51 average score -543.67 epsilon 0.01\n",
            "episode  193 score -574.43 average score -541.44 epsilon 0.01\n",
            "episode  194 score -357.97 average score -539.87 epsilon 0.01\n",
            "episode  195 score -513.78 average score -539.93 epsilon 0.01\n",
            "episode  196 score -782.75 average score -543.36 epsilon 0.01\n",
            "episode  197 score -405.88 average score -538.24 epsilon 0.01\n",
            "episode  198 score -776.97 average score -541.31 epsilon 0.01\n",
            "episode  199 score -737.07 average score -543.44 epsilon 0.01\n",
            "episode  200 score -907.36 average score -548.14 epsilon 0.01\n",
            "episode  201 score -644.03 average score -548.05 epsilon 0.01\n",
            "episode  202 score -403.19 average score -547.14 epsilon 0.01\n",
            "episode  203 score -746.27 average score -550.95 epsilon 0.01\n",
            "episode  204 score -461.35 average score -553.00 epsilon 0.01\n",
            "episode  205 score -547.40 average score -550.49 epsilon 0.01\n",
            "episode  206 score -590.00 average score -552.49 epsilon 0.01\n",
            "episode  207 score -574.58 average score -553.76 epsilon 0.01\n",
            "episode  208 score -803.22 average score -557.35 epsilon 0.01\n",
            "episode  209 score -580.30 average score -557.43 epsilon 0.01\n",
            "episode  210 score -804.09 average score -561.09 epsilon 0.01\n",
            "episode  211 score -489.53 average score -556.47 epsilon 0.01\n",
            "episode  212 score -691.84 average score -560.01 epsilon 0.01\n",
            "episode  213 score -444.55 average score -556.91 epsilon 0.01\n",
            "episode  214 score -550.97 average score -556.98 epsilon 0.01\n",
            "episode  215 score -365.68 average score -551.44 epsilon 0.01\n",
            "episode  216 score -596.59 average score -549.65 epsilon 0.01\n",
            "episode  217 score -790.35 average score -552.55 epsilon 0.01\n",
            "episode  218 score -725.18 average score -551.90 epsilon 0.01\n",
            "episode  219 score -448.11 average score -551.84 epsilon 0.01\n",
            "episode  220 score -797.70 average score -555.70 epsilon 0.01\n",
            "episode  221 score -772.08 average score -554.73 epsilon 0.01\n",
            "episode  222 score -761.83 average score -557.37 epsilon 0.01\n",
            "episode  223 score -478.69 average score -554.38 epsilon 0.01\n",
            "episode  224 score -623.82 average score -553.49 epsilon 0.01\n",
            "episode  225 score -497.57 average score -554.51 epsilon 0.01\n",
            "episode  226 score -693.21 average score -552.59 epsilon 0.01\n",
            "episode  227 score -512.51 average score -550.07 epsilon 0.01\n",
            "episode  228 score -536.19 average score -547.57 epsilon 0.01\n",
            "episode  229 score -454.42 average score -545.27 epsilon 0.01\n",
            "episode  230 score -505.85 average score -545.41 epsilon 0.01\n",
            "episode  231 score -436.83 average score -546.66 epsilon 0.01\n",
            "episode  232 score -531.15 average score -548.08 epsilon 0.01\n",
            "episode  233 score -524.26 average score -545.22 epsilon 0.01\n",
            "episode  234 score -479.00 average score -544.88 epsilon 0.01\n",
            "episode  235 score -447.73 average score -544.17 epsilon 0.01\n",
            "episode  236 score -394.58 average score -543.10 epsilon 0.01\n",
            "episode  237 score -456.81 average score -542.03 epsilon 0.01\n",
            "episode  238 score -722.73 average score -540.50 epsilon 0.01\n",
            "episode  239 score -471.79 average score -538.17 epsilon 0.01\n",
            "episode  240 score -700.96 average score -538.78 epsilon 0.01\n",
            "episode  241 score -334.25 average score -538.87 epsilon 0.01\n",
            "episode  242 score -534.00 average score -539.88 epsilon 0.01\n",
            "episode  243 score -413.09 average score -539.68 epsilon 0.01\n",
            "episode  244 score -766.92 average score -543.35 epsilon 0.01\n",
            "episode  245 score -360.77 average score -539.67 epsilon 0.01\n",
            "episode  246 score -655.50 average score -538.85 epsilon 0.01\n",
            "episode  247 score -532.77 average score -539.30 epsilon 0.01\n",
            "episode  248 score -778.12 average score -544.25 epsilon 0.01\n",
            "episode  249 score -635.07 average score -542.83 epsilon 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent_2.Q_eval)\n",
        "torch.save(agent2.Q_eval.state_dict(), \"/content/files/DQN_fine_tuned_files/DQN_tuned_nosiey_no_layer.zip\")"
      ],
      "metadata": {
        "id": "OoBJ8g39swjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZFGdYjWDhDG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "all_model_rewards = {\n",
        "    \"agent\": {\n",
        "        \"x\": np.array(x_1),\n",
        "        \"y\": np.array(scores_1),\n",
        "    },\n",
        "    \"agent2\": {\n",
        "        \"x\": np.array(x_2),\n",
        "        \"y\": np.array(scores_2),\n",
        "    }\n",
        "}\n",
        "\n",
        "# def generate_sample_models(number_of_models):\n",
        "#     '''\n",
        "#     This is function which generates sample models.\n",
        "#     '''\n",
        "#     for n in range(number_of_models):\n",
        "#         reward_dict = {}\n",
        "#         model_version_num = 'DQN' + str(n)\n",
        "#         x = np.linspace(1,100,100) ## these are the timesteps\n",
        "#         y = random.rand(100)\n",
        "#         reward_dict = {'x': x, 'y' : y, 'comment': 'here comes the comment'}\n",
        "#         all_model_rewards[model_version_num] = reward_dict\n",
        "\n",
        "# generate_sample_models(all_model_rewards)"
      ],
      "metadata": {
        "id": "pOKgF-LxDy-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_evs = {}\n",
        "\n",
        "def model_evaluations(all_model_rewards): \n",
        "    \n",
        "    '''\n",
        "    This is a function performing model evaluations. \n",
        "    \n",
        "    Input: a dict in the following format:\n",
        "    {'DQN5': {'x': array([  1.,   2.,   3.,  ....]),\n",
        "             'y': array([0.47025739, 0.5788533 , 0.72454499,...]),\n",
        "             'comment': 'comment for the model'}}\n",
        "             \n",
        "             \n",
        "             \n",
        "    The function performing the following metrics:\n",
        "    \n",
        "    \n",
        "    1. Max reward. \n",
        "    2. Jumpstart: The initial performance of an agent in a target task may be improved by transfer from a source task.\n",
        "    3. Asymptotic Performance: The final learned performance of an agent in the target task may be improved via transfer.\n",
        "    4. Total Reward: The total reward accumulated by an agent (i.e., the area under the learning curve) may be improved if it uses transfer, compared to learning without transfer.\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    max_reward = {}\n",
        "    jumpstart = {}\n",
        "    total_reward = {}\n",
        "    asymptotic_performance = {}\n",
        "    \n",
        "    for m in all_model_rewards.keys():\n",
        "        \n",
        "        max_reward[m] = all_model_rewards[m]['y'].max()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate jumpstart. \n",
        "        ######### It should start from the beginning of the array. \n",
        "        ####################################\n",
        "        jumpstart[m] =  all_model_rewards[m]['y'][:10].mean() \n",
        "        \n",
        "        total_reward[m] =  all_model_rewards[m]['y'].sum()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate asymptotic performance. \n",
        "        ######### It end at the end of the session.\n",
        "        ####################################\n",
        "        asymptotic_performance[m] =  all_model_rewards[m]['y'][-10:].sum()\n",
        "\n",
        "        \n",
        "        \n",
        "    ## Add the metrics to the final evaluation metric \n",
        "    \n",
        "    model_evs['max_reward'] = max_reward\n",
        "    model_evs['jumpstart'] = jumpstart\n",
        "    model_evs['total_reward'] = total_reward\n",
        "    model_evs['asymptotic_performance'] = asymptotic_performance\n",
        "\n",
        "    # return model_evs\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "model_evaluations(all_model_rewards)  \n",
        "results = pd.DataFrame(model_evs)\n",
        "results.style.highlight_max(color = 'lightgreen', axis = 0)"
      ],
      "metadata": {
        "id": "ODnN1EwXD1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "ac217944-6c06-4a1b-811c-a501ee5cb850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1a38fa1fd0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_6fbb3_row0_col0, #T_6fbb3_row0_col3, #T_6fbb3_row1_col1, #T_6fbb3_row1_col2 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_6fbb3_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >max_reward</th>\n",
              "      <th class=\"col_heading level0 col1\" >jumpstart</th>\n",
              "      <th class=\"col_heading level0 col2\" >total_reward</th>\n",
              "      <th class=\"col_heading level0 col3\" >asymptotic_performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbb3_level0_row0\" class=\"row_heading level0 row0\" >agent</th>\n",
              "      <td id=\"T_6fbb3_row0_col0\" class=\"data row0 col0\" >187.168604</td>\n",
              "      <td id=\"T_6fbb3_row0_col1\" class=\"data row0 col1\" >-204.372757</td>\n",
              "      <td id=\"T_6fbb3_row0_col2\" class=\"data row0 col2\" >-21338.272950</td>\n",
              "      <td id=\"T_6fbb3_row0_col3\" class=\"data row0 col3\" >564.390009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbb3_level0_row1\" class=\"row_heading level0 row1\" >agent2</th>\n",
              "      <td id=\"T_6fbb3_row1_col0\" class=\"data row1 col0\" >-1.998443</td>\n",
              "      <td id=\"T_6fbb3_row1_col1\" class=\"data row1 col1\" >-133.902764</td>\n",
              "      <td id=\"T_6fbb3_row1_col2\" class=\"data row1 col2\" >-1436.851850</td>\n",
              "      <td id=\"T_6fbb3_row1_col3\" class=\"data row1 col3\" >-1093.739594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# peformance matrix\n",
        "\n",
        "model_evaluations()"
      ],
      "metadata": {
        "id": "6YaibuklpShc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "8e73ace7-272b-4caf-9631-27fe30ff385f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c0baad2a86da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# peformance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_evaluations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_evaluations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_ratio(transfer_learner, scratch_model): \n",
        "\n",
        "    ''' Transfer Ratio: The ratio of the total reward accumulated by the transfer learner and the total reward accumulated by the non-transfer learner.'''\n",
        "\n",
        "    transfer_ratio = (transfer_learner['y']/scratch_model['y']).sum()\n",
        "    return (transfer_ratio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def time_to_threshold(transfer_learner, scratch_model):\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    Time to Threshold: The learning time needed by the agent to achieve a pre-specified perfor- mance level may be reduced via knowledge transfer.\n",
        "\n",
        "\n",
        "    This function returns the first timestep of the transfer_learning model when it reaches the scratch\n",
        "    model's maximum value. The threshold could be changed to \n",
        "        - any fixed number\n",
        "        - ratio of the scratch model's maximum reward\n",
        "        - the average of the final performance (averaged over the last n timesteps) of the scratch model. \n",
        "    '''\n",
        "    \n",
        "    threshold = scratch_model['y'].max()\n",
        "    threshold_index = np.where(transfer_learner['y'] >= threshold)[0][0]\n",
        "    \n",
        "    \n",
        "    return 'Transfer learner\\'s performance reaches the threashold (scratch model\\'s max performance) at timestap {}. Threshold is {}'.format(threshold_index, threshold)\n",
        "    \n",
        "transfer_ratio( all_model_rewards[\"agent2\"], all_model_rewards[\"agent\"])  \n",
        "\n",
        "time_to_threshold(all_model_rewards[\"agent2\"], all_model_rewards[\"agent\"])"
      ],
      "metadata": {
        "id": "OS3lNNLXQc2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def jumpstart(y):\n",
        "    return (y[0])\n",
        "\n",
        "\n",
        "def asymptotic_performance(y, rolling_avg=1):\n",
        "    asymptotic_performance = np.mean(y[-rolling_avg:])\n",
        "    return asymptotic_performance\n",
        "\n",
        "\n",
        "def total_reward_fnc(y):\n",
        "    return np.sum(y)\n",
        "\n",
        "\n",
        "def transfer_ratio(y_transfer, y_reference, total_reward):\n",
        "    transfer_ratio = total_reward(y_transfer) / total_reward(y_reference)\n",
        "    return transfer_ratio\n",
        "\n",
        "\n",
        "def time_to_threshold(x, y, threshold=250):\n",
        "    return x[np.argmax(y > threshold)]\n",
        "\n",
        "\n",
        "def max_reward_fnc(y):\n",
        "    return np.max(y), np.argmax(y)\n",
        "\n",
        "\n",
        "def performance_metrics(x, y, rolling_avg=10, threshold=275, plot=True):\n",
        "    '''\n",
        "    TODO: write plots functions\n",
        "    '''\n",
        "\n",
        "    max_reward, idx_training_max = max_reward_fnc(y)\n",
        "    total_reward = total_reward_fnc(y)\n",
        "    n_timesteps = max(x)\n",
        "    reward_per_timestep = total_reward / n_timesteps\n",
        "    asymptote = asymptotic_performance(y, rolling_avg=rolling_avg)\n",
        "    std_asymptote = np.std(y[-rolling_avg:])\n",
        "    asymptote_after_max = asymptotic_performance(y, rolling_avg=n_timesteps - idx_training_max)\n",
        "    std_asymptote_after_max = np.std(y[-(n_timesteps - idx_training_max):])\n",
        "    time_threshold = time_to_threshold(x, y, threshold=threshold)\n",
        "    threshold_80 = 0.80 * threshold\n",
        "    time_threshold_80_of_max = time_to_threshold(x, y, threshold=threshold_80)\n",
        "    print(0.80 * max_reward)\n",
        "\n",
        "    print(f\" Max reward: {max_reward} \\\n",
        "        \\n Time steps to max reward: {idx_training_max}/{n_timesteps} ({idx_training_max * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Total reward: {total_reward:.2f} \\\n",
        "        \\n Reward per timestep: {reward_per_timestep:.2f} \\\n",
        "        \\n Asymptotic performance (last {rolling_avg} timesteps): {asymptote:.2f} ±{std_asymptote:.2f} ({asymptote * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Asymptotic performance after max: {asymptote_after_max:.2f} ±{std_asymptote_after_max:.2f} ({asymptote_after_max * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Time to threshold(={threshold}): {time_threshold}/{n_timesteps} ({time_threshold * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Time to threshold(80% of max={threshold_80}): {time_threshold_80_of_max}/{n_timesteps} ({time_threshold_80_of_max * 100 / n_timesteps:.2f}% of training period)\" \\\n",
        "          )\n",
        "\n",
        "    if plot == True:\n",
        "        sns.scatterplot(x=x, y=y, color='green')\n",
        "        plt.title('Reward Over Timesteps 2')\n",
        "        plt.xlabel('Episode Index')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.show()\n",
        "\n",
        "        # sns.barplot(data=[asymptote])\n",
        "        # plt.show()\n",
        "        numpy_data = ([threshold, time_threshold_80_of_max])\n",
        "\n",
        "        plt.bar([0, 1], [time_threshold, time_threshold_80_of_max], color=['green', 'blue'])\n",
        "        plt.xticks([0, 1], labels=['Time to Max Threshold', 'Time to 80% max threshold'])\n",
        "        plt.title('Threshold Time')\n",
        "        plt.ylabel('Episode Number')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "XlZF54uNVHNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hKvPv0bV8Z5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Double DQN for Stability test"
      ],
      "metadata": {
        "id": "Jr8gMmqu8dji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from collections import namedtuple, deque\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import gym\n",
        "from datetime import datetime\n",
        "import pandas as pd  \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import itertools"
      ],
      "metadata": {
        "id": "hbSZY--F9VI6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        \n",
        "#         print('state:', state.shape)\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "pWQ1syo7J9Yp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, buffer_size, batch_size, priority=False):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer (chosen as multiple of num agents)\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
        "        self.next_states = torch.zeros((buffer_size,)+(state_size,)).to(device)\n",
        "        self.actions = torch.zeros(buffer_size,1, dtype=torch.long).to(device)\n",
        "        self.rewards = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n",
        "        self.dones = torch.zeros(buffer_size, 1, dtype=torch.float).to(device)\n",
        "        self.e = np.zeros((buffer_size, 1), dtype=np.float)\n",
        "        \n",
        "        self.priority = priority\n",
        "\n",
        "        self.ptr = 0\n",
        "        self.n = 0\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        self.states[self.ptr] = torch.from_numpy(state).to(device)\n",
        "        self.next_states[self.ptr] = torch.from_numpy(next_state).to(device)\n",
        "        \n",
        "        self.actions[self.ptr] = torch.from_numpy(np.asarray(action)).to(device)\n",
        "        self.rewards[self.ptr] = torch.from_numpy(np.asarray(reward)).to(device)\n",
        "        self.dones[self.ptr] = done\n",
        "#         self.actions[self.ptr] = action\n",
        "#         self.rewards[self.ptr] = reward\n",
        "#         self.dones[self.ptr] = done\n",
        "        \n",
        "        self.ptr += 1\n",
        "        if self.ptr >= self.buffer_size:\n",
        "            self.ptr = 0\n",
        "            self.n = self.buffer_size\n",
        "\n",
        "    def sample(self, get_all=False):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        n = len(self)\n",
        "        if get_all:\n",
        "            return self.states[:n], self.actions[:n], self.rewards[:n], self.next_states[:n], self.dones[:n]\n",
        "        # else:\n",
        "        if self.priority:\n",
        "            idx = np.random.choice(n, self.batch_size, replace=False, p=self.e)\n",
        "        else:\n",
        "            idx = np.random.choice(n, self.batch_size, replace=False)\n",
        "        \n",
        "        states = self.states[idx]\n",
        "        next_states = self.next_states[idx]\n",
        "        actions = self.actions[idx]\n",
        "        rewards = self.rewards[idx]\n",
        "        dones = self.dones[idx]\n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones), idx\n",
        "      \n",
        "    def update_error(self, e, idx=None):\n",
        "        e = torch.abs(e.detach())\n",
        "        e = e / e.sum()\n",
        "        if idx is not None:\n",
        "            self.e[idx] = e.cpu().numpy()\n",
        "        else:\n",
        "            self.e[:len(self)] = e.cpu().numpy()\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.n == 0:\n",
        "            return self.ptr\n",
        "        else:\n",
        "            return self.n"
      ],
      "metadata": {
        "id": "1we-49uIKDIE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate \n",
        "UPDATE_EVERY = 4        # how often to update the network"
      ],
      "metadata": {
        "id": "xDnA_jzqKIu3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQNAgent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed=42, ddqn=True, priority=False):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.ddqn = ddqn\n",
        "\n",
        "        # Q-Network\n",
        "#         self.qnetwork_local = model(state_size[0], action_size, seed).to(device)\n",
        "#         self.qnetwork_target = model(state_size[0], action_size, seed).to(device)\n",
        "        \n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(state_size, (action_size,), BUFFER_SIZE, BATCH_SIZE)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences, idx = self.memory.sample()\n",
        "                e = self.learn(experiences)\n",
        "                self.memory.update_error(e, idx)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "          \n",
        "    def update_error(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(get_all=True)\n",
        "        with torch.no_grad():\n",
        "            if self.ddqn:\n",
        "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
        "                actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
        "                maxQ = self.qnetwork_target(next_states).gather(-1, actions)\n",
        "                target = rewards+GAMMA*maxQ*(1-dones)\n",
        "            else: # Normal DQN\n",
        "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
        "                target = rewards+GAMMA*maxQ*(1-dones)\n",
        "                old_val = self.qnetwork_local(states).gather(-1, actions)\n",
        "            e = old_val - target\n",
        "            self.memory.update_error(e)\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ## compute and minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        if self.ddqn:\n",
        "            old_val = self.qnetwork_local(states).gather(-1, actions)\n",
        "            with torch.no_grad():\n",
        "                next_actions = self.qnetwork_local(next_states).argmax(-1, keepdim=True)\n",
        "                maxQ = self.qnetwork_target(next_states).gather(-1, next_actions)\n",
        "                target = rewards+GAMMA*maxQ*(1-dones)\n",
        "        else: # Normal DQN\n",
        "            with torch.no_grad():\n",
        "                maxQ = self.qnetwork_target(next_states).max(-1, keepdim=True)[0]\n",
        "                target = rewards+GAMMA*maxQ*(1-dones)\n",
        "            old_val = self.qnetwork_local(states).gather(-1, actions)   \n",
        "        \n",
        "        loss = F.mse_loss(old_val, target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
        "        \n",
        "        return old_val - target\n",
        "\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "metadata": {
        "id": "XsCzFdVXKMwl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs=['LunarLander-v4']\n",
        "ls=[[True,'DQN'],[True,'DDQN']]\n",
        "n_episodes=500\n",
        "max_t=1000\n",
        "eps_start=1.0\n",
        "eps_end=0.01\n",
        "eps_decay=0.995"
      ],
      "metadata": {
        "id": "gTookgzGKVXN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0;\n",
        "\n",
        "env = gym.make(envs[i])\n",
        "# env.seed(0)\n",
        "res=[]\n",
        "for j in ls:\n",
        "    \n",
        "    rewards = []\n",
        "    aver_reward = []\n",
        "    aver = deque(maxlen=100)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size=env.action_space.n\n",
        "    agent = DDQNAgent(state_size, action_size, 1, ddqn=j[0], priority=j[1])\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in tqdm(range(1, n_episodes+1)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "\n",
        "        aver.append(score)     \n",
        "        aver_reward.append(np.mean(aver))\n",
        "        rewards.append(score)\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "    # reward=\"model/0\"+\"_\"+j[0]+\"_\"+str(n_episodes)+\"_\"+str(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "    torch.save(agent.qnetwork_local.state_dict(),'rewards.pt')\n",
        "    res.append(aver_reward)\n"
      ],
      "metadata": {
        "id": "IoIK_g9qKjIU",
        "outputId": "ffefcba1-5885-4d55-f349-97fefffa5924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            " 86%|████████▌ | 428/500 [09:48<03:12,  2.67s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig=plt.figure()   \n",
        "\n",
        "reward='./'+'result'\n",
        "df=pd.DataFrame({'DQN':res[0],'DDQN':res[1]})\n",
        "df.to_csv(reward+'.csv')\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.plot(df['DQN'], 'r', label='DQN')\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.plot(df['DDQN'], 'orange',label='DDQN')\n",
        "\n",
        "# plt.xlabel(\"Episode\")\n",
        "# plt.ylabel(\"Reward\")\n",
        "# plt.plot(df['Priority'],'b',label='PER')\n",
        "\n",
        "plt.title('Learning Curve')\n",
        "\n",
        "# agent.save('Model_dqn.h5')\n",
        "fig.legend(loc='lower right')\n",
        "fig.savefig(reward+'.png', dpi=100)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Ww2YeAregynh",
        "outputId": "8929e69d-53dc-4783-8862-684faa9f1a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making the DDQN and DQN for transfer learning the last hidden layer only for the weights might improve the peformance"
      ],
      "metadata": {
        "id": "dgrunoeViHIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "cuda_id = torch.cuda.current_device()\n",
        "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "import gym\n",
        "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/Noisey_pretrain.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v_2 = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v_2.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "print(dqn_torch_v_2.parameters())\n",
        "for param in dqn_torch_v_2.parameters():\n",
        "  param.requires_grad = False\n",
        "# num_ftrs = 64  # 8 states we have for the polly to move \n",
        "# num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "# dqn_torch_v_2.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v_2.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v_2.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def obs_to_torch(obs):\n",
        "    # TF: NHWC\n",
        "    # PyTorch: NCHW\n",
        "    # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "    # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "    # # Normalize\n",
        "    # obs = obs / 255.0\n",
        "    obs = th.tensor(obs).float()\n",
        "    obs = obs.to(device)\n",
        "    return obs\n",
        "\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "\n",
        "episode_reward = 0\n",
        "done = False\n",
        "obs = env.reset()\n",
        "print(next(dqn_torch_v_2.parameters()).device)\n",
        "while not done:\n",
        "    action = th.argmax(dqn_torch_v_2(obs_to_torch(obs))).item()\n",
        "    # action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "\n",
        "print(episode_reward)"
      ],
      "metadata": {
        "id": "epF6zBzS9kBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E9kBveaqiGIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DJsZv6GTiG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v4')\n",
        "agent2 = Agent_lr(gamma=0.99, epsilon=0.99, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "              input_dims=[8], lr=0.001, eps_dec = 2e-4)\n",
        "\n",
        "\n",
        "\n",
        "scores_2, eps_history = [], []\n",
        "n_games = 250\n",
        "\n",
        "for i in range(n_games):\n",
        "    score = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "      action = agent2.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent2.store_transition(observation, action, reward, \n",
        "                              observation_, done)\n",
        "      agent2.learn()\n",
        "      observation = observation_\n",
        "    scores_2.append(score)\n",
        "    eps_history.append(agent2.epsilon)\n",
        "\n",
        "    avg_score = np.mean(scores_2[-100:])\n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "            'average score %.2f' % avg_score,\n",
        "            'epsilon %.2f' % agent2.epsilon)\n",
        "x_2 = [i+1 for i in range(n_games)]\n",
        "filename = 'lunar_lander.png'\n",
        "foo_2.plotLearning(x_2, scores_2, eps_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "W_ojZkMQ9vHk",
        "outputId": "2f833542-9b13-4d53-f7c0-c2012596dd4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score -276.90 average score -276.90 epsilon 0.98\n",
            "episode  1 score -66.21 average score -171.56 epsilon 0.97\n",
            "episode  2 score -127.14 average score -156.75 epsilon 0.94\n",
            "episode  3 score -305.33 average score -193.90 epsilon 0.92\n",
            "episode  4 score -71.25 average score -169.37 epsilon 0.90\n",
            "episode  5 score -390.62 average score -206.24 epsilon 0.89\n",
            "episode  6 score -2.00 average score -177.07 epsilon 0.86\n",
            "episode  7 score -69.44 average score -163.61 epsilon 0.84\n",
            "episode  8 score -20.45 average score -147.70 epsilon 0.82\n",
            "episode  9 score -9.69 average score -133.90 epsilon 0.80\n",
            "episode  10 score -52.04 average score -126.46 epsilon 0.78\n",
            "episode  11 score -45.78 average score -119.74 epsilon 0.77\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b4720b325baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       agent2.store_transition(observation, action, reward, \n\u001b[1;32m     24\u001b[0m                               observation_, done)\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscores_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-f0914af010ea>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# from Model import Model\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Parameters\n",
        "use_cuda = True\n",
        "episode_limit = 100\n",
        "target_update_delay = 2  # update target net every target_update_delay episodes\n",
        "test_delay = 10\n",
        "learning_rate = 1e-4\n",
        "epsilon = 1  # initial epsilon\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.9 / 2.5e3\n",
        "gamma = 0.99\n",
        "memory_len = 10000\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "# n_features = len(env.observation_space.high)\n",
        "# n_actions = env.action_space.n\n",
        "\n",
        "n_features = 8\n",
        "n_actions = 4\n",
        "memory = deque(maxlen=memory_len)\n",
        "# each memory entry is in form: (state, action, env_reward, next_state)\n",
        "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.MSELoss()\n",
        "policy_net = foo_1.DeepQNetwork(input_dims=[8], n_actions=4, lr = learning_rate, fc1_dims = 256, fc2_dims = 256 ).to(device)\n",
        "target_net = foo_1.DeepQNetwork(input_dims=[8], n_actions =4, lr = learning_rate, fc1_dims = 256, fc2_dims = 256).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "\n",
        "def get_states_tensor(sample, states_idx):\n",
        "    sample_len = len(sample)\n",
        "    states_tensor = torch.empty((sample_len, n_features), dtype=torch.float32, requires_grad=False)\n",
        "\n",
        "    features_range = range(n_features)\n",
        "    for i in range(sample_len):\n",
        "        for j in features_range:\n",
        "            states_tensor[i, j] = sample[i][states_idx][j].item()\n",
        "\n",
        "    return states_tensor\n",
        "\n",
        "\n",
        "def normalize_state(state):\n",
        "    state[0] /= 2.5\n",
        "    state[1] /= 2.5\n",
        "    state[2] /= 0.3\n",
        "    state[3] /= 0.3\n",
        "\n",
        "\n",
        "def state_reward(state, env_reward):\n",
        "    return env_reward - (abs(state[0]) + abs(state[2])) / 2.5\n",
        "\n",
        "\n",
        "def get_action(state, e=min_epsilon):\n",
        "    if random.random() < e:\n",
        "        # explore\n",
        "        action = random.randrange(0, n_actions)\n",
        "    else:\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "        action = policy_net(state).argmax().item()\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def fit(model, inputs, labels):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    train_ds = TensorDataset(inputs, labels)\n",
        "    train_dl = DataLoader(train_ds, batch_size=5)\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in train_dl:\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return total_loss / len(inputs)\n",
        "\n",
        "\n",
        "def optimize_model(train_batch_size=100):\n",
        "    train_batch_size = min(train_batch_size, len(memory))\n",
        "    train_sample = random.sample(memory, train_batch_size)\n",
        "\n",
        "    state = get_states_tensor(train_sample, 0)\n",
        "    next_state = get_states_tensor(train_sample, 3)\n",
        "\n",
        "    q_estimates = policy_net(state.to(device)).detach()\n",
        "    next_state_q_estimates = target_net(next_state.to(device)).detach()\n",
        "    next_actions = policy_net(next_state.to(device)).argmax(dim=1)\n",
        "\n",
        "    for i in range(len(train_sample)):\n",
        "        next_action = next_actions[i].item()\n",
        "        q_estimates[i][train_sample[i][1]] = (state_reward(next_state[i], train_sample[i][2]) +\n",
        "                                              gamma * next_state_q_estimates[i][next_action].item())\n",
        "\n",
        "    fit(policy_net, state, q_estimates)\n",
        "\n",
        "\n",
        "def train_one_episode():\n",
        "    global epsilon\n",
        "    current_state = env.reset()\n",
        "    normalize_state(current_state)\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        action = get_action(current_state, epsilon)\n",
        "        next_state, env_reward, done, _ = env.step(action)\n",
        "        normalize_state(next_state)\n",
        "        memory.append((current_state, action, env_reward, next_state))\n",
        "        current_state = next_state\n",
        "        score += env_reward\n",
        "        reward += state_reward(next_state, env_reward)\n",
        "\n",
        "        optimize_model(100)\n",
        "\n",
        "        epsilon -= epsilon_decay\n",
        "\n",
        "    return score, reward\n",
        "\n",
        "\n",
        "def test():\n",
        "    state = env.reset()\n",
        "    normalize_state(state)\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        action = get_action(state)\n",
        "        state, env_reward, done, _ = env.step(action)\n",
        "        normalize_state(state)\n",
        "        score += env_reward\n",
        "        reward += state_reward(state, env_reward)\n",
        "\n",
        "    return score, reward\n",
        "\n",
        "\n",
        "# def main():\n",
        "best_test_reward = 0\n",
        "\n",
        "for i in range(episode_limit):\n",
        "    score, reward = train_one_episode()\n",
        "\n",
        "    print(f'Episode {i + 1}: score: {score} - reward: {reward}')\n",
        "\n",
        "    if i % target_update_delay == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        target_net.eval()\n",
        "\n",
        "    if (i + 1) % test_delay == 0:\n",
        "        test_score, test_reward = test()\n",
        "        print(f'Test Episode {i + 1}: test score: {test_score} - test reward: {test_reward}')\n",
        "        if test_reward > best_test_reward:\n",
        "            print('New best test reward. Saving model')\n",
        "            best_test_reward = test_reward\n",
        "            torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
        "\n",
        "if episode_limit % test_delay != 0:\n",
        "    test_score, test_reward = test()\n",
        "    print(f'Test Episode {episode_limit}: test score: {test_score} - test reward: {test_reward}')\n",
        "    if test_reward > best_test_reward:\n",
        "        print('New best test reward. Saving model')\n",
        "        best_test_reward = test_reward\n",
        "        torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
        "\n",
        "print(f'best test reward: {best_test_reward}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OegwF3OE8rld",
        "outputId": "d5cd8a3d-4b48-4792-d357-1d81ed23a4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: score: -287.33929915880435 - reward: -418.67842412447214\n",
            "Episode 2: score: -40.45757791416044 - reward: -46.96675247156315\n",
            "Episode 3: score: -108.10030073879943 - reward: -128.7718823677181\n",
            "Episode 4: score: -220.51426749892954 - reward: -292.05148721404794\n",
            "Episode 5: score: -368.7479114075836 - reward: -431.6079856788453\n",
            "Episode 6: score: -25.620954310130273 - reward: -76.17734335584562\n",
            "Episode 7: score: -44.19219721716584 - reward: -70.5089625531131\n",
            "Episode 8: score: -19.20200169422756 - reward: -54.36856657602921\n",
            "Episode 9: score: -98.64207122947074 - reward: -153.47374483968113\n",
            "Episode 10: score: -143.42918180520923 - reward: -213.7526202588835\n",
            "Test Episode 10: test score: -322.3872672008298 - test reward: -407.5009698720596\n",
            "Episode 11: score: -41.07934389665634 - reward: -111.36649480417282\n",
            "Episode 12: score: -157.92888266817693 - reward: -223.5923302777207\n",
            "Episode 13: score: -137.58934109573374 - reward: -190.13649216060645\n",
            "Episode 14: score: -37.338537621488 - reward: -92.1810848370094\n",
            "Episode 15: score: -109.20264719814715 - reward: -150.3197785416311\n",
            "Episode 16: score: -273.04979036980484 - reward: -353.6288494178018\n",
            "Episode 17: score: -249.7450881941827 - reward: -320.0478086242995\n",
            "Episode 18: score: -393.5700875871572 - reward: -603.543055135825\n",
            "Episode 19: score: -244.19549863037014 - reward: -559.7948821186654\n",
            "Episode 20: score: -132.28583030264952 - reward: -216.8680190843729\n",
            "Test Episode 20: test score: -266.4159428427343 - test reward: -570.3788801530682\n",
            "Episode 21: score: -264.3986710913628 - reward: -529.2142771080239\n",
            "Episode 22: score: -230.5304183019539 - reward: -338.37028504891293\n",
            "Episode 23: score: -204.01034161370777 - reward: -439.70945820581625\n",
            "Episode 24: score: -217.71963089781417 - reward: -300.8059121486011\n",
            "Episode 25: score: -360.6132901257274 - reward: -415.5603935784099\n",
            "Episode 26: score: -181.52971385876572 - reward: -276.7997272865001\n",
            "Episode 27: score: -217.77574686970937 - reward: -288.03238273630035\n",
            "Episode 28: score: -205.76373345487906 - reward: -319.7342626916137\n",
            "Episode 29: score: -153.01083443267692 - reward: -417.81897561802066\n",
            "Episode 30: score: -127.041746516523 - reward: -207.6352283496383\n",
            "Test Episode 30: test score: -179.68107458524395 - test reward: -457.4589258211086\n",
            "Episode 31: score: -286.7612327382266 - reward: -563.9038873618772\n",
            "Episode 32: score: -82.246281051858 - reward: -174.77048645041754\n",
            "Episode 33: score: -240.46971340873657 - reward: -381.37307787635746\n",
            "Episode 34: score: -208.6975923895093 - reward: -370.1912157978097\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-40097035000c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Episode {i + 1}: score: {score} - reward: {reward}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-40097035000c>\u001b[0m in \u001b[0;36mtrain_one_episode\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-40097035000c>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(train_batch_size)\u001b[0m\n\u001b[1;32m    108\u001b[0m                                               gamma * next_state_q_estimates[i][next_action].item())\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_estimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-40097035000c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, inputs, labels)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"If capturable=True, params and state_steps must be CUDA tensors.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"If capturable=False, state_steps should not be CUDA tensors.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of Exporting_module.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}