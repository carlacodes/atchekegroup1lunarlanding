{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "9EanvrrhobbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1b3f16-45b8-41e8-aac7-0eb34b47d54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 177 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 448 kB 6.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "pE3qZJLcobbW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor,RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPoZnSTGFvEp",
        "outputId": "489d7c9e-db70-415c-fc65-d589cf586f51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "fp1bUnClobbY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "vJm_qzWkobbg"
      },
      "outputs": [],
      "source": [
        "nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
        "                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
        "\n",
        "learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.\n",
        "                      #Tip: Use smaller step-sizes for larger networks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the obstacle Env for Fine tuning "
      ],
      "metadata": {
        "id": "zb9VfXeFLkbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.box2d import LunarLander\n",
        "from Box2D.b2 import fixtureDef, circleShape, polygonShape, revoluteJointDef, contactListener, edgeShape\n",
        "import math"
      ],
      "metadata": {
        "id": "NfxblZzztMHP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400"
      ],
      "metadata": {
        "id": "E6Ivff2jW7q8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        if (\n",
        "                self.env.lander == contact.fixtureA.body\n",
        "                or self.env.lander == contact.fixtureB.body\n",
        "        ):\n",
        "            self.env.game_over = True\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = False\n",
        "\n",
        "class Custom_LunarLander_obs(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        obs_coords = [10, 10],\n",
        "        enable_obstacle: bool = True\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "\n",
        "        self.obs_coords = obs_coords\n",
        "        self.enable_obstacle = enable_obstacle\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        # defining the polygon obstacle here:\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, \n",
        "                                  pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8  \n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
        "        )   \n",
        "\n",
        "    def reset(self):\n",
        "        self._destroy()\n",
        "        self.world.contactListener_keepref = ContactDetector(self)\n",
        "        self.world.contactListener = self.world.contactListener_keepref\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        # terrain\n",
        "        CHUNKS = 11\n",
        "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
        "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
        "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
        "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
        "        self.helipad_y = H / 4\n",
        "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
        "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
        "        smooth_y = [\n",
        "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
        "            for i in range(CHUNKS)\n",
        "        ]\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "\n",
        "        # defining the polygon obstacle here----------------------------------\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        self.sky_polys = []\n",
        "        for i in range(CHUNKS - 1):\n",
        "            p1 = (chunk_x[i], smooth_y[i])\n",
        "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
        "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
        "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
        "\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        initial_y = VIEWPORT_H / SCALE\n",
        "        self.lander = self.world.CreateDynamicBody(\n",
        "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
        "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
        "        self.lander.ApplyForceToCenter(\n",
        "            (\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "            ),\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        self.legs = []\n",
        "        for i in [-1, +1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001,\n",
        "                ),\n",
        "            )\n",
        "            leg.ground_contact = False\n",
        "            leg.color1 = (0.5, 0.4, 0.9)\n",
        "            leg.color2 = (0.3, 0.3, 0.5)\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=self.lander,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(0, 0),\n",
        "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
        "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
        "            )\n",
        "            if i == -1:\n",
        "                rjd.lowerAngle = (\n",
        "                        +0.9 - 0.5\n",
        "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
        "                rjd.upperAngle = +0.9\n",
        "            else:\n",
        "                rjd.lowerAngle = -0.9\n",
        "                rjd.upperAngle = -0.9 + 0.5\n",
        "            leg.joint = self.world.CreateJoint(rjd)\n",
        "            self.legs.append(leg)\n",
        "\n",
        "        self.drawlist = [self.lander] + self.legs\n",
        "        \n",
        "\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        # print([pos.x,pos.y])\n",
        "        vel = self.lander.linearVelocity\n",
        "      \n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "\n",
        "            # (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2), # 8: x position\n",
        "            # (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2), # 9: y position\n",
        "\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        state_8 = (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2);\n",
        "        state_9 = (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2);\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        # distance_to_obstacle = np.sqrt((pos.x - (self.obs_coords[0] +\n",
        "        #                                     VIEWPORT_W / SCALE / 2)) ** 2 +\n",
        "        #                         (pos.y - (self.obs_coords[1] +\n",
        "\n",
        "        #                                   (self.helipad_y + LEG_DOWN / SCALE))) ** 2)\n",
        "        distance_to_obstacle = np.sqrt(state_8 * state_8 + state_9 * state_9)\n",
        "\n",
        "        # if (distance_to_obstacle <= (1)):\n",
        "        #     print('dangerously close to obstacle!')\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            - 125 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - 75 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "            - 75 * (distance_to_obstacle <= ((20 + 10) / SCALE)) # obstacles radius and the polly radius  \n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0 : # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        from gym.envs.classic_control import rendering\n",
        "\n",
        "        if self.viewer is None:\n",
        "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
        "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "        # print('drawlist')\n",
        "        # print(self.drawlist)\n",
        "        for p in self.sky_polys:\n",
        "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
        "        # editing below line to draw obstacle\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((100, 100))  # Position\n",
        "                    # self.viewer.draw_circle(20).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
        "\n",
        "        for obj2 in [self.obstacle]:\n",
        "            # print('rendering obstacle')\n",
        "            # print(obj2)\n",
        "            for f in obj2.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    # print('printing circle of radius')\n",
        "                    #t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    t = rendering.Transform((self.obs_coords[0], self.obs_coords[1]))\n",
        "                    # print(f.shape.radius)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((10, 10))  # Position\n",
        "                    # self.viewer.draw_circle(2).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj2.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj2.color2, linewidth=2)\n",
        "\n",
        "        for x in [self.helipad_x1, self.helipad_x2]:\n",
        "            flagy1 = self.helipad_y\n",
        "            flagy2 = flagy1 + 50 / SCALE\n",
        "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
        "            self.viewer.draw_polygon(\n",
        "                [\n",
        "                    (x, flagy2),\n",
        "                    (x, flagy2 - 10 / SCALE),\n",
        "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
        "                ],\n",
        "                color=(0.8, 0.8, 0),\n",
        "            )\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "MAwXYRAQhpvg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env2 = Custom_LunarLander_obs()\n",
        "new_env2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c3504d-1729-4995-f108-aeae02f40e42",
        "id": "dxrSUxAgiRPg"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Custom_LunarLander_obs at 0x7ff8b25a4950>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "check_env(new_env2)"
      ],
      "metadata": {
        "id": "5yacX9tliV_Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for env in gym.envs.registration.registry.env_specs.copy():\n",
        "#     if 'LunarLander-v3' in env:\n",
        "#         print(\"Remove {} from registry\".format(env))\n",
        "#         del gym.envs.registration.registry.env_specs[env]"
      ],
      "metadata": {
        "id": "nNOEgfEolroH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander-v4\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_obs,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "pI_O6IZfi6RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577eacff-a67c-487f-8add-8e9d27718e65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment LunarLander-v4\u001b[0m\n",
            "  logger.warn(\"Overriding environment {}\".format(id))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting the Baseline Model to Pytorch and Fine tuning the Last Layer Using the Obstacle Environment"
      ],
      "metadata": {
        "id": "-zXPpu7LQpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "print('run pytorch model')\n",
        "import gym\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from stable_baselines3 import DQN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLbWfW6K8tpF",
        "outputId": "7b1709a7-f4e5-439f-bc34-b5418fb390bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run pytorch model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/Hasanaldhahi3/atchekegroup1lunarlanding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Og3pc49huE",
        "outputId": "d2c1c0b5-6775-4730-c704-522e0a34d59f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'atchekegroup1lunarlanding'...\n",
            "remote: Enumerating objects: 739, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 739 (delta 55), reused 42 (delta 42), pack-reused 678\u001b[K\n",
            "Receiving objects: 100% (739/739), 24.48 MiB | 22.01 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "# log_dir_ = dir_prefix + \"DQN_Youtube/\"\n",
        "# os.makedirs(log_dir_, exist_ok=True)\n",
        "spec=importlib.util.spec_from_file_location(\"DeepQNetwork\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_1 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_1)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"plotLearning\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/utils.py\")\n",
        "foo_2 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_2)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"Agent\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_3 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_3)\n"
      ],
      "metadata": {
        "id": "CsJZwSVj9U_e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_prefix = \"./files/\"\n",
        "log_dir_obstacle = dir_prefix + \"DQN_fine_tuned_Noisey/\"\n",
        "os.makedirs(log_dir_obstacle, exist_ok=True)"
      ],
      "metadata": {
        "id": "K5MHt4nN0SzO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "# cuda_id = torch.cuda.current_device()\n",
        "# print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "# import gym\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/baseline_weights.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "print(dqn_torch_v.parameters())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for param in dqn_torch_v.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = 64  # 8 states we have for the polly to move \n",
        "num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "dqn_torch_v.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# def obs_to_torch(obs):\n",
        "#     # TF: NHWC\n",
        "#     # PyTorch: NCHW\n",
        "#     # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "#     # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "#     # # Normalize\n",
        "#     # obs = obs / 255.0\n",
        "#     obs = th.tensor(obs).float()\n",
        "#     obs = obs.to(device)\n",
        "#     return obs\n",
        "\n",
        "\n",
        "# env = gym.make('LunarLander-v4')\n",
        "\n",
        "# episode_reward = 0\n",
        "# done = False\n",
        "# obs = env.reset()\n",
        "# print(next(dqn_torch_v.parameters()).device)\n",
        "# while not done:\n",
        "#     action = th.argmax(dqn_torch_v(obs_to_torch(obs))).item()\n",
        "#     # action = env.action_space.sample()\n",
        "#     obs, reward, done, _ = env.step(action)\n",
        "#     episode_reward += reward\n",
        "\n",
        "# print(episode_reward)"
      ],
      "metadata": {
        "id": "47vjrBZP9FAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24db898d-3674-4cda-ff93-629eb884ab7b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Is CUDA supported by this system?False\n",
            "CUDA version: 11.3\n",
            "cpu\n",
            "loaded model\n",
            "Parameter containing:\n",
            "tensor([[-0.2699,  0.2215, -0.2805,  ...,  0.0768,  0.2879,  0.3533],\n",
            "        [-0.0257, -0.2925, -0.0951,  ...,  0.1328,  0.1329, -0.3405],\n",
            "        [ 0.3069,  0.2371, -0.0466,  ..., -0.2950,  0.3136, -0.1654],\n",
            "        ...,\n",
            "        [ 0.2993, -0.2371, -0.1341,  ...,  0.1764,  0.0587, -0.2268],\n",
            "        [ 0.1419, -0.3017,  0.3421,  ...,  0.3309,  0.3151, -0.1235],\n",
            "        [-0.2089, -0.1852, -0.3029,  ..., -0.1262, -0.1147,  0.2384]],\n",
            "       requires_grad=True)\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([-0.0336,  0.2579,  0.3049, -0.3168, -0.0405,  0.0551,  0.0542,  0.2759,\n",
            "         0.3215, -0.2255,  0.0892, -0.3376, -0.1296,  0.0549,  0.0086,  0.3483,\n",
            "        -0.3105, -0.0151, -0.1516, -0.0812,  0.2399, -0.0945, -0.0795,  0.1701,\n",
            "         0.0569, -0.1270,  0.1957,  0.0594,  0.0798,  0.3439,  0.2332,  0.2575,\n",
            "        -0.3462,  0.2634, -0.3212, -0.1445,  0.2926,  0.1328, -0.0704,  0.3341,\n",
            "        -0.2070,  0.2270, -0.1216,  0.3309,  0.3299, -0.3321, -0.3431, -0.1876,\n",
            "         0.2460, -0.2405, -0.0594,  0.1098,  0.0619, -0.1331, -0.0226,  0.0106,\n",
            "         0.2693, -0.2799, -0.1559,  0.1557, -0.3094, -0.0167, -0.1749, -0.2252,\n",
            "        -0.2997, -0.0884,  0.2729,  0.1063,  0.2781,  0.0420, -0.1359, -0.1636,\n",
            "        -0.1561, -0.1241, -0.1332,  0.1855, -0.3280,  0.1731,  0.2616,  0.0994,\n",
            "        -0.0322,  0.0901, -0.2865,  0.1353,  0.3408, -0.0473, -0.1434,  0.2903,\n",
            "         0.0677,  0.1557,  0.2872,  0.0385, -0.2873, -0.1876, -0.1646, -0.3522,\n",
            "        -0.1121,  0.0571,  0.0714,  0.2680, -0.0580, -0.2201, -0.2486,  0.2876,\n",
            "         0.0375, -0.0763, -0.0524, -0.1231,  0.2352,  0.2835, -0.1164,  0.3422,\n",
            "        -0.2345, -0.1577, -0.3509, -0.3399,  0.0260,  0.3076,  0.0837, -0.1969,\n",
            "         0.3497,  0.0974,  0.1489,  0.2827, -0.2747, -0.0516,  0.3323,  0.2814,\n",
            "         0.1320,  0.0699,  0.2809, -0.0457,  0.2268, -0.0130, -0.1604, -0.0925,\n",
            "        -0.3000, -0.0935,  0.0340,  0.0236, -0.1917,  0.0240,  0.2989,  0.0586,\n",
            "        -0.1174, -0.1682,  0.3185, -0.2086, -0.0210, -0.0636, -0.3340, -0.0155,\n",
            "         0.0991,  0.2144,  0.0567,  0.2103, -0.2173, -0.0564,  0.0525,  0.3312,\n",
            "         0.0679, -0.2402,  0.2016,  0.3393,  0.3408,  0.0808,  0.3329, -0.1862,\n",
            "        -0.1870, -0.2554,  0.3173, -0.2191,  0.0509, -0.2519, -0.2181, -0.1824,\n",
            "         0.0459,  0.2204,  0.2475,  0.0878,  0.0011,  0.2317,  0.3526, -0.0642,\n",
            "        -0.0062,  0.2188,  0.3320, -0.3254, -0.3214,  0.3010, -0.0947,  0.1782,\n",
            "        -0.3135, -0.0796,  0.2452, -0.1053,  0.2338,  0.1890, -0.1025,  0.3486,\n",
            "         0.0631,  0.0882, -0.0176,  0.1981, -0.3266,  0.2033,  0.0188, -0.3023,\n",
            "         0.2795,  0.3414,  0.1982, -0.3114, -0.0374,  0.3310,  0.2813, -0.3048,\n",
            "         0.0528, -0.1564, -0.1738, -0.0187, -0.1911, -0.2737,  0.2147, -0.2513,\n",
            "         0.2119, -0.0172, -0.1393, -0.1406, -0.0802, -0.1132, -0.0987, -0.0053,\n",
            "        -0.1186,  0.1322, -0.0319,  0.2155, -0.0330, -0.0953, -0.0801,  0.2992,\n",
            "         0.3217, -0.1825, -0.3405,  0.1533,  0.3523, -0.0699,  0.0331, -0.2227,\n",
            "        -0.1190,  0.1804, -0.0838,  0.2980,  0.1437,  0.3370,  0.3465,  0.0366],\n",
            "       requires_grad=True)\n",
            "1\n",
            "<generator object Module.parameters at 0x7ff8b2460f50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('LunarLander-v4')\n",
        "    agent = foo_3.Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "                  input_dims=[8], lr=0.001)\n",
        "    scores, eps_history = [], []\n",
        "    n_games = 10\n",
        "    \n",
        "    for i in range(n_games):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "\n",
        "        avg_score = np.mean(scores[-100:])\n",
        "\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "                'average score %.2f' % avg_score,\n",
        "                'epsilon %.2f' % agent.epsilon)\n",
        "    x = [i+1 for i in range(n_games)]\n",
        "    filename = 'lunar_lander.png'\n",
        "    foo_2.plotLearning(x, scores, eps_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "oMihRr6Q0sES",
        "outputId": "484080eb-b7d1-4a3c-fa34-4f7ecf07f556"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score -106.64 average score -106.64 epsilon 0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  state = T.tensor([observation]).to(self.Q_eval.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  1 score -263.97 average score -185.30 epsilon 0.93\n",
            "episode  2 score -336.99 average score -235.87 epsilon 0.87\n",
            "episode  3 score -280.50 average score -247.02 epsilon 0.82\n",
            "episode  4 score -114.93 average score -220.61 epsilon 0.79\n",
            "episode  5 score -134.34 average score -206.23 epsilon 0.74\n",
            "episode  6 score -44.49 average score -183.12 epsilon 0.69\n",
            "episode  7 score -45.92 average score -165.97 epsilon 0.64\n",
            "episode  8 score -86.04 average score -157.09 epsilon 0.55\n",
            "episode  9 score -84.69 average score -149.85 epsilon 0.50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"/content/sample_data/DQN_tuned_noisy.zip\"\n",
        "\n",
        "# torch.save(agent.state_dict(), \"/content/sample_data/DQN_tuned_noisy.zip\")\n",
        "# agent.load_state_dict(torch.load('agent_weights.pth'))\n",
        "\n",
        "print(agent.Q_eval)\n",
        "\n",
        "# _, weights = get_model_weights(agent.Q_eval, \"jked\")\n",
        "torch.save(agent.Q_eval.state_dict(),  \"./sample_data/DQN_tuned_noisy.zip\")\n",
        "\n",
        "\n",
        "# torch.save(agent.Q_eval,\"/content/sample_data/DQN_tuned_noisy.zip\" )\n",
        "# print(list(agent.parameters()))\n",
        "\n",
        "# import pickle\n",
        "# pickle.dump(, open(filename, 'wb'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "wDOVWK2Q1PM3",
        "outputId": "3a95a1c9-a56e-452c-fa12-b3af1b68ece0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepQNetwork(\n",
            "  (fc1): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            "  (loss): MSELoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_weights(model, model_name):\n",
        "  ''' This is a function to check the model weights. One can compare trained and untraind models' weights in this way.'''\n",
        "  \n",
        "\n",
        "  #print ('Sanity check. Comparing model weights for', model_names, models)\n",
        "  #for model, model_name in zip(models, model_names): \n",
        "  weight_dict = {}\n",
        "  keys = [item for item in model.state_dict()]\n",
        "  for key in keys:\n",
        "    weight_dict[key] = torch.sum(model.policy.state_dict()[key])\n",
        "  return model_name, weight_dict"
      ],
      "metadata": {
        "id": "3xlJTptslhA9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def jumpstart(y):\n",
        "    return (y[0])\n",
        "\n",
        "\n",
        "def asymptotic_performance(y, rolling_avg=1):\n",
        "    asymptotic_performance = np.mean(y[-rolling_avg:])\n",
        "    return asymptotic_performance\n",
        "\n",
        "\n",
        "def total_reward_fnc(y):\n",
        "    return np.sum(y)\n",
        "\n",
        "\n",
        "def transfer_ratio(y_transfer, y_reference, total_reward):\n",
        "    transfer_ratio = total_reward(y_transfer) / total_reward(y_reference)\n",
        "    return transfer_ratio\n",
        "\n",
        "\n",
        "def time_to_threshold(x, y, threshold=250):\n",
        "    return x[np.argmax(y > threshold)]\n",
        "\n",
        "\n",
        "def max_reward_fnc(y):\n",
        "    return np.max(y), np.argmax(y)\n",
        "\n",
        "\n",
        "def performance_metrics(x, y, rolling_avg=10, threshold=275, plot=True):\n",
        "    '''\n",
        "    TODO: write plots functions\n",
        "    '''\n",
        "\n",
        "    max_reward, idx_training_max = max_reward_fnc(y)\n",
        "    total_reward = total_reward_fnc(y)\n",
        "    n_timesteps = x.max()\n",
        "    reward_per_timestep = total_reward / n_timesteps\n",
        "    asymptote = asymptotic_performance(y, rolling_avg=rolling_avg)\n",
        "    std_asymptote = np.std(y[-rolling_avg:])\n",
        "    asymptote_after_max = asymptotic_performance(y, rolling_avg=n_timesteps - idx_training_max)\n",
        "    std_asymptote_after_max = np.std(y[-(n_timesteps - idx_training_max):])\n",
        "    time_threshold = time_to_threshold(x, y, threshold=threshold)\n",
        "    threshold_80 = 0.80 * threshold\n",
        "    time_threshold_80_of_max = time_to_threshold(x, y, threshold=threshold_80)\n",
        "    print(0.80 * max_reward)\n",
        "\n",
        "    print(f\" Max reward: {max_reward} \\\n",
        "        \\n Time steps to max reward: {idx_training_max}/{n_timesteps} ({idx_training_max * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Total reward: {total_reward:.2f} \\\n",
        "        \\n Reward per timestep: {reward_per_timestep:.2f} \\\n",
        "        \\n Asymptotic performance (last {rolling_avg} timesteps): {asymptote:.2f} ±{std_asymptote:.2f} ({asymptote * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Asymptotic performance after max: {asymptote_after_max:.2f} ±{std_asymptote_after_max:.2f} ({asymptote_after_max * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Time to threshold(={threshold}): {time_threshold}/{n_timesteps} ({time_threshold * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Time to threshold(80% of max={threshold_80}): {time_threshold_80_of_max}/{n_timesteps} ({time_threshold_80_of_max * 100 / n_timesteps:.2f}% of training period)\" \\\n",
        "          )\n",
        "\n",
        "    if plot == True:\n",
        "        sns.scatterplot(x=x, y=y, color='green')\n",
        "        plt.title('Reward Over Timesteps 2')\n",
        "        plt.xlabel('Episode Index')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.show()\n",
        "\n",
        "        # sns.barplot(data=[asymptote])\n",
        "        # plt.show()\n",
        "        numpy_data = ([threshold, time_threshold_80_of_max])\n",
        "\n",
        "        plt.bar([0, 1], [time_threshold, time_threshold_80_of_max], color=['green', 'blue'])\n",
        "        plt.xticks([0, 1], labels=['Time to Max Threshold', 'Time to 80% max threshold'])\n",
        "        plt.title('Threshold Time')\n",
        "        plt.ylabel('Episode Number')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "XlZF54uNVHNN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EZFGdYjWDhDG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_rewards = {\n",
        "    \"Agent\": agent\n",
        "}\n",
        "\n",
        "def generate_sample_models(number_of_models):\n",
        "    '''\n",
        "    This is function which generates sample models.\n",
        "    '''\n",
        "    for n in range(number_of_models):\n",
        "        reward_dict = {}\n",
        "        model_version_num = 'DQN' + str(n)\n",
        "        x = np.linspace(1,100,100) ## these are the timesteps\n",
        "        y = random.rand(100)\n",
        "        reward_dict = {'x': x, 'y' : y, 'comment': 'here comes the comment'}\n",
        "        all_model_rewards[model_version_num] = reward_dict\n",
        "generate_sample_models(10)"
      ],
      "metadata": {
        "id": "pOKgF-LxDy-y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluations(all_model_rewards): \n",
        "    \n",
        "    '''\n",
        "    This is a function performing model evaluations. \n",
        "    \n",
        "    Input: a dict in the following format:\n",
        "    {'DQN5': {'x': array([  1.,   2.,   3.,  ....]),\n",
        "             'y': array([0.47025739, 0.5788533 , 0.72454499,...]),\n",
        "             'comment': 'comment for the model'}}\n",
        "             \n",
        "             \n",
        "             \n",
        "    The function performing the following metrics:\n",
        "    \n",
        "    \n",
        "    1. Max reward. \n",
        "    2. Jumpstart: The initial performance of an agent in a target task may be improved by transfer from a source task.\n",
        "    3. Asymptotic Performance: The final learned performance of an agent in the target task may be improved via transfer.\n",
        "    4. Total Reward: The total reward accumulated by an agent (i.e., the area under the learning curve) may be improved if it uses transfer, compared to learning without transfer.\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    max_reward = {}\n",
        "    jumpstart = {}\n",
        "    total_reward = {}\n",
        "    asymptotic_performance = {}\n",
        "    \n",
        "    for m in all_model_rewards.keys():\n",
        "        \n",
        "        max_reward[m] = all_model_rewards[m]['y'].max()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate jumpstart. \n",
        "        ######### It should start from the beginning of the array. \n",
        "        ####################################\n",
        "        jumpstart[m] =  all_model_rewards[m]['y'][:10].mean() \n",
        "        \n",
        "        total_reward[m] =  all_model_rewards[m]['y'].sum()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate asymptotic performance. \n",
        "        ######### It end at the end of the session.\n",
        "        ####################################\n",
        "        asymptotic_performance[m] =  all_model_rewards[m]['y'][-10:].sum()\n",
        "\n",
        "        \n",
        "        \n",
        "    ## Add the metrics to the final evaluation metric \n",
        "    \n",
        "    model_evs['max_reward'] = max_reward\n",
        "    model_evs['jumpstart'] = jumpstart\n",
        "    model_evs['total_reward'] = total_reward\n",
        "    model_evs['asymptotic_performance'] = asymptotic_performance\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "model_evaluations(all_model_rewards)  \n",
        "results = pd.DataFrame(model_evs)\n",
        "results.style.highlight_max(color = 'lightgreen', axis = 0)"
      ],
      "metadata": {
        "id": "ODnN1EwXD1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f9153fba-3249-4366-d7d1-9ff1f2095f55"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-c8556c8396dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mmodel_evaluations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_evs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighlight_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lightgreen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-c8556c8396dc>\u001b[0m in \u001b[0;36mmodel_evaluations\u001b[0;34m(all_model_rewards)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_model_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmax_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_model_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Agent' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_ratio(transfer_learner, scratch_model): \n",
        "\n",
        "    ''' Transfer Ratio: The ratio of the total reward accumulated by the transfer learner and the total reward accumulated by the non-transfer learner.'''\n",
        "\n",
        "    transfer_ratio = (transfer_learner['y']/scratch_model['y']).sum()\n",
        "    return (transfer_ratio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def time_to_threshold(transfer_learner, scratch_model):\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    Time to Threshold: The learning time needed by the agent to achieve a pre-specified perfor- mance level may be reduced via knowledge transfer.\n",
        "\n",
        "\n",
        "    This function returns the first timestep of the transfer_learning model when it reaches the scratch\n",
        "    model's maximum value. The threshold could be changed to \n",
        "        - any fixed number\n",
        "        - ratio of the scratch model's maximum reward\n",
        "        - the average of the final performance (averaged over the last n timesteps) of the scratch model. \n",
        "    '''\n",
        "    \n",
        "    threshold = scratch_model['y'].max()\n",
        "    threshold_index = np.where(transfer_learner['y'] >= threshold)[0][0]\n",
        "    \n",
        "    \n",
        "    return 'Transfer learner\\'s performance reaches the threashold (scratch model\\'s max performance) at timestap {}. Threshold is {}'.format(threshold_index, threshold)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "OS3lNNLXQc2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Exporting_module.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}