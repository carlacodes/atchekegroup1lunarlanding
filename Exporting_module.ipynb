{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!sudo apt-get update > /dev/null 2>&1\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install rarfile --quiet\n",
    "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
    "!pip install box2d-py --quiet\n",
    "!pip install gym pyvirtualdisplay --quiet\n",
    "\n",
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor,RecordVideo\n",
    "\n",
    "print(gym.__version__)\n",
    "\n",
    "# @title Plotting/Video functions\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment\n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else:\n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  # env = RecordVideo(env, './video')\n",
    "  return env\n",
    "\n",
    "class LoggingCallback:\n",
    "    def __init__(self,threshold,trial_number,patience):\n",
    "      '''\n",
    "      threshold: int tolerance for increase in reward\n",
    "      trial_number: int Prune after minimum number of trials\n",
    "      patience: int patience for the threshold\n",
    "      '''\n",
    "      self.threshold = threshold\n",
    "      self.trial_number  = trial_number\n",
    "      self.patience = patience\n",
    "      self.cb_list = [] #Trials list for which threshold is reached\n",
    "    def __call__(self,study:optuna.study, frozen_trial:optuna.Trial):\n",
    "      #Setting the best value in the current trial\n",
    "      study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "      \n",
    "      #Checking if the minimum number of trials have pass\n",
    "      if frozen_trial.number >self.trial_number:\n",
    "          previous_best_value = study.user_attrs.get(\"previous_best_value\",None)\n",
    "          #Checking if the previous and current objective values have the same sign\n",
    "          if previous_best_value * study.best_value >=0:\n",
    "              #Checking for the threshold condition\n",
    "              if abs(previous_best_value-study.best_value) < self.threshold: \n",
    "                  self.cb_list.append(frozen_trial.number)\n",
    "                  #If threshold is achieved for the patience amount of time\n",
    "                  if len(self.cb_list)>self.patience:\n",
    "                      print('The study stops now...')\n",
    "                      print('With number',frozen_trial.number ,'and value ',frozen_trial.value)\n",
    "                      print('The previous and current best values are {} and {} respectively'\n",
    "                              .format(previous_best_value, study.best_value))\n",
    "                      study.stop()\n",
    "\n",
    "log_dir = \"models\"\n",
    "os.makedirs(log_dir,exist_ok=True) \n",
    "\n",
    "def objective(trial:optuna.Trial):\n",
    "\n",
    "  # Create environment\n",
    "  env = gym.make('LunarLander-v2')\n",
    "  env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "\n",
    "  #Trial will suggest a set of hyperparamters from the specified range\n",
    "  hyperparameters = sample_dqn_params(trial)\n",
    "  model_dqn = DQN(\"MlpPolicy\", env, **hyperparameters) #Set verbose to 1 to observe training logs. We encourage you to set the verbose to 1.\n",
    "\n",
    "  # define learning steps\n",
    "  trained_dqn = model_dqn.learn(total_timesteps=1, log_interval=10)#100000 , callback=callback\n",
    "  # save model\n",
    "  trained_dqn.save('models/dqn_{}.pth'.format(trial.number)) \n",
    "\n",
    "  x, y = ts2xy(load_results(log_dir), 'episodes') # timesteps\n",
    "  # clear_output(wait=True)\n",
    "  #For the given hyperparamters, determine reward\n",
    "  reward = sum(y)\n",
    "  return reward\n",
    "\n",
    "#Create a study object and specify the direction as 'maximize'\n",
    "#As you want to maximize reward\n",
    "#Pruner stops not promising iterations\n",
    "#Use a pruner, else you will get error related to divergence of model\n",
    "#You can also use Multivariate samplere\n",
    "#sampler = optuna.samplers.TPESampler(multivarite=True,seed=42)\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(study_name=\"dqn_study\",direction='maximize',\n",
    "                            sampler = sampler, pruner=optuna.pruners.HyperbandPruner())\n",
    "\n",
    "logging_callback = LoggingCallback(threshold=10, patience=30, trial_number=5)\n",
    "#You can increase the n_trials for a better search space scanning\n",
    "study.optimize(objective, n_trials=4, catch=(ValueError,),callbacks=[logging_callback])\n",
    "\n",
    "nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
    "                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
    "\n",
    "learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.\n",
    "                      #Tip: Use smaller step-sizes for larger networks.\n",
    "\n",
    "## save the model \n",
    "dir_prefix = \"./files/\"\n",
    "log_dir_windy = dir_prefix + \"DQN_windy/\"\n",
    "os.makedirs(log_dir_windy, exist_ok=True)\n",
    "\n",
    "model1.save(log_dir_windy + \"DQN_windy_model.zip\")\n",
    "\n",
    "\n",
    "\n",
    "dir_prefix = \"./files/\"\n",
    "log_dir_obstacle = dir_prefix + \"DQN_obstacle/\"\n",
    "os.makedirs(log_dir_obstacle, exist_ok=True)\n",
    "\n",
    "model2.save(log_dir_obstacle + \"DQN_obstacle_model.zip\")\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "print('run pytorch model')\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!git clone  https://github.com/Hasanaldhahi3/atchekegroup1lunarlanding.git\n",
    "\n",
    "import importlib.util\n",
    "# log_dir_ = dir_prefix + \"DQN_Youtube/\"\n",
    "# os.makedirs(log_dir_, exist_ok=True)\n",
    "spec=importlib.util.spec_from_file_location(\"DeepQNetwork\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
    "foo_1 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(foo_1)\n",
    "\n",
    "spec=importlib.util.spec_from_file_location(\"plotLearning\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/utils.py\")\n",
    "foo_2 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(foo_2)\n",
    "\n",
    "spec=importlib.util.spec_from_file_location(\"Agent\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
    "foo_3 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(foo_3)\n",
    "\n",
    "\n",
    "dir_prefix = \"./files/\"\n",
    "log_dir_obstacle = dir_prefix + \"DQN_obstacle/\"\n",
    "os.makedirs(log_dir_obstacle, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
    "\n",
    "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "# cuda_id = torch.cuda.current_device()\n",
    "# print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "\n",
    "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
    "# import gym\n",
    "\n",
    "cuda = torch.device('cuda')  # Default CUDA device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# model_path = \"\".format('dqn_lunar')\n",
    "\n",
    "model_path = log_dir_obstacle + \"dqn_0.zip\"\n",
    "model_test = DQN.load(model_path)\n",
    "print('loaded model')\n",
    "# for key, value in model_test.get_parameters().items():\n",
    "#     print(key, value.shape)\n",
    "\n",
    "env = gym.make(\"LunarLander-v4\").unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "paramshapes = model_test.get_parameters()\n",
    "\n",
    "\n",
    "def copy_dqn_weights(baselines_model):\n",
    "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
    "    model_params = baselines_model.get_parameters()\n",
    "    # Get only the policy parameters\n",
    "    model_params = model_params['policy']\n",
    "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
    "    policy_params = [model_params[key] for key in policy_keys]\n",
    "\n",
    "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
    "        param = policy_param.copy()\n",
    "        # Copy parameters from stable baselines model to pytorch model\n",
    "\n",
    "        # Conv layer\n",
    "        if len(param.shape) == 4:\n",
    "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
    "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
    "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
    "            param = np.transpose(param, (3, 2, 0, 1))\n",
    "\n",
    "        # weight of fully connected layer\n",
    "        if len(param.shape) == 2:\n",
    "            param = param.T\n",
    "\n",
    "        # bias\n",
    "        if 'b' in key:\n",
    "            param = param.squeeze()\n",
    "\n",
    "        param = torch.from_numpy(param)\n",
    "        pytorch_param.data.copy_(param.data.clone())\n",
    "\n",
    "    return torch_dqn\n",
    "\n",
    "\n",
    "dqn_torch_v = copy_dqn_weights(model_test)\n",
    "ct = 0\n",
    "\n",
    "for child in dqn_torch_v.children():\n",
    "    ct += 1\n",
    "    if ct < 2:\n",
    "        for param in child.parameters():\n",
    "            print(param)\n",
    "            print(ct)\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "print(dqn_torch_v.parameters())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for param in dqn_torch_v.parameters():\n",
    "  param.requires_grad = False\n",
    "num_ftrs = 64  # 8 states we have for the polly to move \n",
    "num_classes = 4 # number of Actions at final layer \n",
    "# ResNet final fully connected layer\n",
    "dqn_torch_v.fc = nn.Linear(num_ftrs, num_classes)\n",
    "dqn_torch_v.to(device)\n",
    "optimizer = torch.optim.Adam(dqn_torch_v.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# import gym\n",
    "\n",
    "\n",
    "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def obs_to_torch(obs):\n",
    "#     # TF: NHWC\n",
    "#     # PyTorch: NCHW\n",
    "#     # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
    "#     # obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "#     # # Normalize\n",
    "#     # obs = obs / 255.0\n",
    "#     obs = th.tensor(obs).float()\n",
    "#     obs = obs.to(device)\n",
    "#     return obs\n",
    "\n",
    "\n",
    "# env = gym.make('LunarLander-v4')\n",
    "\n",
    "# episode_reward = 0\n",
    "# done = False\n",
    "# obs = env.reset()\n",
    "# print(next(dqn_torch_v.parameters()).device)\n",
    "# while not done:\n",
    "#     action = th.argmax(dqn_torch_v(obs_to_torch(obs))).item()\n",
    "#     # action = env.action_space.sample()\n",
    "#     obs, reward, done, _ = env.step(action)\n",
    "#     episode_reward += reward\n",
    "\n",
    "# print(episode_reward)\n",
    "\n",
    "import gym\n",
    "# from simple_dqn_torch_2020 import Agent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    agent = foo_3.Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
    "                  input_dims=[8], lr=0.001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games = 500\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, \n",
    "                                    observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    filename = 'lunar_lander.png'\n",
    "    foo_2.plotLearning(x, scores, eps_history, filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
