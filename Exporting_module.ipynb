{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {},
        "id": "9EanvrrhobbU"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] ale-py==0.7.4 --quiet\n",
        "!pip install box2d-py --quiet\n",
        "!pip install gym pyvirtualdisplay --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "pE3qZJLcobbW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor,RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPoZnSTGFvEp",
        "outputId": "c6659f9b-5bae-476d-ffd4-c9e4b1ea3a03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "fp1bUnClobbY"
      },
      "outputs": [],
      "source": [
        "# @title Plotting/Video functions\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "vJm_qzWkobbg"
      },
      "outputs": [],
      "source": [
        "nn_layers = [64,64] #This is the configuration of your neural network. Currently, we have two layers, each consisting of 64 neurons.\n",
        "                    #If you want three layers with 64 neurons each, set the value to [64,64,64] and so on.\n",
        "\n",
        "learning_rate = 0.001 #This is the step-size with which the gradient descent is carried out.\n",
        "                      #Tip: Use smaller step-sizes for larger networks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the obstacle Env for Fine tuning "
      ],
      "metadata": {
        "id": "zb9VfXeFLkbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.box2d import LunarLander\n",
        "from Box2D.b2 import fixtureDef, circleShape, polygonShape, revoluteJointDef, contactListener, edgeShape\n",
        "import math"
      ],
      "metadata": {
        "id": "NfxblZzztMHP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400"
      ],
      "metadata": {
        "id": "E6Ivff2jW7q8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        if (\n",
        "                self.env.lander == contact.fixtureA.body\n",
        "                or self.env.lander == contact.fixtureB.body\n",
        "        ):\n",
        "            self.env.game_over = True\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = False\n",
        "\n",
        "class Custom_LunarLander_obs(LunarLander):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
        "    continuous = False\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        obs_coords = [10, 10],\n",
        "        enable_obstacle: bool = True\n",
        "    ):\n",
        "        LunarLander.__init__(self)\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "\n",
        "        self.obs_coords = obs_coords\n",
        "        self.enable_obstacle = enable_obstacle\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        # defining the polygon obstacle here:\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, \n",
        "                                  pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8  \n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
        "        )   \n",
        "\n",
        "    def reset(self):\n",
        "        self._destroy()\n",
        "        self.world.contactListener_keepref = ContactDetector(self)\n",
        "        self.world.contactListener = self.world.contactListener_keepref\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        # terrain\n",
        "        CHUNKS = 11\n",
        "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
        "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
        "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
        "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
        "        self.helipad_y = H / 4\n",
        "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
        "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
        "        smooth_y = [\n",
        "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
        "            for i in range(CHUNKS)\n",
        "        ]\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "\n",
        "        # defining the polygon obstacle here----------------------------------\n",
        "        vertices_poly = [(5, 5), (5, 2), (2, 2), (2, 5)]  # may need to change later\n",
        "        # self.obstacle = self.world.CreateStaticBody(\n",
        "        #\n",
        "        #     # shapes=polygonShape(centroid=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #     #                         self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #     #                    vertices= [(x / SCALE, y / SCALE) for x, y in vertices_poly]),\n",
        "        #     shapes=circleShape(pos=(self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "        #                             self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE)),\n",
        "        #                        radius=2),\n",
        "        #                         categoryBits=0x1000,\n",
        "        #\n",
        "        # )\n",
        "        self.obstacle = self.world.CreateStaticBody(\n",
        "            position=(self.obs_coords[0], self.obs_coords[1]),\n",
        "            # (self.obs_coords[0] + VIEWPORT_W / 2 / SCALE,\n",
        "            # self.obs_coords[1] + (self.helipad_y + LEG_DOWN / SCALE))\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                #circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                shape=circleShape(radius=20 / SCALE, pos=(self.obs_coords[0],\n",
        "                                       self.obs_coords[1])),\n",
        "                # density=5.0,\n",
        "                # friction=0.1,\n",
        "                # categoryBits=0x0010,\n",
        "                # # maskBits=0x001,  # collide only with ground\n",
        "                # restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "\n",
        "        self.obstacle.color1 = (0.5, 0.4, 0.9)\n",
        "        self.obstacle.color2 = (1, 1, 1)\n",
        "        # self.obstacle.alpha = 0.8\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "        self.sky_polys = []\n",
        "        for i in range(CHUNKS - 1):\n",
        "            p1 = (chunk_x[i], smooth_y[i])\n",
        "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
        "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
        "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
        "\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        initial_y = VIEWPORT_H / SCALE\n",
        "        self.lander = self.world.CreateDynamicBody(\n",
        "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
        "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
        "        self.lander.ApplyForceToCenter(\n",
        "            (\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "            ),\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        self.legs = []\n",
        "        for i in [-1, +1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001,\n",
        "                ),\n",
        "            )\n",
        "            leg.ground_contact = False\n",
        "            leg.color1 = (0.5, 0.4, 0.9)\n",
        "            leg.color2 = (0.3, 0.3, 0.5)\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=self.lander,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(0, 0),\n",
        "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
        "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
        "            )\n",
        "            if i == -1:\n",
        "                rjd.lowerAngle = (\n",
        "                        +0.9 - 0.5\n",
        "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
        "                rjd.upperAngle = +0.9\n",
        "            else:\n",
        "                rjd.lowerAngle = -0.9\n",
        "                rjd.upperAngle = -0.9 + 0.5\n",
        "            leg.joint = self.world.CreateJoint(rjd)\n",
        "            self.legs.append(leg)\n",
        "\n",
        "        self.drawlist = [self.lander] + self.legs\n",
        "        \n",
        "\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(action), \"%r (%s) invalid \" % (\n",
        "                action,\n",
        "                type(action),\n",
        "            )\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            ox = (\n",
        "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            )  # 4 is move a bit downwards, +-2 for randomness\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        # print([pos.x,pos.y])\n",
        "        vel = self.lander.linearVelocity\n",
        "      \n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2), # 0: x position\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2), # 1: y position\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS, # 2\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS, # 3\n",
        "            self.lander.angle, # 4\n",
        "            20.0 * self.lander.angularVelocity / FPS, # 5\n",
        "            1.0 if self.legs[0].ground_contact else 0.0, # 6\n",
        "            1.0 if self.legs[1].ground_contact else 0.0, # 7\n",
        "\n",
        "            # (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2), # 8: x position\n",
        "            # (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2), # 9: y position\n",
        "\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        state_8 = (pos.x - self.obs_coords[0] / SCALE) / (VIEWPORT_W / SCALE / 2);\n",
        "        state_9 = (pos.y - self.obs_coords[1] / SCALE) / (VIEWPORT_H / SCALE / 2);\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        # reward\n",
        "        # distance_to_obstacle = np.sqrt((pos.x - (self.obs_coords[0] +\n",
        "        #                                     VIEWPORT_W / SCALE / 2)) ** 2 +\n",
        "        #                         (pos.y - (self.obs_coords[1] +\n",
        "\n",
        "        #                                   (self.helipad_y + LEG_DOWN / SCALE))) ** 2)\n",
        "        distance_to_obstacle = np.sqrt(state_8 * state_8 + state_9 * state_9)\n",
        "\n",
        "        # if (distance_to_obstacle <= (1)):\n",
        "        #     print('dangerously close to obstacle!')\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            # If the lander moves away from the landing pad, it loses reward\n",
        "            - 100 * np.sqrt(state[0] * state[0] + state[1] * state[1]) # Euclidean distance\n",
        "            - 50 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "\n",
        "            - 100 * abs(state[4])\n",
        "            # Each leg with ground contact is +10 points.\n",
        "            + 10 * state[6]\n",
        "            + 10 * state[7]\n",
        "            - 50 * (distance_to_obstacle <= (20 / SCALE)) # obstacles \n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        # Firing the main engine is -0.3 points each frame. \n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        # Firing the side engine is -0.03 points each frame.\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        done = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0: # crashed?\n",
        "            done = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake and (np.sqrt(state[0] * state[0] + state[1] * state[1]) == 0) and (np.sqrt(state[2] * state[2] + state[3] * state[3])==0): # rest\n",
        "            done = True\n",
        "            reward = +200\n",
        "\n",
        "        return np.array(state, dtype=np.float32), reward, done, {}\n",
        "        # ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        from gym.envs.classic_control import rendering\n",
        "\n",
        "        if self.viewer is None:\n",
        "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
        "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                max(0.2, 0.2 + obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "                max(0.2, 0.5 * obj.ttl),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "        # print('drawlist')\n",
        "        # print(self.drawlist)\n",
        "        for p in self.sky_polys:\n",
        "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
        "        # editing below line to draw obstacle\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((100, 100))  # Position\n",
        "                    # self.viewer.draw_circle(20).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
        "\n",
        "        for obj2 in [self.obstacle]:\n",
        "            # print('rendering obstacle')\n",
        "            # print(obj2)\n",
        "            for f in obj2.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    # print('printing circle of radius')\n",
        "                    #t = rendering.Transform(translation=trans * f.shape.pos)\n",
        "                    t = rendering.Transform((self.obs_coords[0], self.obs_coords[1]))\n",
        "                    # print(f.shape.radius)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color1, filled=True\n",
        "                    ).add_attr(t)\n",
        "                    self.viewer.draw_circle(\n",
        "                        f.shape.radius, 20, color=obj2.color2, filled=False, linewidth=2\n",
        "                    ).add_attr(t)\n",
        "                    # t = rendering.Transform((10, 10))  # Position\n",
        "                    # self.viewer.draw_circle(2).add_attr(t)  # Add transform for position\n",
        "                    # self.viewer.render()\n",
        "                else:\n",
        "                    path = [trans * v for v in f.shape.vertices]\n",
        "                    # print('poly shape in object fixtures')\n",
        "                    # print(f)\n",
        "                    self.viewer.draw_polygon(path, color=obj2.color1)\n",
        "                    path.append(path[0])\n",
        "                    self.viewer.draw_polyline(path, color=obj2.color2, linewidth=2)\n",
        "\n",
        "        for x in [self.helipad_x1, self.helipad_x2]:\n",
        "            flagy1 = self.helipad_y\n",
        "            flagy2 = flagy1 + 50 / SCALE\n",
        "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
        "            self.viewer.draw_polygon(\n",
        "                [\n",
        "                    (x, flagy2),\n",
        "                    (x, flagy2 - 10 / SCALE),\n",
        "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
        "                ],\n",
        "                color=(0.8, 0.8, 0),\n",
        "            )\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
        "\n",
        "\n",
        "    \n",
        "  # def reset(self):\n",
        "  #     pass  # reward, done, info can't be included\n",
        "\n",
        "  # def render(self, mode='human'):\n",
        "  #     pass\n",
        "\n",
        "  # def close(self):\n",
        "  #     pass"
      ],
      "metadata": {
        "id": "MAwXYRAQhpvg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_env2 = Custom_LunarLander_obs()\n",
        "new_env2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ac9130-cf21-4a4e-8b87-29960ea09e7d",
        "id": "dxrSUxAgiRPg"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Custom_LunarLander_obs at 0x7f64511a7e50>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "check_env(new_env2)"
      ],
      "metadata": {
        "id": "5yacX9tliV_Z"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for env in gym.envs.registration.registry.env_specs.copy():\n",
        "    if 'LunarLander-v3' in env:\n",
        "        print(\"Remove {} from registry\".format(env))\n",
        "        del gym.envs.registration.registry.env_specs[env]"
      ],
      "metadata": {
        "id": "nNOEgfEolroH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "# Example for the CartPole environment\n",
        "register(\n",
        "    # unique identifier for the env `name-version`\n",
        "    id=\"LunarLander-v4\",\n",
        "    # path to the class for creating the env\n",
        "    # Note: entry_point also accept a class as input (and not only a string)\n",
        "    entry_point= Custom_LunarLander_obs,\n",
        "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
        "    max_episode_steps=500,\n",
        ")"
      ],
      "metadata": {
        "id": "pI_O6IZfi6RW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting the Baseline Model to Pytorch and Fine tuning the Last Layer Using the Obstacle Environment"
      ],
      "metadata": {
        "id": "-zXPpu7LQpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "print('run pytorch model')\n",
        "import gym\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from stable_baselines3 import DQN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLbWfW6K8tpF",
        "outputId": "c677047f-b29e-4b35-a239-d6a8677b885d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run pytorch model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/Hasanaldhahi3/atchekegroup1lunarlanding.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Og3pc49huE",
        "outputId": "a3aca937-5746-4ba2-8910-885515863681"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'atchekegroup1lunarlanding'...\n",
            "remote: Enumerating objects: 739, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 739 (delta 55), reused 42 (delta 42), pack-reused 678\u001b[K\n",
            "Receiving objects: 100% (739/739), 24.48 MiB | 29.74 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "# log_dir_ = dir_prefix + \"DQN_Youtube/\"\n",
        "# os.makedirs(log_dir_, exist_ok=True)\n",
        "spec=importlib.util.spec_from_file_location(\"DeepQNetwork\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_1 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_1)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"plotLearning\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/utils.py\")\n",
        "foo_2 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_2)\n",
        "\n",
        "spec=importlib.util.spec_from_file_location(\"Agent\",\"/content/atchekegroup1lunarlanding/YoutubeCodeRepository/ReinforcementLearning/DeepQLearning/simple_dqn_torch_2020.py\")\n",
        "foo_3 = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(foo_3)\n"
      ],
      "metadata": {
        "id": "CsJZwSVj9U_e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_prefix = \"./files/\"\n",
        "log_dir_obstacle = dir_prefix + \"DQN_obstacle/\"\n",
        "os.makedirs(log_dir_obstacle, exist_ok=True)"
      ],
      "metadata": {
        "id": "K5MHt4nN0SzO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning import simple_dqn_torch_2020\n",
        "\n",
        "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "# Storing ID of current CUDA device\n",
        "# cuda_id = torch.cuda.current_device()\n",
        "# print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
        "\n",
        "# print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
        "# import gym\n",
        "\n",
        "cuda = torch.device('cuda')  # Default CUDA device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3 import DQN\n",
        "\n",
        "# model_path = \"\".format('dqn_lunar')\n",
        "\n",
        "# model_path = log_dir_obstacle + \"model_stable_avg_reward_300 (3).zip\"\n",
        "model_test = DQN.load(\"/content/files/DQN_windy_model.zip\")\n",
        "print('loaded model')\n",
        "# for key, value in model_test.get_parameters().items():\n",
        "#     print(key, value.shape)\n",
        "\n",
        "env = gym.make(\"LunarLander-v4\").unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "paramshapes = model_test.get_parameters()\n",
        "\n",
        "\n",
        "def copy_dqn_weights(baselines_model):\n",
        "    torch_dqn = foo_1.DeepQNetwork(lr=0.001, n_actions=4, input_dims=[8], fc1_dims=256, fc2_dims=256)\n",
        "    model_params = baselines_model.get_parameters()\n",
        "    # Get only the policy parameters\n",
        "    model_params = model_params['policy']\n",
        "    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n",
        "    policy_params = [model_params[key] for key in policy_keys]\n",
        "\n",
        "    for (th_key, pytorch_param), key, policy_param in zip(torch_dqn.named_parameters(), policy_keys, policy_params):\n",
        "        param = policy_param.copy()\n",
        "        # Copy parameters from stable baselines model to pytorch model\n",
        "\n",
        "        # Conv layer\n",
        "        if len(param.shape) == 4:\n",
        "            # https://gist.github.com/chirag1992m/4c1f2cb27d7c138a4dc76aeddfe940c2\n",
        "            # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n",
        "            # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n",
        "            param = np.transpose(param, (3, 2, 0, 1))\n",
        "\n",
        "        # weight of fully connected layer\n",
        "        if len(param.shape) == 2:\n",
        "            param = param.T\n",
        "\n",
        "        # bias\n",
        "        if 'b' in key:\n",
        "            param = param.squeeze()\n",
        "\n",
        "        param = torch.from_numpy(param)\n",
        "        pytorch_param.data.copy_(param.data.clone())\n",
        "\n",
        "    return torch_dqn\n",
        "\n",
        "\n",
        "dqn_torch_v = copy_dqn_weights(model_test)\n",
        "ct = 0\n",
        "\n",
        "for child in dqn_torch_v.children():\n",
        "    ct += 1\n",
        "    if ct < 2:\n",
        "        for param in child.parameters():\n",
        "            print(param)\n",
        "            print(ct)\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "print(dqn_torch_v.parameters())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for param in dqn_torch_v.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = 64  # 8 states we have for the polly to move \n",
        "num_classes = 4 # number of Actions at final layer \n",
        "# ResNet final fully connected layer\n",
        "dqn_torch_v.fc = nn.Linear(num_ftrs, num_classes)\n",
        "dqn_torch_v.to(device)\n",
        "optimizer = torch.optim.Adam(dqn_torch_v.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# import gym\n",
        "\n",
        "\n",
        "# # from YoutubeCodeRepository.ReinforcementLearning.DeepQLearning.utils import plotLearning\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# def obs_to_torch(obs):\n",
        "#     # TF: NHWC\n",
        "#     # PyTorch: NCHW\n",
        "#     # https://discuss.pytorch.org/t/dimensions-of-an-input-image/19439\n",
        "#     # obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "#     # # Normalize\n",
        "#     # obs = obs / 255.0\n",
        "#     obs = th.tensor(obs).float()\n",
        "#     obs = obs.to(device)\n",
        "#     return obs\n",
        "\n",
        "\n",
        "# env = gym.make('LunarLander-v4')\n",
        "\n",
        "# episode_reward = 0\n",
        "# done = False\n",
        "# obs = env.reset()\n",
        "# print(next(dqn_torch_v.parameters()).device)\n",
        "# while not done:\n",
        "#     action = th.argmax(dqn_torch_v(obs_to_torch(obs))).item()\n",
        "#     # action = env.action_space.sample()\n",
        "#     obs, reward, done, _ = env.step(action)\n",
        "#     episode_reward += reward\n",
        "\n",
        "# print(episode_reward)"
      ],
      "metadata": {
        "id": "47vjrBZP9FAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a556ed-9357-4722-cf94-b23d9601803d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Is CUDA supported by this system?False\n",
            "CUDA version: 11.3\n",
            "cpu\n",
            "loaded model\n",
            "Parameter containing:\n",
            "tensor([[ 0.0321, -0.3339, -0.3467,  ..., -0.0535, -0.2962,  0.2011],\n",
            "        [ 0.1848,  0.1691,  0.0960,  ...,  0.3414, -0.0273,  0.0221],\n",
            "        [-0.0858,  0.1447, -0.2099,  ..., -0.3408, -0.1245,  0.1822],\n",
            "        ...,\n",
            "        [ 0.0040,  0.1856, -0.3443,  ...,  0.2173, -0.2872, -0.2651],\n",
            "        [ 0.2279, -0.3445, -0.0954,  ...,  0.0215, -0.0946,  0.3258],\n",
            "        [ 0.0539, -0.0594,  0.2301,  ...,  0.1384,  0.1601, -0.2227]],\n",
            "       requires_grad=True)\n",
            "1\n",
            "Parameter containing:\n",
            "tensor([ 0.2387, -0.1104,  0.0523,  0.1887,  0.3394,  0.2088, -0.3160, -0.2021,\n",
            "         0.0812, -0.1849,  0.0119, -0.2836, -0.2194, -0.1197,  0.0573, -0.2761,\n",
            "        -0.2277, -0.2673, -0.2273, -0.2968,  0.3231, -0.2577,  0.2361, -0.0663,\n",
            "         0.0938, -0.1911, -0.2810, -0.3089, -0.0762,  0.3024, -0.0129,  0.0651,\n",
            "         0.1218,  0.2820,  0.0572, -0.0534,  0.1629, -0.1096,  0.1724,  0.2093,\n",
            "        -0.0435, -0.0871, -0.1758, -0.2768,  0.1336,  0.1729, -0.0758, -0.0221,\n",
            "        -0.0532, -0.0213, -0.0649, -0.3261,  0.0790,  0.2244, -0.1020,  0.2354,\n",
            "        -0.2365, -0.2656,  0.1420, -0.3466, -0.0661,  0.0011, -0.2377, -0.1688,\n",
            "         0.2425, -0.1646, -0.0890, -0.2526, -0.1614, -0.3336, -0.2612,  0.1344,\n",
            "         0.0041,  0.1586, -0.2620,  0.2334,  0.1893, -0.0913, -0.1134,  0.2851,\n",
            "         0.0936,  0.0596, -0.0088, -0.0290, -0.0959,  0.0360,  0.3531,  0.2204,\n",
            "         0.3444, -0.2194, -0.2392,  0.2651, -0.0732, -0.0966, -0.1134, -0.1832,\n",
            "        -0.0720,  0.0314,  0.0438, -0.1022,  0.2350,  0.2166, -0.0854,  0.0523,\n",
            "         0.1245,  0.0032, -0.2168, -0.0294,  0.1147, -0.1226, -0.1698, -0.1956,\n",
            "        -0.2772, -0.1618, -0.2137, -0.1029,  0.0836,  0.0505,  0.0075,  0.2837,\n",
            "        -0.3155, -0.1217,  0.2876,  0.0661,  0.2700, -0.0029,  0.3129, -0.0317,\n",
            "         0.1271, -0.1170, -0.2690, -0.2677,  0.2125, -0.1075, -0.2838,  0.3396,\n",
            "        -0.0565,  0.2757,  0.2573,  0.1519, -0.0739,  0.3264,  0.2769,  0.0841,\n",
            "         0.2088,  0.0274,  0.1781,  0.0381, -0.1056,  0.0761,  0.0868,  0.3381,\n",
            "        -0.2363,  0.1388, -0.0042, -0.3315,  0.0693, -0.2842, -0.2489,  0.3246,\n",
            "        -0.3074, -0.3499, -0.1084, -0.2169,  0.2056, -0.1732, -0.1822,  0.0459,\n",
            "         0.1721,  0.2011, -0.0008, -0.3158,  0.0131,  0.0856,  0.2929, -0.2971,\n",
            "         0.0478, -0.0490,  0.1797, -0.2664,  0.0915, -0.0011,  0.1746, -0.0520,\n",
            "         0.2618,  0.1203, -0.0988, -0.1970, -0.0103, -0.2918, -0.2664,  0.0709,\n",
            "        -0.0648,  0.2930,  0.2687, -0.1217,  0.2209,  0.1695,  0.1058,  0.0637,\n",
            "        -0.1861, -0.2108, -0.0622, -0.0296, -0.0158,  0.0671, -0.2740, -0.0235,\n",
            "        -0.2264, -0.3294,  0.1534,  0.3297, -0.1784,  0.0914, -0.2163,  0.2082,\n",
            "         0.3329, -0.0317, -0.1126, -0.1043, -0.3155,  0.0431, -0.1658, -0.2501,\n",
            "         0.2205,  0.1217, -0.2911, -0.1782,  0.2111,  0.1622,  0.3397, -0.0231,\n",
            "         0.1084, -0.1458, -0.2135, -0.1699, -0.0202, -0.0039, -0.3521,  0.3012,\n",
            "        -0.1826, -0.2312, -0.2608,  0.0384, -0.3314, -0.0654,  0.2585, -0.1144,\n",
            "         0.0011, -0.0725,  0.1479,  0.2432,  0.0914,  0.0964,  0.3033,  0.0528],\n",
            "       requires_grad=True)\n",
            "1\n",
            "<generator object Module.parameters at 0x7f6451296ed0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from simple_dqn_torch_2020 import Agent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('LunarLander-v4')\n",
        "    agent = foo_3.Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "                  input_dims=[8], lr=0.001)\n",
        "    scores, eps_history = [], []\n",
        "    n_games = 500\n",
        "    \n",
        "    for i in range(n_games):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "\n",
        "        avg_score = np.mean(scores[-100:])\n",
        "\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "                'average score %.2f' % avg_score,\n",
        "                'epsilon %.2f' % agent.epsilon)\n",
        "    x = [i+1 for i in range(n_games)]\n",
        "    filename = 'lunar_lander.png'\n",
        "    foo_2.plotLearning(x, scores, eps_history, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oMihRr6Q0sES",
        "outputId": "8883b86f-49fa-4cb4-97e9-3a2df08bd0ce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score -146.02 average score -146.02 epsilon 0.98\n",
            "episode  1 score -102.94 average score -124.48 epsilon 0.92\n",
            "episode  2 score -16.12 average score -88.36 epsilon 0.88\n",
            "episode  3 score -32.06 average score -74.28 epsilon 0.83\n",
            "episode  4 score -36.73 average score -66.77 epsilon 0.78\n",
            "episode  5 score -177.53 average score -85.23 epsilon 0.73\n",
            "episode  6 score -198.69 average score -101.44 epsilon 0.67\n",
            "episode  7 score -46.29 average score -94.55 epsilon 0.62\n",
            "episode  8 score -208.25 average score -107.18 epsilon 0.58\n",
            "episode  9 score -78.44 average score -104.31 epsilon 0.54\n",
            "episode  10 score -143.66 average score -107.88 epsilon 0.48\n",
            "episode  11 score -68.22 average score -104.58 epsilon 0.43\n",
            "episode  12 score -51.93 average score -100.53 epsilon 0.36\n",
            "episode  13 score -38.15 average score -96.07 epsilon 0.31\n",
            "episode  14 score -49.22 average score -92.95 epsilon 0.25\n",
            "episode  15 score -211.19 average score -100.34 epsilon 0.18\n",
            "episode  16 score -42.79 average score -96.95 epsilon 0.08\n",
            "episode  17 score -260.92 average score -106.06 epsilon 0.03\n",
            "episode  18 score -226.81 average score -112.42 epsilon 0.01\n",
            "episode  19 score -83.23 average score -110.96 epsilon 0.01\n",
            "episode  20 score -6.39 average score -105.98 epsilon 0.01\n",
            "episode  21 score -17.96 average score -101.98 epsilon 0.01\n",
            "episode  22 score -36.71 average score -99.14 epsilon 0.01\n",
            "episode  23 score -67.69 average score -97.83 epsilon 0.01\n",
            "episode  24 score -8.09 average score -94.24 epsilon 0.01\n",
            "episode  25 score -246.71 average score -100.11 epsilon 0.01\n",
            "episode  26 score -120.47 average score -100.86 epsilon 0.01\n",
            "episode  27 score -28.52 average score -98.28 epsilon 0.01\n",
            "episode  28 score 3.94 average score -94.75 epsilon 0.01\n",
            "episode  29 score -24.60 average score -92.41 epsilon 0.01\n",
            "episode  30 score 19.11 average score -88.82 epsilon 0.01\n",
            "episode  31 score 5.25 average score -85.88 epsilon 0.01\n",
            "episode  32 score -112.06 average score -86.67 epsilon 0.01\n",
            "episode  33 score 8.14 average score -83.88 epsilon 0.01\n",
            "episode  34 score 3.66 average score -81.38 epsilon 0.01\n",
            "episode  35 score -33.52 average score -80.05 epsilon 0.01\n",
            "episode  36 score -14.11 average score -78.27 epsilon 0.01\n",
            "episode  37 score -30.63 average score -77.01 epsilon 0.01\n",
            "episode  38 score -59.93 average score -76.58 epsilon 0.01\n",
            "episode  39 score -308.89 average score -82.38 epsilon 0.01\n",
            "episode  40 score -399.06 average score -90.11 epsilon 0.01\n",
            "episode  41 score -295.29 average score -94.99 epsilon 0.01\n",
            "episode  42 score -181.20 average score -97.00 epsilon 0.01\n",
            "episode  43 score -50.21 average score -95.93 epsilon 0.01\n",
            "episode  44 score -30.36 average score -94.48 epsilon 0.01\n",
            "episode  45 score -258.94 average score -98.05 epsilon 0.01\n",
            "episode  46 score -14.48 average score -96.27 epsilon 0.01\n",
            "episode  47 score -219.26 average score -98.84 epsilon 0.01\n",
            "episode  48 score -195.60 average score -100.81 epsilon 0.01\n",
            "episode  49 score -38.67 average score -99.57 epsilon 0.01\n",
            "episode  50 score -60.89 average score -98.81 epsilon 0.01\n",
            "episode  51 score -20.18 average score -97.30 epsilon 0.01\n",
            "episode  52 score -73.14 average score -96.84 epsilon 0.01\n",
            "episode  53 score -65.58 average score -96.26 epsilon 0.01\n",
            "episode  54 score -216.80 average score -98.46 epsilon 0.01\n",
            "episode  55 score -176.76 average score -99.85 epsilon 0.01\n",
            "episode  56 score -263.01 average score -102.72 epsilon 0.01\n",
            "episode  57 score -7.51 average score -101.07 epsilon 0.01\n",
            "episode  58 score -516.88 average score -108.12 epsilon 0.01\n",
            "episode  59 score -361.51 average score -112.34 epsilon 0.01\n",
            "episode  60 score -464.40 average score -118.12 epsilon 0.01\n",
            "episode  61 score -379.70 average score -122.34 epsilon 0.01\n",
            "episode  62 score -253.94 average score -124.42 epsilon 0.01\n",
            "episode  63 score -405.96 average score -128.82 epsilon 0.01\n",
            "episode  64 score -463.89 average score -133.98 epsilon 0.01\n",
            "episode  65 score -304.14 average score -136.56 epsilon 0.01\n",
            "episode  66 score -206.06 average score -137.59 epsilon 0.01\n",
            "episode  67 score -225.60 average score -138.89 epsilon 0.01\n",
            "episode  68 score -252.82 average score -140.54 epsilon 0.01\n",
            "episode  69 score -315.91 average score -143.04 epsilon 0.01\n",
            "episode  70 score 25.46 average score -140.67 epsilon 0.01\n",
            "episode  71 score -204.22 average score -141.55 epsilon 0.01\n",
            "episode  72 score -63.39 average score -140.48 epsilon 0.01\n",
            "episode  73 score -272.62 average score -142.27 epsilon 0.01\n",
            "episode  74 score -129.58 average score -142.10 epsilon 0.01\n",
            "episode  75 score -44.52 average score -140.82 epsilon 0.01\n",
            "episode  76 score -309.13 average score -143.00 epsilon 0.01\n",
            "episode  77 score -334.66 average score -145.46 epsilon 0.01\n",
            "episode  78 score -23.93 average score -143.92 epsilon 0.01\n",
            "episode  79 score -62.99 average score -142.91 epsilon 0.01\n",
            "episode  80 score -108.72 average score -142.49 epsilon 0.01\n",
            "episode  81 score -135.83 average score -142.41 epsilon 0.01\n",
            "episode  82 score -206.25 average score -143.18 epsilon 0.01\n",
            "episode  83 score -85.92 average score -142.49 epsilon 0.01\n",
            "episode  84 score -64.41 average score -141.57 epsilon 0.01\n",
            "episode  85 score -27.39 average score -140.25 epsilon 0.01\n",
            "episode  86 score -247.38 average score -141.48 epsilon 0.01\n",
            "episode  87 score -119.32 average score -141.23 epsilon 0.01\n",
            "episode  88 score -24.50 average score -139.92 epsilon 0.01\n",
            "episode  89 score -368.00 average score -142.45 epsilon 0.01\n",
            "episode  90 score -23.14 average score -141.14 epsilon 0.01\n",
            "episode  91 score -120.41 average score -140.91 epsilon 0.01\n",
            "episode  92 score -88.82 average score -140.35 epsilon 0.01\n",
            "episode  93 score -192.47 average score -140.91 epsilon 0.01\n",
            "episode  94 score -49.66 average score -139.95 epsilon 0.01\n",
            "episode  95 score -53.97 average score -139.05 epsilon 0.01\n",
            "episode  96 score -412.54 average score -141.87 epsilon 0.01\n",
            "episode  97 score -208.36 average score -142.55 epsilon 0.01\n",
            "episode  98 score -124.04 average score -142.36 epsilon 0.01\n",
            "episode  99 score 64.31 average score -140.30 epsilon 0.01\n",
            "episode  100 score -24.89 average score -139.08 epsilon 0.01\n",
            "episode  101 score 8.99 average score -137.96 epsilon 0.01\n",
            "episode  102 score -5.97 average score -137.86 epsilon 0.01\n",
            "episode  103 score -26.45 average score -137.81 epsilon 0.01\n",
            "episode  104 score -56.14 average score -138.00 epsilon 0.01\n",
            "episode  105 score 24.10 average score -135.99 epsilon 0.01\n",
            "episode  106 score -73.54 average score -134.73 epsilon 0.01\n",
            "episode  107 score -3.63 average score -134.31 epsilon 0.01\n",
            "episode  108 score -16.50 average score -132.39 epsilon 0.01\n",
            "episode  109 score -41.13 average score -132.02 epsilon 0.01\n",
            "episode  110 score -80.63 average score -131.39 epsilon 0.01\n",
            "episode  111 score -65.68 average score -131.36 epsilon 0.01\n",
            "episode  112 score -33.43 average score -131.18 epsilon 0.01\n",
            "episode  113 score -27.63 average score -131.07 epsilon 0.01\n",
            "episode  114 score 29.08 average score -130.29 epsilon 0.01\n",
            "episode  115 score -323.05 average score -131.41 epsilon 0.01\n",
            "episode  116 score -20.49 average score -131.18 epsilon 0.01\n",
            "episode  117 score -136.79 average score -129.94 epsilon 0.01\n",
            "episode  118 score 9.10 average score -127.58 epsilon 0.01\n",
            "episode  119 score 3.43 average score -126.72 epsilon 0.01\n",
            "episode  120 score -92.22 average score -127.57 epsilon 0.01\n",
            "episode  121 score -136.84 average score -128.76 epsilon 0.01\n",
            "episode  122 score 20.68 average score -128.19 epsilon 0.01\n",
            "episode  123 score -24.06 average score -127.75 epsilon 0.01\n",
            "episode  124 score -36.10 average score -128.03 epsilon 0.01\n",
            "episode  125 score -122.33 average score -126.79 epsilon 0.01\n",
            "episode  126 score 41.00 average score -125.17 epsilon 0.01\n",
            "episode  127 score 47.17 average score -124.42 epsilon 0.01\n",
            "episode  128 score -113.06 average score -125.59 epsilon 0.01\n",
            "episode  129 score -276.48 average score -128.11 epsilon 0.01\n",
            "episode  130 score 3.15 average score -128.27 epsilon 0.01\n",
            "episode  131 score 64.74 average score -127.67 epsilon 0.01\n",
            "episode  132 score -145.50 average score -128.01 epsilon 0.01\n",
            "episode  133 score -151.11 average score -129.60 epsilon 0.01\n",
            "episode  134 score 42.72 average score -129.21 epsilon 0.01\n",
            "episode  135 score -200.27 average score -130.88 epsilon 0.01\n",
            "episode  136 score -344.17 average score -134.18 epsilon 0.01\n",
            "episode  137 score 33.52 average score -133.53 epsilon 0.01\n",
            "episode  138 score 16.68 average score -132.77 epsilon 0.01\n",
            "episode  139 score -278.20 average score -132.46 epsilon 0.01\n",
            "episode  140 score -27.64 average score -128.75 epsilon 0.01\n",
            "episode  141 score 33.62 average score -125.46 epsilon 0.01\n",
            "episode  142 score -170.93 average score -125.36 epsilon 0.01\n",
            "episode  143 score -54.42 average score -125.40 epsilon 0.01\n",
            "episode  144 score 13.52 average score -124.96 epsilon 0.01\n",
            "episode  145 score -182.49 average score -124.19 epsilon 0.01\n",
            "episode  146 score -142.43 average score -125.47 epsilon 0.01\n",
            "episode  147 score 0.93 average score -123.27 epsilon 0.01\n",
            "episode  148 score 14.29 average score -121.17 epsilon 0.01\n",
            "episode  149 score -159.69 average score -122.38 epsilon 0.01\n",
            "episode  150 score -8.03 average score -121.85 epsilon 0.01\n",
            "episode  151 score -112.86 average score -122.78 epsilon 0.01\n",
            "episode  152 score 8.50 average score -121.96 epsilon 0.01\n",
            "episode  153 score -152.92 average score -122.84 epsilon 0.01\n",
            "episode  154 score -138.01 average score -122.05 epsilon 0.01\n",
            "episode  155 score -4.16 average score -120.32 epsilon 0.01\n",
            "episode  156 score -52.14 average score -118.22 epsilon 0.01\n",
            "episode  157 score -59.39 average score -118.73 epsilon 0.01\n",
            "episode  158 score -326.10 average score -116.83 epsilon 0.01\n",
            "episode  159 score -0.67 average score -113.22 epsilon 0.01\n",
            "episode  160 score 36.79 average score -108.21 epsilon 0.01\n",
            "episode  161 score 18.34 average score -104.23 epsilon 0.01\n",
            "episode  162 score 10.23 average score -101.58 epsilon 0.01\n",
            "episode  163 score 22.42 average score -97.30 epsilon 0.01\n",
            "episode  164 score 0.87 average score -92.65 epsilon 0.01\n",
            "episode  165 score -17.24 average score -89.78 epsilon 0.01\n",
            "episode  166 score -0.23 average score -87.73 epsilon 0.01\n",
            "episode  167 score 20.29 average score -85.27 epsilon 0.01\n",
            "episode  168 score 31.66 average score -82.42 epsilon 0.01\n",
            "episode  169 score -3.95 average score -79.30 epsilon 0.01\n",
            "episode  170 score 26.95 average score -79.29 epsilon 0.01\n",
            "episode  171 score -15.79 average score -77.40 epsilon 0.01\n",
            "episode  172 score -72.69 average score -77.50 epsilon 0.01\n",
            "episode  173 score 11.66 average score -74.65 epsilon 0.01\n",
            "episode  174 score 9.02 average score -73.27 epsilon 0.01\n",
            "episode  175 score 32.16 average score -72.50 epsilon 0.01\n",
            "episode  176 score 12.49 average score -69.28 epsilon 0.01\n",
            "episode  177 score 35.86 average score -65.58 epsilon 0.01\n",
            "episode  178 score 2.53 average score -65.31 epsilon 0.01\n",
            "episode  179 score -6.85 average score -64.75 epsilon 0.01\n",
            "episode  180 score 7.38 average score -63.59 epsilon 0.01\n",
            "episode  181 score -54.09 average score -62.77 epsilon 0.01\n",
            "episode  182 score 26.54 average score -60.45 epsilon 0.01\n",
            "episode  183 score 20.95 average score -59.38 epsilon 0.01\n",
            "episode  184 score 21.05 average score -58.52 epsilon 0.01\n",
            "episode  185 score 54.87 average score -57.70 epsilon 0.01\n",
            "episode  186 score 5.07 average score -55.18 epsilon 0.01\n",
            "episode  187 score 5.15 average score -53.93 epsilon 0.01\n",
            "episode  188 score 18.97 average score -53.50 epsilon 0.01\n",
            "episode  189 score -7.29 average score -49.89 epsilon 0.01\n",
            "episode  190 score -91.86 average score -50.58 epsilon 0.01\n",
            "episode  191 score 20.27 average score -49.17 epsilon 0.01\n",
            "episode  192 score 55.20 average score -47.73 epsilon 0.01\n",
            "episode  193 score 56.79 average score -45.24 epsilon 0.01\n",
            "episode  194 score -0.68 average score -44.75 epsilon 0.01\n",
            "episode  195 score 13.06 average score -44.08 epsilon 0.01\n",
            "episode  196 score 10.70 average score -39.84 epsilon 0.01\n",
            "episode  197 score 38.22 average score -37.38 epsilon 0.01\n",
            "episode  198 score 12.70 average score -36.01 epsilon 0.01\n",
            "episode  199 score -350.53 average score -40.16 epsilon 0.01\n",
            "episode  200 score 19.40 average score -39.72 epsilon 0.01\n",
            "episode  201 score 9.68 average score -39.71 epsilon 0.01\n",
            "episode  202 score 15.48 average score -39.50 epsilon 0.01\n",
            "episode  203 score 38.40 average score -38.85 epsilon 0.01\n",
            "episode  204 score 27.17 average score -38.01 epsilon 0.01\n",
            "episode  205 score 27.68 average score -37.98 epsilon 0.01\n",
            "episode  206 score 16.15 average score -37.08 epsilon 0.01\n",
            "episode  207 score 27.61 average score -36.77 epsilon 0.01\n",
            "episode  208 score 46.74 average score -36.14 epsilon 0.01\n",
            "episode  209 score 8.48 average score -35.64 epsilon 0.01\n",
            "episode  210 score 39.11 average score -34.44 epsilon 0.01\n",
            "episode  211 score 62.38 average score -33.16 epsilon 0.01\n",
            "episode  212 score 35.46 average score -32.47 epsilon 0.01\n",
            "episode  213 score -0.03 average score -32.20 epsilon 0.01\n",
            "episode  214 score 14.21 average score -32.35 epsilon 0.01\n",
            "episode  215 score 53.70 average score -28.58 epsilon 0.01\n",
            "episode  216 score 27.25 average score -28.10 epsilon 0.01\n",
            "episode  217 score 49.50 average score -26.24 epsilon 0.01\n",
            "episode  218 score 11.17 average score -26.22 epsilon 0.01\n",
            "episode  219 score 30.23 average score -25.95 epsilon 0.01\n",
            "episode  220 score 36.88 average score -24.66 epsilon 0.01\n",
            "episode  221 score 66.20 average score -22.63 epsilon 0.01\n",
            "episode  222 score 41.54 average score -22.42 epsilon 0.01\n",
            "episode  223 score -48.12 average score -22.66 epsilon 0.01\n",
            "episode  224 score 31.71 average score -21.98 epsilon 0.01\n",
            "episode  225 score 22.53 average score -20.53 epsilon 0.01\n",
            "episode  226 score 5.96 average score -20.88 epsilon 0.01\n",
            "episode  227 score 19.22 average score -21.16 epsilon 0.01\n",
            "episode  228 score 43.20 average score -19.60 epsilon 0.01\n",
            "episode  229 score 34.19 average score -16.49 epsilon 0.01\n",
            "episode  230 score -4.72 average score -16.57 epsilon 0.01\n",
            "episode  231 score 36.73 average score -16.85 epsilon 0.01\n",
            "episode  232 score 20.95 average score -15.19 epsilon 0.01\n",
            "episode  233 score 16.96 average score -13.51 epsilon 0.01\n",
            "episode  234 score 39.98 average score -13.54 epsilon 0.01\n",
            "episode  235 score 15.29 average score -11.38 epsilon 0.01\n",
            "episode  236 score 1.55 average score -7.92 epsilon 0.01\n",
            "episode  237 score 36.78 average score -7.89 epsilon 0.01\n",
            "episode  238 score 42.88 average score -7.63 epsilon 0.01\n",
            "episode  239 score 41.14 average score -4.43 epsilon 0.01\n",
            "episode  240 score 10.46 average score -4.05 epsilon 0.01\n",
            "episode  241 score -56.22 average score -4.95 epsilon 0.01\n",
            "episode  242 score 63.23 average score -2.61 epsilon 0.01\n",
            "episode  243 score 17.16 average score -1.89 epsilon 0.01\n",
            "episode  244 score 14.66 average score -1.88 epsilon 0.01\n",
            "episode  245 score -247.61 average score -2.53 epsilon 0.01\n",
            "episode  246 score -12.49 average score -1.24 epsilon 0.01\n",
            "episode  247 score -4.11 average score -1.29 epsilon 0.01\n",
            "episode  248 score 30.02 average score -1.13 epsilon 0.01\n",
            "episode  249 score 55.48 average score 1.02 epsilon 0.01\n",
            "episode  250 score 30.35 average score 1.41 epsilon 0.01\n",
            "episode  251 score 54.58 average score 3.08 epsilon 0.01\n",
            "episode  252 score 42.42 average score 3.42 epsilon 0.01\n",
            "episode  253 score 28.14 average score 5.23 epsilon 0.01\n",
            "episode  254 score 39.49 average score 7.01 epsilon 0.01\n",
            "episode  255 score 37.06 average score 7.42 epsilon 0.01\n",
            "episode  256 score -4.96 average score 7.89 epsilon 0.01\n",
            "episode  257 score 49.00 average score 8.97 epsilon 0.01\n",
            "episode  258 score 76.59 average score 13.00 epsilon 0.01\n",
            "episode  259 score 38.55 average score 13.39 epsilon 0.01\n",
            "episode  260 score 28.44 average score 13.31 epsilon 0.01\n",
            "episode  261 score 57.80 average score 13.70 epsilon 0.01\n",
            "episode  262 score 43.44 average score 14.04 epsilon 0.01\n",
            "episode  263 score 6.03 average score 13.87 epsilon 0.01\n",
            "episode  264 score 70.39 average score 14.57 epsilon 0.01\n",
            "episode  265 score 47.34 average score 15.21 epsilon 0.01\n",
            "episode  266 score 39.88 average score 15.61 epsilon 0.01\n",
            "episode  267 score 39.27 average score 15.80 epsilon 0.01\n",
            "episode  268 score 31.08 average score 15.80 epsilon 0.01\n",
            "episode  269 score 23.01 average score 16.07 epsilon 0.01\n",
            "episode  270 score 25.91 average score 16.06 epsilon 0.01\n",
            "episode  271 score 27.25 average score 16.49 epsilon 0.01\n",
            "episode  272 score 22.11 average score 17.44 epsilon 0.01\n",
            "episode  273 score 52.84 average score 17.85 epsilon 0.01\n",
            "episode  274 score 52.06 average score 18.28 epsilon 0.01\n",
            "episode  275 score 28.63 average score 18.24 epsilon 0.01\n",
            "episode  276 score 44.63 average score 18.56 epsilon 0.01\n",
            "episode  277 score 44.83 average score 18.65 epsilon 0.01\n",
            "episode  278 score 35.44 average score 18.98 epsilon 0.01\n",
            "episode  279 score 59.14 average score 19.64 epsilon 0.01\n",
            "episode  280 score 29.90 average score 19.87 epsilon 0.01\n",
            "episode  281 score 47.95 average score 20.89 epsilon 0.01\n",
            "episode  282 score 43.30 average score 21.06 epsilon 0.01\n",
            "episode  283 score 48.96 average score 21.34 epsilon 0.01\n",
            "episode  284 score 30.65 average score 21.43 epsilon 0.01\n",
            "episode  285 score 42.45 average score 21.31 epsilon 0.01\n",
            "episode  286 score 36.97 average score 21.63 epsilon 0.01\n",
            "episode  287 score 52.39 average score 22.10 epsilon 0.01\n",
            "episode  288 score 43.64 average score 22.35 epsilon 0.01\n",
            "episode  289 score 32.30 average score 22.74 epsilon 0.01\n",
            "episode  290 score 33.85 average score 24.00 epsilon 0.01\n",
            "episode  291 score -34.89 average score 23.45 epsilon 0.01\n",
            "episode  292 score 80.91 average score 23.71 epsilon 0.01\n",
            "episode  293 score 62.53 average score 23.76 epsilon 0.01\n",
            "episode  294 score 53.82 average score 24.31 epsilon 0.01\n",
            "episode  295 score 58.72 average score 24.76 epsilon 0.01\n",
            "episode  296 score 60.27 average score 25.26 epsilon 0.01\n",
            "episode  297 score 41.95 average score 25.30 epsilon 0.01\n",
            "episode  298 score 57.08 average score 25.74 epsilon 0.01\n",
            "episode  299 score 59.01 average score 29.84 epsilon 0.01\n",
            "episode  300 score -13.11 average score 29.51 epsilon 0.01\n",
            "episode  301 score 46.59 average score 29.88 epsilon 0.01\n",
            "episode  302 score 49.90 average score 30.22 epsilon 0.01\n",
            "episode  303 score 66.42 average score 30.51 epsilon 0.01\n",
            "episode  304 score 30.35 average score 30.54 epsilon 0.01\n",
            "episode  305 score 59.59 average score 30.86 epsilon 0.01\n",
            "episode  306 score 59.89 average score 31.29 epsilon 0.01\n",
            "episode  307 score 38.30 average score 31.40 epsilon 0.01\n",
            "episode  308 score 11.35 average score 31.05 epsilon 0.01\n",
            "episode  309 score 64.36 average score 31.61 epsilon 0.01\n",
            "episode  310 score 54.37 average score 31.76 epsilon 0.01\n",
            "episode  311 score 34.42 average score 31.48 epsilon 0.01\n",
            "episode  312 score -3.44 average score 31.09 epsilon 0.01\n",
            "episode  313 score 34.96 average score 31.44 epsilon 0.01\n",
            "episode  314 score 54.62 average score 31.84 epsilon 0.01\n",
            "episode  315 score 9.47 average score 31.40 epsilon 0.01\n",
            "episode  316 score 64.14 average score 31.77 epsilon 0.01\n",
            "episode  317 score 39.23 average score 31.67 epsilon 0.01\n",
            "episode  318 score 58.28 average score 32.14 epsilon 0.01\n",
            "episode  319 score 56.65 average score 32.40 epsilon 0.01\n",
            "episode  320 score 37.42 average score 32.41 epsilon 0.01\n",
            "episode  321 score -44.30 average score 31.30 epsilon 0.01\n",
            "episode  322 score 56.10 average score 31.45 epsilon 0.01\n",
            "episode  323 score 48.20 average score 32.41 epsilon 0.01\n",
            "episode  324 score 34.39 average score 32.44 epsilon 0.01\n",
            "episode  325 score 18.13 average score 32.39 epsilon 0.01\n",
            "episode  326 score 21.23 average score 32.55 epsilon 0.01\n",
            "episode  327 score 42.93 average score 32.78 epsilon 0.01\n",
            "episode  328 score -43.21 average score 31.92 epsilon 0.01\n",
            "episode  329 score 37.44 average score 31.95 epsilon 0.01\n",
            "episode  330 score 46.96 average score 32.47 epsilon 0.01\n",
            "episode  331 score 55.66 average score 32.66 epsilon 0.01\n",
            "episode  332 score 55.08 average score 33.00 epsilon 0.01\n",
            "episode  333 score 26.28 average score 33.09 epsilon 0.01\n",
            "episode  334 score 16.41 average score 32.86 epsilon 0.01\n",
            "episode  335 score -1.64 average score 32.69 epsilon 0.01\n",
            "episode  336 score 51.89 average score 33.19 epsilon 0.01\n",
            "episode  337 score -8.20 average score 32.74 epsilon 0.01\n",
            "episode  338 score 44.83 average score 32.76 epsilon 0.01\n",
            "episode  339 score 37.87 average score 32.73 epsilon 0.01\n",
            "episode  340 score -69.75 average score 31.93 epsilon 0.01\n",
            "episode  341 score -85.84 average score 31.63 epsilon 0.01\n",
            "episode  342 score 33.05 average score 31.33 epsilon 0.01\n",
            "episode  343 score -69.34 average score 30.46 epsilon 0.01\n",
            "episode  344 score -87.64 average score 29.44 epsilon 0.01\n",
            "episode  345 score -59.54 average score 31.32 epsilon 0.01\n",
            "episode  346 score 32.40 average score 31.77 epsilon 0.01\n",
            "episode  347 score -79.65 average score 31.02 epsilon 0.01\n",
            "episode  348 score 31.72 average score 31.03 epsilon 0.01\n",
            "episode  349 score 10.20 average score 30.58 epsilon 0.01\n",
            "episode  350 score 41.24 average score 30.69 epsilon 0.01\n",
            "episode  351 score 27.07 average score 30.41 epsilon 0.01\n",
            "episode  352 score 78.18 average score 30.77 epsilon 0.01\n",
            "episode  353 score 49.67 average score 30.99 epsilon 0.01\n",
            "episode  354 score 52.01 average score 31.11 epsilon 0.01\n",
            "episode  355 score 33.99 average score 31.08 epsilon 0.01\n",
            "episode  356 score -43.71 average score 30.69 epsilon 0.01\n",
            "episode  357 score 26.36 average score 30.47 epsilon 0.01\n",
            "episode  358 score 26.74 average score 29.97 epsilon 0.01\n",
            "episode  359 score -17.26 average score 29.41 epsilon 0.01\n",
            "episode  360 score 12.39 average score 29.25 epsilon 0.01\n",
            "episode  361 score 52.19 average score 29.19 epsilon 0.01\n",
            "episode  362 score 20.49 average score 28.96 epsilon 0.01\n",
            "episode  363 score 19.74 average score 29.10 epsilon 0.01\n",
            "episode  364 score 46.16 average score 28.86 epsilon 0.01\n",
            "episode  365 score -47.68 average score 27.91 epsilon 0.01\n",
            "episode  366 score 17.46 average score 27.68 epsilon 0.01\n",
            "episode  367 score -13.40 average score 27.16 epsilon 0.01\n",
            "episode  368 score 4.76 average score 26.89 epsilon 0.01\n",
            "episode  369 score -32.00 average score 26.34 epsilon 0.01\n",
            "episode  370 score 47.43 average score 26.56 epsilon 0.01\n",
            "episode  371 score -11.86 average score 26.17 epsilon 0.01\n",
            "episode  372 score -45.08 average score 25.50 epsilon 0.01\n",
            "episode  373 score 46.37 average score 25.43 epsilon 0.01\n",
            "episode  374 score -9.46 average score 24.82 epsilon 0.01\n",
            "episode  375 score 35.12 average score 24.88 epsilon 0.01\n",
            "episode  376 score 52.86 average score 24.96 epsilon 0.01\n",
            "episode  377 score -175.53 average score 22.76 epsilon 0.01\n",
            "episode  378 score 22.49 average score 22.63 epsilon 0.01\n",
            "episode  379 score -80.26 average score 21.24 epsilon 0.01\n",
            "episode  380 score 45.91 average score 21.40 epsilon 0.01\n",
            "episode  381 score 53.43 average score 21.45 epsilon 0.01\n",
            "episode  382 score -63.07 average score 20.39 epsilon 0.01\n",
            "episode  383 score 33.09 average score 20.23 epsilon 0.01\n",
            "episode  384 score 49.07 average score 20.41 epsilon 0.01\n",
            "episode  385 score -217.17 average score 17.82 epsilon 0.01\n",
            "episode  386 score 10.30 average score 17.55 epsilon 0.01\n",
            "episode  387 score 47.10 average score 17.50 epsilon 0.01\n",
            "episode  388 score -64.60 average score 16.42 epsilon 0.01\n",
            "episode  389 score -31.42 average score 15.78 epsilon 0.01\n",
            "episode  390 score -168.31 average score 13.76 epsilon 0.01\n",
            "episode  391 score -5.72 average score 14.05 epsilon 0.01\n",
            "episode  392 score -50.16 average score 12.74 epsilon 0.01\n",
            "episode  393 score -38.10 average score 11.73 epsilon 0.01\n",
            "episode  394 score -18.73 average score 11.01 epsilon 0.01\n",
            "episode  395 score -80.98 average score 9.61 epsilon 0.01\n",
            "episode  396 score -173.26 average score 7.27 epsilon 0.01\n",
            "episode  397 score 31.62 average score 7.17 epsilon 0.01\n",
            "episode  398 score -48.52 average score 6.11 epsilon 0.01\n",
            "episode  399 score -87.30 average score 4.65 epsilon 0.01\n",
            "episode  400 score -56.06 average score 4.22 epsilon 0.01\n",
            "episode  401 score -22.15 average score 3.53 epsilon 0.01\n",
            "episode  402 score -106.86 average score 1.97 epsilon 0.01\n",
            "episode  403 score -30.58 average score 1.00 epsilon 0.01\n",
            "episode  404 score -59.53 average score 0.10 epsilon 0.01\n",
            "episode  405 score -52.46 average score -1.02 epsilon 0.01\n",
            "episode  406 score -138.86 average score -3.01 epsilon 0.01\n",
            "episode  407 score -30.43 average score -3.70 epsilon 0.01\n",
            "episode  408 score -35.02 average score -4.16 epsilon 0.01\n",
            "episode  409 score -115.22 average score -5.96 epsilon 0.01\n",
            "episode  410 score -56.64 average score -7.07 epsilon 0.01\n",
            "episode  411 score 42.76 average score -6.98 epsilon 0.01\n",
            "episode  412 score 52.34 average score -6.43 epsilon 0.01\n",
            "episode  413 score 52.59 average score -6.25 epsilon 0.01\n",
            "episode  414 score 21.07 average score -6.59 epsilon 0.01\n",
            "episode  415 score 25.59 average score -6.42 epsilon 0.01\n",
            "episode  416 score 40.13 average score -6.66 epsilon 0.01\n",
            "episode  417 score -72.03 average score -7.78 epsilon 0.01\n",
            "episode  418 score 30.15 average score -8.06 epsilon 0.01\n",
            "episode  419 score -84.75 average score -9.47 epsilon 0.01\n",
            "episode  420 score -118.40 average score -11.03 epsilon 0.01\n",
            "episode  421 score -45.06 average score -11.04 epsilon 0.01\n",
            "episode  422 score -90.25 average score -12.50 epsilon 0.01\n",
            "episode  423 score 24.15 average score -12.74 epsilon 0.01\n",
            "episode  424 score -53.68 average score -13.62 epsilon 0.01\n",
            "episode  425 score 59.25 average score -13.21 epsilon 0.01\n",
            "episode  426 score 33.81 average score -13.09 epsilon 0.01\n",
            "episode  427 score 58.48 average score -12.93 epsilon 0.01\n",
            "episode  428 score 56.29 average score -11.94 epsilon 0.01\n",
            "episode  429 score 49.95 average score -11.81 epsilon 0.01\n",
            "episode  430 score 37.88 average score -11.90 epsilon 0.01\n",
            "episode  431 score -61.87 average score -13.08 epsilon 0.01\n",
            "episode  432 score 34.92 average score -13.28 epsilon 0.01\n",
            "episode  433 score 36.70 average score -13.17 epsilon 0.01\n",
            "episode  434 score 0.55 average score -13.33 epsilon 0.01\n",
            "episode  435 score 31.24 average score -13.00 epsilon 0.01\n",
            "episode  436 score 33.21 average score -13.19 epsilon 0.01\n",
            "episode  437 score 41.23 average score -12.70 epsilon 0.01\n",
            "episode  438 score 54.75 average score -12.60 epsilon 0.01\n",
            "episode  439 score 31.87 average score -12.66 epsilon 0.01\n",
            "episode  440 score 35.56 average score -11.60 epsilon 0.01\n",
            "episode  441 score 48.91 average score -10.26 epsilon 0.01\n",
            "episode  442 score 61.49 average score -9.97 epsilon 0.01\n",
            "episode  443 score 35.08 average score -8.93 epsilon 0.01\n",
            "episode  444 score 43.71 average score -7.61 epsilon 0.01\n",
            "episode  445 score 40.65 average score -6.61 epsilon 0.01\n",
            "episode  446 score 36.82 average score -6.57 epsilon 0.01\n",
            "episode  447 score 44.25 average score -5.33 epsilon 0.01\n",
            "episode  448 score 39.41 average score -5.25 epsilon 0.01\n",
            "episode  449 score 41.75 average score -4.94 epsilon 0.01\n",
            "episode  450 score 52.90 average score -4.82 epsilon 0.01\n",
            "episode  451 score 53.74 average score -4.55 epsilon 0.01\n",
            "episode  452 score 42.76 average score -4.91 epsilon 0.01\n",
            "episode  453 score 30.85 average score -5.10 epsilon 0.01\n",
            "episode  454 score 12.92 average score -5.49 epsilon 0.01\n",
            "episode  455 score 70.08 average score -5.13 epsilon 0.01\n",
            "episode  456 score 58.23 average score -4.11 epsilon 0.01\n",
            "episode  457 score -115.83 average score -5.53 epsilon 0.01\n",
            "episode  458 score 26.70 average score -5.53 epsilon 0.01\n",
            "episode  459 score -64.77 average score -6.00 epsilon 0.01\n",
            "episode  460 score 73.45 average score -5.39 epsilon 0.01\n",
            "episode  461 score 73.77 average score -5.18 epsilon 0.01\n",
            "episode  462 score 39.41 average score -4.99 epsilon 0.01\n",
            "episode  463 score 61.20 average score -4.57 epsilon 0.01\n",
            "episode  464 score -86.20 average score -5.90 epsilon 0.01\n",
            "episode  465 score 0.74 average score -5.41 epsilon 0.01\n",
            "episode  466 score 48.08 average score -5.11 epsilon 0.01\n",
            "episode  467 score 72.35 average score -4.25 epsilon 0.01\n",
            "episode  468 score 37.69 average score -3.92 epsilon 0.01\n",
            "episode  469 score 48.36 average score -3.12 epsilon 0.01\n",
            "episode  470 score 40.50 average score -3.19 epsilon 0.01\n",
            "episode  471 score 56.31 average score -2.50 epsilon 0.01\n",
            "episode  472 score 59.17 average score -1.46 epsilon 0.01\n",
            "episode  473 score 77.46 average score -1.15 epsilon 0.01\n",
            "episode  474 score 43.11 average score -0.62 epsilon 0.01\n",
            "episode  475 score 61.06 average score -0.37 epsilon 0.01\n",
            "episode  476 score 27.52 average score -0.62 epsilon 0.01\n",
            "episode  477 score 33.22 average score 1.47 epsilon 0.01\n",
            "episode  478 score 5.40 average score 1.30 epsilon 0.01\n",
            "episode  479 score 55.86 average score 2.66 epsilon 0.01\n",
            "episode  480 score 59.14 average score 2.79 epsilon 0.01\n",
            "episode  481 score 31.21 average score 2.57 epsilon 0.01\n",
            "episode  482 score 40.81 average score 3.61 epsilon 0.01\n",
            "episode  483 score 29.60 average score 3.57 epsilon 0.01\n",
            "episode  484 score 48.40 average score 3.57 epsilon 0.01\n",
            "episode  485 score 35.37 average score 6.09 epsilon 0.01\n",
            "episode  486 score 52.95 average score 6.52 epsilon 0.01\n",
            "episode  487 score 46.00 average score 6.51 epsilon 0.01\n",
            "episode  488 score 75.09 average score 7.90 epsilon 0.01\n",
            "episode  489 score 29.92 average score 8.52 epsilon 0.01\n",
            "episode  490 score 61.71 average score 10.82 epsilon 0.01\n",
            "episode  491 score 10.27 average score 10.98 epsilon 0.01\n",
            "episode  492 score 40.09 average score 11.88 epsilon 0.01\n",
            "episode  493 score 60.95 average score 12.87 epsilon 0.01\n",
            "episode  494 score -12.80 average score 12.93 epsilon 0.01\n",
            "episode  495 score 66.77 average score 14.41 epsilon 0.01\n",
            "episode  496 score 51.31 average score 16.65 epsilon 0.01\n",
            "episode  497 score 11.36 average score 16.45 epsilon 0.01\n",
            "episode  498 score 18.13 average score 17.12 epsilon 0.01\n",
            "episode  499 score 30.42 average score 18.29 epsilon 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def jumpstart(y):\n",
        "    return (y[0])\n",
        "\n",
        "\n",
        "def asymptotic_performance(y, rolling_avg=1):\n",
        "    asymptotic_performance = np.mean(y[-rolling_avg:])\n",
        "    return asymptotic_performance\n",
        "\n",
        "\n",
        "def total_reward_fnc(y):\n",
        "    return np.sum(y)\n",
        "\n",
        "\n",
        "def transfer_ratio(y_transfer, y_reference, total_reward):\n",
        "    transfer_ratio = total_reward(y_transfer) / total_reward(y_reference)\n",
        "    return transfer_ratio\n",
        "\n",
        "\n",
        "def time_to_threshold(x, y, threshold=250):\n",
        "    return x[np.argmax(y > threshold)]\n",
        "\n",
        "\n",
        "def max_reward_fnc(y):\n",
        "    return np.max(y), np.argmax(y)\n",
        "\n",
        "\n",
        "def performance_metrics(x, y, rolling_avg=10, threshold=275, plot=True):\n",
        "    '''\n",
        "    TODO: write plots functions\n",
        "    '''\n",
        "\n",
        "    max_reward, idx_training_max = max_reward_fnc(y)\n",
        "    total_reward = total_reward_fnc(y)\n",
        "    n_timesteps = x.max()\n",
        "    reward_per_timestep = total_reward / n_timesteps\n",
        "    asymptote = asymptotic_performance(y, rolling_avg=rolling_avg)\n",
        "    std_asymptote = np.std(y[-rolling_avg:])\n",
        "    asymptote_after_max = asymptotic_performance(y, rolling_avg=n_timesteps - idx_training_max)\n",
        "    std_asymptote_after_max = np.std(y[-(n_timesteps - idx_training_max):])\n",
        "    time_threshold = time_to_threshold(x, y, threshold=threshold)\n",
        "    threshold_80 = 0.80 * threshold\n",
        "    time_threshold_80_of_max = time_to_threshold(x, y, threshold=threshold_80)\n",
        "    print(0.80 * max_reward)\n",
        "\n",
        "    print(f\" Max reward: {max_reward} \\\n",
        "        \\n Time steps to max reward: {idx_training_max}/{n_timesteps} ({idx_training_max * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Total reward: {total_reward:.2f} \\\n",
        "        \\n Reward per timestep: {reward_per_timestep:.2f} \\\n",
        "        \\n Asymptotic performance (last {rolling_avg} timesteps): {asymptote:.2f} ±{std_asymptote:.2f} ({asymptote * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Asymptotic performance after max: {asymptote_after_max:.2f} ±{std_asymptote_after_max:.2f} ({asymptote_after_max * 100 / max_reward:.2f}% of max reward) \\\n",
        "        \\n Time to threshold(={threshold}): {time_threshold}/{n_timesteps} ({time_threshold * 100 / n_timesteps:.2f}% of training period) \\\n",
        "        \\n Time to threshold(80% of max={threshold_80}): {time_threshold_80_of_max}/{n_timesteps} ({time_threshold_80_of_max * 100 / n_timesteps:.2f}% of training period)\" \\\n",
        "          )\n",
        "\n",
        "    if plot == True:\n",
        "        sns.scatterplot(x=x, y=y, color='green')\n",
        "        plt.title('Reward Over Timesteps 2')\n",
        "        plt.xlabel('Episode Index')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.show()\n",
        "\n",
        "        # sns.barplot(data=[asymptote])\n",
        "        # plt.show()\n",
        "        numpy_data = ([threshold, time_threshold_80_of_max])\n",
        "\n",
        "        plt.bar([0, 1], [time_threshold, time_threshold_80_of_max], color=['green', 'blue'])\n",
        "        plt.xticks([0, 1], labels=['Time to Max Threshold', 'Time to 80% max threshold'])\n",
        "        plt.title('Threshold Time')\n",
        "        plt.ylabel('Episode Number')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "XlZF54uNVHNN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EZFGdYjWDhDG"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_rewards = {\n",
        "    \"Agent\": agent\n",
        "}\n",
        "\n",
        "def generate_sample_models(number_of_models):\n",
        "    '''\n",
        "    This is function which generates sample models.\n",
        "    '''\n",
        "    for n in range(number_of_models):\n",
        "        reward_dict = {}\n",
        "        model_version_num = 'DQN' + str(n)\n",
        "        x = np.linspace(1,100,100) ## these are the timesteps\n",
        "        y = random.rand(100)\n",
        "        reward_dict = {'x': x, 'y' : y, 'comment': 'here comes the comment'}\n",
        "        all_model_rewards[model_version_num] = reward_dict\n",
        "generate_sample_models(10)"
      ],
      "metadata": {
        "id": "pOKgF-LxDy-y"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluations(all_model_rewards): \n",
        "    \n",
        "    '''\n",
        "    This is a function performing model evaluations. \n",
        "    \n",
        "    Input: a dict in the following format:\n",
        "    {'DQN5': {'x': array([  1.,   2.,   3.,  ....]),\n",
        "             'y': array([0.47025739, 0.5788533 , 0.72454499,...]),\n",
        "             'comment': 'comment for the model'}}\n",
        "             \n",
        "             \n",
        "             \n",
        "    The function performing the following metrics:\n",
        "    \n",
        "    \n",
        "    1. Max reward. \n",
        "    2. Jumpstart: The initial performance of an agent in a target task may be improved by transfer from a source task.\n",
        "    3. Asymptotic Performance: The final learned performance of an agent in the target task may be improved via transfer.\n",
        "    4. Total Reward: The total reward accumulated by an agent (i.e., the area under the learning curve) may be improved if it uses transfer, compared to learning without transfer.\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    max_reward = {}\n",
        "    jumpstart = {}\n",
        "    total_reward = {}\n",
        "    asymptotic_performance = {}\n",
        "    \n",
        "    for m in all_model_rewards.keys():\n",
        "        \n",
        "        max_reward[m] = all_model_rewards[m]['y'].max()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate jumpstart. \n",
        "        ######### It should start from the beginning of the array. \n",
        "        ####################################\n",
        "        jumpstart[m] =  all_model_rewards[m]['y'][:10].mean() \n",
        "        \n",
        "        total_reward[m] =  all_model_rewards[m]['y'].sum()\n",
        "        \n",
        "        ####################################\n",
        "        ######### NB! Here you can define on how long section do you want to evaluate asymptotic performance. \n",
        "        ######### It end at the end of the session.\n",
        "        ####################################\n",
        "        asymptotic_performance[m] =  all_model_rewards[m]['y'][-10:].sum()\n",
        "\n",
        "        \n",
        "        \n",
        "    ## Add the metrics to the final evaluation metric \n",
        "    \n",
        "    model_evs['max_reward'] = max_reward\n",
        "    model_evs['jumpstart'] = jumpstart\n",
        "    model_evs['total_reward'] = total_reward\n",
        "    model_evs['asymptotic_performance'] = asymptotic_performance\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "model_evaluations(all_model_rewards)  \n",
        "results = pd.DataFrame(model_evs)\n",
        "results.style.highlight_max(color = 'lightgreen', axis = 0)"
      ],
      "metadata": {
        "id": "ODnN1EwXD1b6",
        "outputId": "f9153fba-3249-4366-d7d1-9ff1f2095f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-c8556c8396dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mmodel_evaluations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_evs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighlight_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lightgreen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-c8556c8396dc>\u001b[0m in \u001b[0;36mmodel_evaluations\u001b[0;34m(all_model_rewards)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_model_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmax_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_model_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Agent' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_ratio(transfer_learner, scratch_model): \n",
        "\n",
        "    ''' Transfer Ratio: The ratio of the total reward accumulated by the transfer learner and the total reward accumulated by the non-transfer learner.'''\n",
        "\n",
        "    transfer_ratio = (transfer_learner['y']/scratch_model['y']).sum()\n",
        "    return (transfer_ratio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def time_to_threshold(transfer_learner, scratch_model):\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    Time to Threshold: The learning time needed by the agent to achieve a pre-specified perfor- mance level may be reduced via knowledge transfer.\n",
        "\n",
        "\n",
        "    This function returns the first timestep of the transfer_learning model when it reaches the scratch\n",
        "    model's maximum value. The threshold could be changed to \n",
        "        - any fixed number\n",
        "        - ratio of the scratch model's maximum reward\n",
        "        - the average of the final performance (averaged over the last n timesteps) of the scratch model. \n",
        "    '''\n",
        "    \n",
        "    threshold = scratch_model['y'].max()\n",
        "    threshold_index = np.where(transfer_learner['y'] >= threshold)[0][0]\n",
        "    \n",
        "    \n",
        "    return 'Transfer learner\\'s performance reaches the threashold (scratch model\\'s max performance) at timestap {}. Threshold is {}'.format(threshold_index, threshold)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "OS3lNNLXQc2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Exporting_module.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}